commit 1ee07a2a1678a7630040aa372de86a2a898aad9e
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 24 06:54:16 2023 -0400

    final code cleanup for train

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 22af547c..8d5b1579 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -3,13 +3,14 @@ Abel AI - A Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guid
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
-Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+Out of the 2000~ Lines-of-Code, approximately 1700~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
 To modify training behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TRAINING_TYPE Global knob on line 61)
 
 To debug (outside of the PySC2 Run Loop Environment) to see Python Tracebacks, use this string:
 python -m pysc2.bin.agent --map Simple64 --agent train_abel_ai.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
----- add to readme:
+
+---- File Descriptor Requirements (also present in README):
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
 craig		soft	nofile		8192
 craig 	hard	nofile		1048576
@@ -112,7 +113,7 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_1400_reward_1.18.pt'
+_INITIAL_WEIGHTS = 'abel_final_weights.pt'
 # Specify Naming Scheme of Future Model Checkpoints
 _DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
@@ -211,7 +212,6 @@ quadrants = [
 # This is used in the smart_attack functions (16 locations with offsets)
 # Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
 
-
 def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
     corner_offset = 3
     mini_quadrant_size = 16  # Each quadrant is divided further into 4
@@ -316,7 +316,7 @@ print("--------------------")
 # Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
 # Technically I believe this architecture could be called a "Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
-# 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 1. The 2M size deque where state/actions are appended/popped directly in O(1) time per game step
 # 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
@@ -537,10 +537,6 @@ class DQNModel(nn.Module):
         print("Replay buffer currently has: ",
               len(self.training_buffer), "entries")
 
-    # # Sample random transitions from replay buffer
-    # def random_sample(self, batch_size):
-        # return random_sample(self.training_buffer, batch_size)
-
     def random_sample(self, batch_size):
         """
         This function implements a biased random sampling strategy.
@@ -578,7 +574,7 @@ class DQNModel(nn.Module):
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
     # Implementation that Steven Brown (PySC2 Dev) created here:
     # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
-    # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
+    # I've of course heavily modified it as mentioned above, but...the initial work is his
     def choose_action(self, current_state, excluded_actions=[]):
         # Extract non_spatial_data and rgb_minimap from current_state
         non_spatial_data = current_state["non_spatial"]
@@ -642,7 +638,8 @@ class DQNModel(nn.Module):
         return action
 
     # This is where we train the model
-    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
+    # It samples stochastically from the replay buffer with a slight bias (30% comes from most recent 20%)
+    # Not as effective as PER, but "good enough"!
     def learn(self, target_network):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.training_buffer) < self.training_buffer_requirement:
@@ -831,7 +828,6 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # Provide the x/y coordinates of our command center(s)
-
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
@@ -939,8 +935,7 @@ class DQNModel(nn.Module):
         return fixed_length_units
 
 # Agent Implementation
-
-
+#
 class DQNAgent(base_agent.BaseAgent):
 
     def __init__(self):
@@ -1003,12 +998,13 @@ class DQNAgent(base_agent.BaseAgent):
         self.command_center = []
 
         # Action mapping
-        # Define which actions correspond to which quadrants
+        # Define which actions correspond to which map quadrants
         self.top_left_actions = [10, 11, 12, 13]
         self.top_right_actions = [14, 15, 16, 17]
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
 
+        # Load in our initial weights
         if os.path.isfile(_INITIAL_WEIGHTS):
             print("Loading previous model: ", _INITIAL_WEIGHTS)
             self.dqn_model.load_model(_INITIAL_WEIGHTS)
@@ -1019,7 +1015,6 @@ class DQNAgent(base_agent.BaseAgent):
         self.target_network = self.target_network.to(self.dqn_model.device)
 
     # We identify the current per-step reward based on in-game score and normalize it
-
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
@@ -1168,8 +1163,7 @@ class DQNAgent(base_agent.BaseAgent):
         # Return the final total_reward
         return total_reward
 
-    # BOILER PLATE CODE AGAIN
-
+    ## BOILER PLATE CODE From Stephen Brown
     def transformLocation(self, x, y):
         # Debug
         # print("Before transforming, x and y are set to: ", x , y)
@@ -1191,8 +1185,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         return [x, y]
 
-    # Return of boiler plate
-
+    ## BOILER PLATE CODE From Stephen Brown
     def splitAction(self, action_id):
         smart_action = smart_actions[action_id]
 
@@ -1203,7 +1196,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         return (smart_action, x, y)
 
-    # CUSTOM
+    # END OF BOILER PLATE
 
     # This function checks to see if we can add more marines to our build queue
     # Used to improve our reward system for the agent
@@ -1222,6 +1215,8 @@ class DQNAgent(base_agent.BaseAgent):
     def normalize(self, value, min_value, max_value):
         return (value - min_value) / (max_value - min_value)
 
+    # This is where the bulk of the "in game" logic resides"
+    # For each game step, PySC2 executes this code
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1274,12 +1269,12 @@ class DQNAgent(base_agent.BaseAgent):
                 print("Saving our model weights (Checkpoint)...")
                 self.dqn_model.save_model(
                     _DATA_FILE, self.episode_count, avg_reward)
-
             print("Previous average reward was: ",
                   self.previous_avg_reward)
             print("Our rolling-average reward is: ", avg_reward)
             print("Latest game reward was: ", combined_reward)
             print("Number of steps were: ", episode_steps)
+
             # Backpropagate the final reward multiplier to previous actions
             print(
                 f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
@@ -1290,14 +1285,13 @@ class DQNAgent(base_agent.BaseAgent):
             if self.episode_count % 10 == 0:
                 self.dqn_model.transfer_buffer()
 
-            # Print statements if our buffer is large enough to train on...
+            # If our replay buffer is large enough to begin training
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                # Display how many in-game training runs there were
                 print("Number of in-game model updates: ",
-                      self.in_game_training_iterations)
+                self.in_game_training_iterations)
                 print("Training the model after game completion...")
-
-            # Is our buffer large enough to begin training...
-            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                # Train one more time and record/print/export the time it takes
                 training_start_time = time.time()
                 self.dqn_model.learn(self.target_network)
                 training_end_time = time.time() - training_start_time
@@ -1340,8 +1334,8 @@ class DQNAgent(base_agent.BaseAgent):
                     except Exception as e:
                         print(f"Error logging Reward Histogram: {e}")
 
-            # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
+            # Reset remaining, episode-specific counters
             self.previous_avg_reward = avg_reward
             self.previous_action = None
             self.previous_state = None
@@ -1361,7 +1355,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Resetting List of each reward, displayed as a histogram (sampled every 25 games infrequently)
             self.episode_rewards = []
 
-            # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
+            # There is a predictable SC2 Segfault after every 986th game currently (memory leak induced), this code is a workaround
+            # Only occurs when multi-agent play is occurring, does not occur against built-in Bots
             if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
                 print(
                     "----Restarting underlying SC2 Environment for Crash Avoidance!!----")
@@ -1372,20 +1367,11 @@ class DQNAgent(base_agent.BaseAgent):
 
             return actions.FunctionCall(_NO_OP, [])
 
-        # BOILER PLATE Action-Space Guardrails
-        # Used as a way of limiting the potential action space at the beginning of the game for the agent
-
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
+        # If this is the first step in our game
         if obs.first():
 
-            # Original logic, doesn't work properly...
-            # player_y, player_x = (
-            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
-
-            # # print("Player x and y are set to: ", self.player_x, self.player_y)
-
             self.cc_y, self.cc_x = (
                 unit_type == _TERRAN_COMMANDCENTER).nonzero()
 
@@ -1438,7 +1424,7 @@ class DQNAgent(base_agent.BaseAgent):
         # if self.command_center:
         #     print("Command centers are set to:", self.command_center)
 
-        # Minerals
+        # Non-Spatial Information
         self.minerals = obs.observation['player'][1]
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
@@ -1447,8 +1433,6 @@ class DQNAgent(base_agent.BaseAgent):
         idle_worker_count = obs.observation['player'][7]
         army_count = obs.observation['player'][8]
 
-        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
-
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
@@ -1465,13 +1449,14 @@ class DQNAgent(base_agent.BaseAgent):
         supply_depot_recently_built = False
         # print("Game step is: ", self.actual_root_level_steps_taken)
 
+        # Check to see if we've recently built a supply depot
+        # This is passed via non_spatial_metrics
         if self.last_supply_depot_built_step is not None:
             if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
                 # print("supply_depot_recently_built = True" )
                 supply_depot_recently_built = True
-            # else:
-                # print("supply_depot_recently_built = False" )
 
+        # Non-Spatial-Metrics are passed to the model at each game step along with `visible_units`
         non_spatial_metrics = {
             'minerals': self.minerals,
             'supply_limit': supply_limit,
@@ -1490,13 +1475,14 @@ class DQNAgent(base_agent.BaseAgent):
             'camera_location': self.last_camera_action,
             'game_step_progress': self.normalize(self.actual_root_level_steps_taken, 1, 1500),
             'game_score': self.normalize(obs.observation.score_cumulative.score, 1, 12000),
-            # Prevents NaN from being sent to the model, breaking things
             'previous_action': self.previous_action or 0
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
         # print("Non-spatial barracks count is: ", non_spatial_metrics["barracks_count"])
 
+        # This is where we combine our spatial and non-spatial data which is transformed
+        # before passing to the model
         current_state = {
             "non_spatial": np.zeros(300),
             "rgb_minimap": None
@@ -1561,6 +1547,8 @@ class DQNAgent(base_agent.BaseAgent):
         if not hasattr(self, 'intended_action'):
             self.intended_action = None
 
+        # excluded_actions and a lot of the code within it can be found in Stephen Brown's work noted earlier
+        # It's mostly been refactored, but the structure and approach is very similar with some extra logic and guardrails (e.g intent)
         if self.move_number == 0:
             self.move_number += 1
 
@@ -1613,7 +1601,6 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
-            # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
@@ -1663,6 +1650,7 @@ class DQNAgent(base_agent.BaseAgent):
                     excluded_actions.append(action_to_exclude)
 
             # If our Guardrails are disabled, allow any action to be chosen
+            # This code controls full-action-space training
             if _TRAINING_GUARDRAILS_ENABLED == False:
                 excluded_actions.clear()
 
@@ -1806,10 +1794,6 @@ class DQNAgent(base_agent.BaseAgent):
 
             smart_action, x, y = self.splitAction(self.previous_action)
 
-            # end of boiler plate
-
-            # Custom code / initial design similar to boiler plate
-
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
 
@@ -1833,8 +1817,6 @@ class DQNAgent(base_agent.BaseAgent):
                     target[1] = max(BORDER_PADDING, min(
                         target[1], 83 - BORDER_PADDING))
 
-                    # Start our timer for reward shaping
-
                     # print("Trying to build a supply depot at:", target)
                     return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
@@ -1952,6 +1934,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         return actions.FunctionCall(_NO_OP, [])
 
+# main() is responsible for setting up the PySC2 run_loop() with all required environment variables
+# Works perfectly for Kane and Abel self-training, but there are occassionally errors with _TRAINING_TYPE == Bot
+# It's generally less frustrating to execute the script via the debug methodology (`python -m pysc2.bin.agent ...`)
 def main(args):
     agent1 = DQNAgent()
     agent1_name = "Abel_AI"

commit 57a5c6e97a8c03e7740600b1482f187c51631114
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 24 06:53:54 2023 -0400

    final code cleanup for test

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index df9f6de5..3049c81a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -3,11 +3,15 @@ Abel AI - A Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guid
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
-Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+Out of the 2000~ Lines-of-Code, approximately 1700~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
 To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the _TRAINING_TYPE Global knob on line 61)
 
----- add to readme:
+To debug (outside of the PySC2 Run Loop Environment) to see Python Tracebacks, use this string:
+python -m pysc2.bin.agent --map Simple64 --agent test_abel_ai.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
+
+
+---- File Descriptor Requirements (also present in README):
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
 craig		soft	nofile		8192
 craig 	hard	nofile		1048576
@@ -78,7 +82,6 @@ class CustomRestartException(Exception):
 # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
 # His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
 
-
 ################################## Start of BoilerPlate Code #####################################################
 _NO_OP = actions.FUNCTIONS.no_op.id
 _SELECT_POINT = actions.FUNCTIONS.select_point.id
@@ -110,7 +113,7 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-d.pt_episode_1700_reward_1.71.pt'
+_INITIAL_WEIGHTS = 'abel_final_weights.pt'
 # Specify Naming Scheme of Future Model Checkpoints
 # _DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
@@ -209,7 +212,6 @@ quadrants = [
 # This is used in the smart_attack functions (16 locations with offsets)
 # Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
 
-
 def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
     corner_offset = 3
     mini_quadrant_size = 16  # Each quadrant is divided further into 4
@@ -314,7 +316,7 @@ print("--------------------")
 # Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
 # Technically I believe this architecture could be called a "Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
-# 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 1. The 2M size deque where state/actions are appended/popped directly in O(1) time per game step
 # 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
@@ -535,10 +537,6 @@ class DQNModel(nn.Module):
         print("Replay buffer currently has: ",
               len(self.training_buffer), "entries")
 
-    # # Sample random transitions from replay buffer
-    # def random_sample(self, batch_size):
-        # return random_sample(self.training_buffer, batch_size)
-
     def random_sample(self, batch_size):
         """
         This function implements a biased random sampling strategy.
@@ -640,7 +638,8 @@ class DQNModel(nn.Module):
         return action
 
     # This is where we train the model
-    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
+    # It samples stochastically from the replay buffer with a slight bias (30% comes from most recent 20%)
+    # Not as effective as PER, but "good enough"!
     def learn(self, target_network):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.training_buffer) < self.training_buffer_requirement:
@@ -829,7 +828,6 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # Provide the x/y coordinates of our command center(s)
-
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
@@ -937,8 +935,7 @@ class DQNModel(nn.Module):
         return fixed_length_units
 
 # Agent Implementation
-
-
+#
 class DQNAgent(base_agent.BaseAgent):
 
     def __init__(self):
@@ -1001,12 +998,13 @@ class DQNAgent(base_agent.BaseAgent):
         self.command_center = []
 
         # Action mapping
-        # Define which actions correspond to which quadrants
+        # Define which actions correspond to which map quadrants
         self.top_left_actions = [10, 11, 12, 13]
         self.top_right_actions = [14, 15, 16, 17]
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
 
+        # Load in our initial weights
         if os.path.isfile(_INITIAL_WEIGHTS):
             print("Loading previous model: ", _INITIAL_WEIGHTS)
             self.dqn_model.load_model(_INITIAL_WEIGHTS)
@@ -1017,7 +1015,6 @@ class DQNAgent(base_agent.BaseAgent):
         self.target_network = self.target_network.to(self.dqn_model.device)
 
     # We identify the current per-step reward based on in-game score and normalize it
-
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
@@ -1166,8 +1163,7 @@ class DQNAgent(base_agent.BaseAgent):
         # Return the final total_reward
         return total_reward
 
-    # BOILER PLATE CODE AGAIN
-
+    ## BOILER PLATE CODE From Stephen Brown
     def transformLocation(self, x, y):
         # Debug
         # print("Before transforming, x and y are set to: ", x , y)
@@ -1189,8 +1185,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         return [x, y]
 
-    # Return of boiler plate
-
+    ## BOILER PLATE CODE From Stephen Brown
     def splitAction(self, action_id):
         smart_action = smart_actions[action_id]
 
@@ -1201,7 +1196,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         return (smart_action, x, y)
 
-    # CUSTOM
+    # END OF BOILER PLATE
 
     # This function checks to see if we can add more marines to our build queue
     # Used to improve our reward system for the agent
@@ -1219,7 +1214,9 @@ class DQNAgent(base_agent.BaseAgent):
     # A simple normalize function
     def normalize(self, value, min_value, max_value):
         return (value - min_value) / (max_value - min_value)
-
+    
+    # This is where the bulk of the "in game" logic resides"
+    # For each game step, PySC2 executes this code
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1339,8 +1336,8 @@ class DQNAgent(base_agent.BaseAgent):
                     except Exception as e:
                         print(f"Error logging Reward Histogram: {e}")
 
-            # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
+            # Reset remaining, episode-specific counters
             self.previous_avg_reward = avg_reward
             self.previous_action = None
             self.previous_state = None
@@ -1360,7 +1357,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Resetting List of each reward, displayed as a histogram (sampled every 25 games infrequently)
             self.episode_rewards = []
 
-            # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
+            # There is a predictable SC2 Segfault after every 986th game currently (memory leak induced), this code is a workaround
+            # Only occurs when multi-agent play is occurring, does not occur against built-in Bots
             if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
                 print(
                     "----Restarting underlying SC2 Environment for Crash Avoidance!!----")
@@ -1371,20 +1369,11 @@ class DQNAgent(base_agent.BaseAgent):
 
             return actions.FunctionCall(_NO_OP, [])
 
-        # BOILER PLATE Action-Space Guardrails
-        # Used as a way of limiting the potential action space at the beginning of the game for the agent
-
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
-
+        
+        # If this is the first step in our game
         if obs.first():
 
-            # Original logic, doesn't work properly...
-            # player_y, player_x = (
-            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
-
-            # # print("Player x and y are set to: ", self.player_x, self.player_y)
-
             self.cc_y, self.cc_x = (
                 unit_type == _TERRAN_COMMANDCENTER).nonzero()
 
@@ -1437,7 +1426,7 @@ class DQNAgent(base_agent.BaseAgent):
         # if self.command_center:
         #     print("Command centers are set to:", self.command_center)
 
-        # Minerals
+        # Non-Spatial Information
         self.minerals = obs.observation['player'][1]
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
@@ -1446,8 +1435,6 @@ class DQNAgent(base_agent.BaseAgent):
         idle_worker_count = obs.observation['player'][7]
         army_count = obs.observation['player'][8]
 
-        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
-
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
@@ -1464,13 +1451,15 @@ class DQNAgent(base_agent.BaseAgent):
         supply_depot_recently_built = False
         # print("Game step is: ", self.actual_root_level_steps_taken)
 
+        # Check to see if we've recently built a supply depot
+        # This is passed via non_spatial_metrics
         if self.last_supply_depot_built_step is not None:
             if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
                 # print("supply_depot_recently_built = True" )
                 supply_depot_recently_built = True
-            # else:
-                # print("supply_depot_recently_built = False" )
 
+        # This is where we combine our spatial and non-spatial data which is transformed
+        # before passing to the model
         non_spatial_metrics = {
             'minerals': self.minerals,
             'supply_limit': supply_limit,
@@ -1489,7 +1478,6 @@ class DQNAgent(base_agent.BaseAgent):
             'camera_location': self.last_camera_action,
             'game_step_progress': self.normalize(self.actual_root_level_steps_taken, 1, 1500),
             'game_score': self.normalize(obs.observation.score_cumulative.score, 1, 12000),
-            # Prevents NaN from being sent to the model, breaking things
             'previous_action': self.previous_action or 0
         }
 
@@ -1561,6 +1549,8 @@ class DQNAgent(base_agent.BaseAgent):
         if not hasattr(self, 'intended_action'):
             self.intended_action = None
 
+        # excluded_actions and a lot of the code within it can be found in Stephen Brown's work noted earlier
+        # It's mostly been refactored, but the structure and approach is very similar with some extra logic and guardrails (e.g intent)
         if self.move_number == 0:
             self.move_number += 1
 
@@ -1613,7 +1603,6 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
-            # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
@@ -1663,6 +1652,7 @@ class DQNAgent(base_agent.BaseAgent):
                     excluded_actions.append(action_to_exclude)
 
             # If our Guardrails are disabled, allow any action to be chosen
+            # This code controls full-action-space training
             if _TRAINING_GUARDRAILS_ENABLED == False:
                 excluded_actions.clear()
 
@@ -1806,10 +1796,6 @@ class DQNAgent(base_agent.BaseAgent):
 
             smart_action, x, y = self.splitAction(self.previous_action)
 
-            # end of boiler plate
-
-            # Custom code / initial design similar to boiler plate
-
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
 
@@ -1833,8 +1819,6 @@ class DQNAgent(base_agent.BaseAgent):
                     target[1] = max(BORDER_PADDING, min(
                         target[1], 83 - BORDER_PADDING))
 
-                    # Start our timer for reward shaping
-
                     # print("Trying to build a supply depot at:", target)
                     return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
@@ -1952,6 +1936,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         return actions.FunctionCall(_NO_OP, [])
 
+# main() is responsible for setting up the PySC2 run_loop() with all required environment variables
+# Works perfectly for Kane and Abel self-training, but there are occassionally errors with _TRAINING_TYPE == Bot
+# It's generally less frustrating to execute the script via the debug methodology (`python -m pysc2.bin.agent ...`)
 def main(args):
     agent1 = DQNAgent()
     agent1_name = "Abel_AI"

commit 3f99e046445951acc00003b7273a53dd78d22b3b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 23 14:16:03 2023 -0400

    Update train_abel_ai.py

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 15a9c862..22af547c 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -61,7 +61,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-_TRAINING_TYPE = "bot"  # Possible values: "self", "kane", "bot"
+_TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
 _BOT_DIFFICULTY = "easy"
 # Enable/Disable Training Guardrails
@@ -112,9 +112,9 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_400_reward_-1.00.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_1400_reward_1.18.pt'
 # Specify Naming Scheme of Future Model Checkpoints
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-c.pt'
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -348,7 +348,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-c-very_easy-bot-NGR'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-d-kane-NGR'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
@@ -447,9 +447,9 @@ class DQNModel(nn.Module):
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with a static learning rate)
-        # Using a min of 0.00015 to avoid zeroing out and regressing...
+        # Using a min of 0.0001 to avoid zeroing out and potentially regressing...
         self.scheduler = CosineAnnealingLR(
-            self.optimizer, T_max=5000, eta_min=0.00015)
+            self.optimizer, T_max=5000, eta_min=0.0001)
 
     #
     def _get_conv_out(self, shape):
@@ -1952,9 +1952,6 @@ class DQNAgent(base_agent.BaseAgent):
 
         return actions.FunctionCall(_NO_OP, [])
 
-    # end of boiler plate code
-
-
 def main(args):
     agent1 = DQNAgent()
     agent1_name = "Abel_AI"
@@ -2012,6 +2009,5 @@ def main(args):
             # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
             continue
 
-
 if __name__ == "__main__":
     app.run(main)

commit df19ca2027b7dc401075911592fd3169ebd3a743
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 23 09:58:29 2023 -0400

    fixed logging

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 7941d9d1..df9f6de5 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -1284,9 +1284,9 @@ class DQNAgent(base_agent.BaseAgent):
             # self.dqn_model.backpropagate_final_reward(
             #     final_reward_multiplier, self.actual_root_level_steps_taken)
 
-            # # Copy over our deque replay buffer into our list if it's been 10 games
-            # if self.episode_count % 10 == 0:
-            #     self.dqn_model.transfer_buffer()
+            # # Copy over our deque replay buffer into our list if it's been 1 game
+            if self.episode_count % 1 == 0:
+                self.dqn_model.transfer_buffer()
 
             # # Print statements if our buffer is large enough to train on...
             # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
@@ -1311,9 +1311,9 @@ class DQNAgent(base_agent.BaseAgent):
                 #     f"This training loop took {training_end_time:.4f} seconds.")
                 # print("Training complete")
                 # # Log our average reward to TensorBoard
-                # with SummaryWriter(self.dqn_model.writer_path) as writer:
-                #     writer.add_scalar(
-                #         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar(
+                        'Average Reward/value', avg_reward, self.episode_count)
                 # # Log training time
                 # with SummaryWriter(self.dqn_model.writer_path) as writer:
                 #     writer.add_scalar('Average Training Time in Seconds/train_duration',
@@ -1326,7 +1326,7 @@ class DQNAgent(base_agent.BaseAgent):
                     self.total_episode_rewards) / len(self.total_episode_rewards)
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar(
-                        'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
+                        'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.episode_count)
                 # Update our reward histogram every 3 games (sampled due to expense)
                 # In try/except block as these routinely fail/crash simulation
                 # Update our reward histogram every 3 games (sampled due to expense)
@@ -1335,7 +1335,7 @@ class DQNAgent(base_agent.BaseAgent):
                         with SummaryWriter(self.dqn_model.writer_path) as writer:
                             # This will log the distribution of rewards per game step
                             writer.add_histogram(
-                                'Per-Step Rewards', np.array(self.episode_rewards), self.dqn_model.global_training_steps)
+                                'Per-Step Rewards', np.array(self.episode_rewards), self.episode_count)
                     except Exception as e:
                         print(f"Error logging Reward Histogram: {e}")
 

commit c7de193ed6236935e8c560d025457cbd87b515cd
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 23 09:54:35 2023 -0400

    fixed conversions with test script

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 6c4cb75d..7941d9d1 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -1,11 +1,11 @@
 '''
-Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
+Abel AI - A Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
 Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
-To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the _TESTING_TYPE Global knob on line 61)
+To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the _TRAINING_TYPE Global knob on line 61)
 
 ---- add to readme:
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
@@ -21,17 +21,6 @@ FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many op
 '''
 
 
-# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
-# craig		soft	nofile		8192
-# craig 	hard	nofile		1048576
-# Additionally, tensorboard needs to be run as root (due to higher FD limits):
-# ulimit -n 1048576
-# tensorboard --logdir=runs
-#
-# Validated with: ulimit -s -H && ulimit -n -S
-# required as after 1k episodes we run out of FD's:
-# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
-
 # Import our other agents
 from Kane_AI import KaneAI
 
@@ -49,6 +38,8 @@ import torch.optim as optim
 import torch.nn.functional as F
 # queue used for replay buffer
 from collections import deque
+# from action-reptition detection in our rewards queue (and tensorboard metrics...)
+from collections import Counter
 
 # Using TensorBoard for model performance tracking & visualizations
 from torch.utils.tensorboard import SummaryWriter
@@ -57,7 +48,6 @@ from torch.cuda.amp import autocast, GradScaler
 # Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
 # The static learning rate of 0.01 was...quite poor excellent.
 from torch.optim.lr_scheduler import CosineAnnealingLR
-from collections import Counter
 # Using itertools to slice deque's efficiently
 import itertools
 
@@ -69,15 +59,16 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-_TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
-# Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
-_BOT_DIFFICULTY = "very_easy"
+_TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+# Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
+_BOT_DIFFICULTY = "easy"
 # Enable/Disable Training Guardrails
 _TRAINING_GUARDRAILS_ENABLED = False
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
 
+
 class CustomRestartException(Exception):
     pass
 
@@ -119,9 +110,9 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_1400_reward_1.18.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-d.pt_episode_1700_reward_1.71.pt'
 # Specify Naming Scheme of Future Model Checkpoints
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
+# _DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -320,7 +311,6 @@ print("--------------------")
 # --------------------
 
 
-
 # Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
 # Technically I believe this architecture could be called a "Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
@@ -329,7 +319,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.000, gamma=0.90, e_greedy=0.0, buffer_capacity=2000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0, gamma=0.90, e_greedy=0.0, buffer_capacity=2000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -356,9 +346,9 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-d-kane-NGR'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-kane-NGR-final-match'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 30000
+        self.training_buffer_requirement = 1
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -389,6 +379,7 @@ class DQNModel(nn.Module):
             24: 0,    # 'attack_45_61'
             25: 0     # 'attack_61_61'
         }
+
         # Setting up quadrant mapping to avoid repetitive copies in the hot path
         # Define which actions correspond to which quadrants
         # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
@@ -484,7 +475,6 @@ class DQNModel(nn.Module):
         return self.fc_after_lstm(lstm_out)
 
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
-
     def get_writer(self):
         return SummaryWriter(self.writer_path)
 
@@ -521,6 +511,7 @@ class DQNModel(nn.Module):
     # Win/loss/draw are multipliers
     # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
     # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
+
     def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
         # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
         steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
@@ -544,13 +535,9 @@ class DQNModel(nn.Module):
         print("Replay buffer currently has: ",
               len(self.training_buffer), "entries")
 
-    # # sample transitions from replay buffer queue for the last game
-    # def sample(self, batch_size):
-    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
-    #     return list(self.buffer)[-batch_size:]
-
-    # Sample random transitions from replay buffer
-    # Hopefully in a stochastic manner...
+    # # Sample random transitions from replay buffer
+    # def random_sample(self, batch_size):
+        # return random_sample(self.training_buffer, batch_size)
 
     def random_sample(self, batch_size):
         """
@@ -648,13 +635,13 @@ class DQNModel(nn.Module):
         self.epsilon -= self.epsilon_decay_rate
         self.epsilon = max(self.final_epsilon, self.epsilon)
 
-        # No training with Testing mode
-        # self.train()
+        # Turn training mode back on
+        self.train()
         return action
 
     # This is where we train the model
     # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
-    def learn(self):
+    def learn(self, target_network):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.training_buffer) < self.training_buffer_requirement:
             print("Replay buffer is currently too small to conduct training...")
@@ -702,6 +689,15 @@ class DQNModel(nn.Module):
         actions = torch.tensor(actions_np).to(self.device)
         rewards = torch.tensor(rewards_np).to(self.device)
 
+        # If this is the first training step, save the model's graph to TensorBoard for visualization purposes
+        if self.global_training_steps == 1:
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_graph(
+                        self, (non_spatial_states, rgb_minimap_states))
+            except Exception as e:
+                print(f"Error logging model graph: {e}")
+
         # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
         with autocast():
             # Compute the Q-values for the current states
@@ -709,8 +705,10 @@ class DQNModel(nn.Module):
             q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
 
             # Compute the target Q-values
-            next_q_values = self(non_spatial_next_states,
-                                 rgb_minimap_next_states)
+            # next_q_values = self(non_spatial_next_states,
+            #                      rgb_minimap_next_states)
+            next_q_values = target_network(
+                non_spatial_next_states, rgb_minimap_next_states)
             max_next_q_values = next_q_values.max(1)[0]
             q_target = rewards + self.gamma * max_next_q_values
 
@@ -814,13 +812,13 @@ class DQNModel(nn.Module):
             except Exception as e:
                 print(f"Error logging Actions/Frequency: {e}")
 
-    def save_model(self, file_path, episode_count, reward):
-        # Save our checkpoint weights
-        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
-        torch.save(self.state_dict(), save_path)
+    # def save_model(self, file_path, episode_count, reward):
+    #     # Save our checkpoint weights
+    #     save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+    #     torch.save(self.state_dict(), save_path)
 
-        # Also overwrite the top of tree so that we always have the latest to load if necessary:
-        torch.save(self.state_dict(), file_path)
+    #     # Also overwrite the top of tree so that we always have the latest to load if necessary:
+    #     torch.save(self.state_dict(), file_path)
 
     def load_model(self, file_path):
         print("Loading last checkpoint: ", file_path)
@@ -831,6 +829,7 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # Provide the x/y coordinates of our command center(s)
+
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
@@ -937,7 +936,6 @@ class DQNModel(nn.Module):
 
         return fixed_length_units
 
-
 # Agent Implementation
 
 
@@ -1008,13 +1006,18 @@ class DQNAgent(base_agent.BaseAgent):
         self.top_right_actions = [14, 15, 16, 17]
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
-        self.bottom_right_actions = [22, 23, 24, 25]
 
         if os.path.isfile(_INITIAL_WEIGHTS):
             print("Loading previous model: ", _INITIAL_WEIGHTS)
             self.dqn_model.load_model(_INITIAL_WEIGHTS)
-    
+            self.target_network.load_model(_INITIAL_WEIGHTS)
+
+        # Offload to GPU
+        self.dqn_model = self.target_network.to(self.dqn_model.device)
+        self.target_network = self.target_network.to(self.dqn_model.device)
+
     # We identify the current per-step reward based on in-game score and normalize it
+
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
@@ -1162,8 +1165,9 @@ class DQNAgent(base_agent.BaseAgent):
         #
         # Return the final total_reward
         return total_reward
-    
+
     # BOILER PLATE CODE AGAIN
+
     def transformLocation(self, x, y):
         # Debug
         # print("Before transforming, x and y are set to: ", x , y)
@@ -1196,7 +1200,7 @@ class DQNAgent(base_agent.BaseAgent):
             smart_action, x, y = smart_action.split('_')
 
         return (smart_action, x, y)
-    
+
     # CUSTOM
 
     # This function checks to see if we can add more marines to our build queue
@@ -1280,11 +1284,11 @@ class DQNAgent(base_agent.BaseAgent):
             # self.dqn_model.backpropagate_final_reward(
             #     final_reward_multiplier, self.actual_root_level_steps_taken)
 
-            # Copy over our deque replay buffer into our list if it's been 10 games
+            # # Copy over our deque replay buffer into our list if it's been 10 games
             # if self.episode_count % 10 == 0:
             #     self.dqn_model.transfer_buffer()
 
-            # Print statements if our buffer is large enough to train on...
+            # # Print statements if our buffer is large enough to train on...
             # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
             #     print("Number of in-game model updates: ",
             #           self.in_game_training_iterations)
@@ -1293,7 +1297,6 @@ class DQNAgent(base_agent.BaseAgent):
             # Is our buffer large enough to begin training...
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 # Disabled for training
-                #
                 # training_start_time = time.time()
                 # self.dqn_model.learn(self.target_network)
                 # training_end_time = time.time() - training_start_time
@@ -1334,7 +1337,7 @@ class DQNAgent(base_agent.BaseAgent):
                             writer.add_histogram(
                                 'Per-Step Rewards', np.array(self.episode_rewards), self.dqn_model.global_training_steps)
                     except Exception as e:
-                        print(f"Error logging Reward Histogram: {e}")me, self.dqn_model.global_training_steps)
+                        print(f"Error logging Reward Histogram: {e}")
 
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -1358,7 +1361,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.episode_rewards = []
 
             # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
-            if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TESTING_TYPE != "bot"):
+            if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
                 print(
                     "----Restarting underlying SC2 Environment for Crash Avoidance!!----")
                 # Raising a custom exception to get caught in our main PySC2 run_loop to trigger restart
@@ -1538,22 +1541,21 @@ class DQNAgent(base_agent.BaseAgent):
             [4.83e+02 3.00e+00 1.00e+04 2.10e+01 8.00e+00]
             [45.  1. 45. 12. 27.]
         '''
-        # Disabled for Testing
-        #
+
         # Push s/a/r/s_next our replay buffer
         # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
-        # if self.previous_action is not None:
-        #     self.actual_root_level_steps_taken += 1
-        #     # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
-        #     self.dqn_model.store_transition(self.previous_state,
-        #                                     self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics,supply_depot_recently_built), current_state)
+        if self.previous_action is not None:
+            self.actual_root_level_steps_taken += 1
+            # print("Pushing to the replay buffer: ", self.previous_action, self.get_normalized_reward(obs, self.previous_action, non_spatial_metrics))
+            self.dqn_model.store_transition(self.previous_state,
+                                            self.previous_action, self.get_normalized_reward(obs, self.previous_action, non_spatial_metrics, self.last_five_actions), current_state)
 
+        # Disabled for training
         # # Do in-game training of the model for every 100 root actions the agent takes
         # if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
         #     # print("Beginning in-game training for the model.")
         #     self.in_game_training_iterations += 1
         #     self.dqn_model.learn(self.target_network)
-        ############
 
         # Simple intent state tracker
         if not hasattr(self, 'intended_action'):
@@ -1737,6 +1739,7 @@ class DQNAgent(base_agent.BaseAgent):
                     target = safe_cc_x, safe_cc_y
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
             # To Do:
             # If get_command_center_coordinates returns None, None
             # This means we've probably lost our command center and need to build a new one
@@ -1951,22 +1954,22 @@ class DQNAgent(base_agent.BaseAgent):
 
 def main(args):
     agent1 = DQNAgent()
-    agent1_name = "Abel_AI" 
+    agent1_name = "Abel_AI"
 
     # Depending on the training type, create agent2
     # Self-Reinforcement Learning
-    if _TESTING_TYPE == "self":
+    if _TRAINING_TYPE == "self":
         agent2 = DQNAgent()
         agent2_name = "Abel_AI"
-    elif _TESTING_TYPE == "kane":
+    elif _TRAINING_TYPE == "kane":
         agent2 = KaneAI()
         agent2_name = "Kane_AI"
-    elif _TESTING_TYPE == "bot":
+    elif _TRAINING_TYPE == "bot":
         agent2 = sc2_env.Bot(sc2_env.Race.terran,
                              sc2_env.Difficulty[_BOT_DIFFICULTY])
         agent2_name = f"{_BOT_DIFFICULTY}_bot"
     else:
-        raise ValueError(f"Unknown TRAINING_TYPE: {_TESTING_TYPE}")
+        raise ValueError(f"Unknown TRAINING_TYPE: {_TRAINING_TYPE}")
 
     USE_FEATURE_UNITS = True
     RGB_SCREEN_SIZE = 84
@@ -1978,7 +1981,8 @@ def main(args):
                 map_name="Simple64",
                 players=[
                     sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
-                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TESTING_TYPE == "bot" else sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
+                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TRAINING_TYPE == "bot" else sc2_env.Agent(
+                        sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,

commit 139c52f4a35fb8b9c5ebcd566a85ed780b23bbf8
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 23 09:23:04 2023 -0400

    Full update to testing

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 75a5e188..6c4cb75d 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -73,10 +73,11 @@ _TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
 _BOT_DIFFICULTY = "very_easy"
 # Enable/Disable Training Guardrails
-_TRAINING_GUARDRAILS_ENABLED = True
+_TRAINING_GUARDRAILS_ENABLED = False
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
+
 class CustomRestartException(Exception):
     pass
 
@@ -86,9 +87,6 @@ class CustomRestartException(Exception):
 # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
 # His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
 
-####### CUSTOM CODE ######
-_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
 _NO_OP = actions.FUNCTIONS.no_op.id
@@ -120,8 +118,10 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-a.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
+# Specify Previous Network-Compatable Weights:
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_1400_reward_1.18.pt'
+# Specify Naming Scheme of Future Model Checkpoints
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -132,7 +132,7 @@ ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
 ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
 ###### END OF GLOBAL CUSTOM CODE #####
 
-# Steven Brown also used smart_actions although for this bot I've expanded them considerably
+# Steven Brown created the PySC2 concept of smart_actions although for this bot I've expanded them considerably
 smart_actions = [
     ACTION_DO_NOTHING,
     ACTION_BUILD_SUPPLY_DEPOT,
@@ -280,6 +280,7 @@ for i, quad in enumerate(quadrants):
     for x, y in points:
         smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
 
+
 # print("smart_actions is set to: ", smart_actions)
 print("--------------------")
 
@@ -289,44 +290,46 @@ for index, action in enumerate(smart_actions):
 print("--------------------")
 
 # --------------------
-# # Action Mapping
-# # 0: 'donothing'
-# # 1: 'buildsupplydepot'
-# # 2: 'buildbarracks'
-# # 3: 'buildmarine'
-# # 4: 'attackunit'
-# # 5: 'buildscv'
-# # 6: 'resetcamera'
-# # 7: 'mcselfexpansion'
-# # 8: 'mcenemyexpansion'
-# # 9: 'mcenemyprimary'
-# # 10: 'attack_1_1'
-# # 11: 'attack_13_1'
-# # 12: 'attack_1_13'
-# # 13: 'attack_13_13'
-# # 14: 'attack_33_1'
-# # 15: 'attack_45_1'
-# # 16: 'attack_33_13'
-# # 17: 'attack_45_13'
-# # 18: 'attack_1_33'
-# # 19: 'attack_13_33'
-# # 20: 'attack_1_45'
-# # 21: 'attack_13_45'
-# # 22: 'attack_33_33'
-# # 23: 'attack_45_33'
-# # 24: 'attack_33_45'
-# # 25: 'attack_45_45'
+# Action Mapping
+# 0: 'donothing'
+# 1: 'buildsupplydepot'
+# 2: 'buildbarracks'
+# 3: 'buildmarine'
+# 4: 'attackunit'
+# 5: 'buildscv'
+# 6: 'resetcamera'
+# 7: 'mcselfexpansion'
+# 8: 'mcenemyexpansion'
+# 9: 'mcenemyprimary'
+# 10: 'attack_3_3'
+# 11: 'attack_19_3'
+# 12: 'attack_3_19'
+# 13: 'attack_19_19'
+# 14: 'attack_45_3'
+# 15: 'attack_61_3'
+# 16: 'attack_45_19'
+# 17: 'attack_61_19'
+# 18: 'attack_3_45'
+# 19: 'attack_19_45'
+# 20: 'attack_3_61'
+# 21: 'attack_19_61'
+# 22: 'attack_45_45'
+# 23: 'attack_61_45'
+# 24: 'attack_45_61'
+# 25: 'attack_61_61'
 # --------------------
 
 
-# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
-# Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
-# To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
-# 1. The 1.2M size deque where state/actions are appended/popped directly in O(1) time
-# 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
+
+# Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
+# Technically I believe this architecture could be called a "Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
+# To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
+# 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
+# Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0, gamma=0.9, e_greedy=0.001, buffer_capacity=5000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.000, gamma=0.90, e_greedy=0.0, buffer_capacity=2000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -335,9 +338,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.01
-        # We decay over 2.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 2500000
+            self.epsilon - self.final_epsilon) / 1000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -353,40 +356,38 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v15b-testing'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-d-kane-NGR'
         # Our replay buffer training theshold size
-        # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 150000
+        self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
-        # Some rewards are further shaped based on game state, such as buildsupplydepot
         self.action_rewards = {
-            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
-            1: 1,  # 'buildsupplydepot'
-            2: 0.95,   # 'buildbarracks'
-            3: 0.8,  # 'buildmarine'
-            4: 0.8,  # 'attackunit' - High reward, attack visible units
-            5: 0.5,   # 'buildscv'
-            6: 0.6,   # 'resetcamera' - High reward as is required when available
-            7: 0.05,   # 'mcselfexpansion' - checks our expansion
-            8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
-            9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
-            10: 0,    # 'attack_4_4'
-            11: 0,    # 'attack_12_4'
-            12: 0,    # 'attack_4_12'
-            13: 0,    # 'attack_12_12'
-            14: 0,    # 'attack_36_4'
-            15: 0,    # 'attack_44_4'
-            16: 0,    # 'attack_36_12'
-            17: 0,    # 'attack_44_12'
-            18: 0,    # 'attack_4_36'
-            19: 0,    # 'attack_12_36'
-            20: 0,    # 'attack_4_44'
-            21: 0,    # 'attack_12_44'
-            22: 0,    # 'attack_36_36'
-            23: 0,    # 'attack_44_36'
-            24: 0,    # 'attack_36_44'
-            25: 0     # 'attack_44_44'
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as the AI sees it as a 'safe' move in perpetuity
+            1: 0.9,     # 'buildsupplydepot'
+            2: 0.9,  # 'buildbarracks'
+            3: 1,   # 'buildmarine'
+            4: 0.4,   # 'attackunit' - High reward, attack visible units
+            5: 0.9,   # 'buildscv'
+            6: 0.4,   # 'resetcamera' - High reward as is required when available
+            7: 0.05,  # 'mcselfexpansion' - moves camera - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - moves camera - checks enemy expansion
+            9: 0.15,  # 'mcenemyprimary' - moves camera - checks enemy primary base
+            10: 0,    # 'attack_3_3'
+            11: 0,    # 'attack_19_3'
+            12: 0,    # 'attack_3_19'
+            13: 0,    # 'attack_19_19'
+            14: 0,    # 'attack_45_3'
+            15: 0,    # 'attack_61_3'
+            16: 0,    # 'attack_45_19'
+            17: 0,    # 'attack_61_19'
+            18: 0,    # 'attack_3_45'
+            19: 0,    # 'attack_19_45'
+            20: 0,    # 'attack_3_61'
+            21: 0,    # 'attack_19_61'
+            22: 0,    # 'attack_45_45'
+            23: 0,    # 'attack_61_45'
+            24: 0,    # 'attack_45_61'
+            25: 0     # 'attack_61_61'
         }
         # Setting up quadrant mapping to avoid repetitive copies in the hot path
         # Define which actions correspond to which quadrants
@@ -429,7 +430,7 @@ class DQNModel(nn.Module):
         # Not using dropout as it loses our limited/sparse data (unfortunately...)
         # Overfitting hasn't been an issue with this particular setup
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(5, 64), 
+            nn.Linear(5, 64),
             nn.LeakyReLU(),
             nn.Linear(64, 128),
             nn.LeakyReLU(),
@@ -439,20 +440,23 @@ class DQNModel(nn.Module):
             nn.LeakyReLU(),
             nn.Linear(32, 16),
             nn.LeakyReLU(),
-        ).to(self.device) # Moving to GPU if available
+        ).to(self.device)  # Moving to GPU if available
 
         # Decision-making LSTM pass-through (takes concatenated, processed outputs from CNN and FCN, hits the LSTM, then to a single 4k->action_length layer)
         # Offloads to GPU if available
-        self.lstm_decision = nn.LSTM(input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
-        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(self.device)  # to get action probabilities/scores
-
+        self.lstm_decision = nn.LSTM(
+            input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
+        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(
+            self.device)  # to get action probabilities/scores
 
         self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
-        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=15000)
+        # The goal is to help the model move out of local minima (this happened a lot with a static learning rate)
+        # Using a min of 0.0001 to avoid zeroing out and potentially regressing...
+        self.scheduler = CosineAnnealingLR(
+            self.optimizer, T_max=5000, eta_min=0.0001)
 
     #
     def _get_conv_out(self, shape):
@@ -470,10 +474,12 @@ class DQNModel(nn.Module):
             non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
-        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)  # LSTM requires a sequence dimension...
+        # LSTM requires a sequence dimension...
+        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)
 
         lstm_out, _ = self.lstm_decision(combined)
-        lstm_out = lstm_out.squeeze(1)  # And now we remove the sequence dimension
+        # And now we remove the sequence dimension
+        lstm_out = lstm_out.squeeze(1)
 
         return self.fc_after_lstm(lstm_out)
 
@@ -547,7 +553,38 @@ class DQNModel(nn.Module):
     # Hopefully in a stochastic manner...
 
     def random_sample(self, batch_size):
-        return random_sample(self.training_buffer, batch_size)
+        """
+        This function implements a biased random sampling strategy.
+        Instead of uniformly sampling the replay buffer, we introduce a bias
+        towards more recent experiences. This has an effect similar to a
+        stochastic prioritization, where more recent experiences have a
+        higher probability of being sampled compared to older experiences.
+
+        The intuition behind this is that more recent experiences might be
+        more relevant to the current policy, and therefore might benefit
+        the agent more during training.
+        """
+
+        # Determine the count for recent experiences
+        # Here, we consider the most recent 20% of the replay buffer.
+        recent_count = int(0.2 * len(self.training_buffer))
+
+        # We take 30% of our batch size from recent experiences
+        # and the rest from the older experiences.
+        num_from_recent = int(0.3 * batch_size)
+        num_from_older = batch_size - num_from_recent
+
+        # Sample from the recent experiences segment.
+        # Larger index values are newer (it's copied directly from a queue, where the newest entries are the last)
+        recent_samples = random.sample(
+            self.training_buffer[-recent_count:], num_from_recent)
+
+        # Sample from the older experiences segment.
+        older_samples = random.sample(
+            self.training_buffer[:-recent_count], num_from_older)
+
+        # Combine and return the samples from both segments.
+        return recent_samples + older_samples
 
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
     # Implementation that Steven Brown (PySC2 Dev) created here:
@@ -563,11 +600,11 @@ class DQNModel(nn.Module):
 
         # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
         self.eval()
-        
+
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(
             non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
-        
+
         # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
@@ -575,20 +612,25 @@ class DQNModel(nn.Module):
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
-            available_actions = [a for a in self.actions if a not in excluded_actions]
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            # print("Available actions are:", available_actions)
             action = np.random.choice(available_actions)
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
-            conv_output = conv_output.view(conv_output.size(0), -1)  # No need for the third dimension anymore
+            # No need for the third dimension anymore
+            conv_output = conv_output.view(conv_output.size(0), -1)
 
             # Pass the non-spatial data through its layers
             # print("Shape of non_spatial_data_tensor:", non_spatial_data_tensor.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1)  # No need for the third dimension
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1)  # No need for the third dimension
 
             # Concatenate the two outputs along the second dimension (feature axis)
-            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(1)  # Adding sequence dimension for LSTM
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(
+                1)  # Adding sequence dimension for LSTM
             # print("------------Combined tensor shape:", combined_output.shape)
 
             # Pass through LSTM and subsequent fully connected layer
@@ -606,7 +648,7 @@ class DQNModel(nn.Module):
         self.epsilon -= self.epsilon_decay_rate
         self.epsilon = max(self.final_epsilon, self.epsilon)
 
-        # Disabled Turn training mode due to 'testing'
+        # No training with Testing mode
         # self.train()
         return action
 
@@ -788,71 +830,6 @@ class DQNModel(nn.Module):
     def get_units_by_type(self, obs, unit_type):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
-    # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action):
-        # Extract the cumulative score from the observation
-        score = obs.observation.score_cumulative.score
-        # Anything about 12k score results in a full score being provided to the model
-        max_score = 12000
-        # Normalize the score to be between 0 and 1
-        normalized_reward = min(score / max_score, 1)
-
-        # --------------------
-        # Action Mapping Reference
-        # 0: 'donothing'
-        # 1: 'buildsupplydepot'
-        # 2: 'buildbarracks'
-        # 3: 'buildmarine'
-        # 4: 'attackunit'
-        # 5: 'buildscv'
-        # 6: 'resetcamera'
-        # 7: 'mcselfexpansion'
-        # 8: 'mcenemyexpansion'
-        # 9: 'mcenemyprimary'
-        # 10: 'attack_4_4'
-        # 11: 'attack_12_4'
-        # 12: 'attack_4_12'
-        # 13: 'attack_12_12'
-        # 14: 'attack_36_4'
-        # 15: 'attack_44_4'
-        # 16: 'attack_36_12'
-        # 17: 'attack_44_12'
-        # 18: 'attack_4_36'
-        # 19: 'attack_12_36'
-        # 20: 'attack_4_44'
-        # 21: 'attack_12_44'
-        # 22: 'attack_36_36'
-        # 23: 'attack_44_36'
-        # 24: 'attack_36_44'
-        # 25: 'attack_44_44'
-        # --------------------
-
-        # Incentive multipliers for attack logic
-        attack_opposite_quadrant = 1
-        attack_adjacent_quadrant = 0.5
-        penalty_home_quadrant = -1
-        penalty_home_expansion = -0.5
-
-        # Incentivize attacks to opposite quadrant & their expansion
-        # Also de-incentivize attacking home base & expansion
-        if previous_action in self.bottom_right_actions:
-            normalized_reward += attack_opposite_quadrant
-        elif previous_action in self.bottom_left_actions:
-            normalized_reward += attack_adjacent_quadrant
-        elif previous_action in self.top_left_actions:
-            normalized_reward += penalty_home_quadrant
-        elif previous_action in self.top_right_actions:
-            normalized_reward += penalty_home_expansion
-
-        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
-        action_reward = self.action_rewards.get(previous_action, 0)
-        # Add the action reward, ensuring the total reward stays between 0 and 1
-        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
-
-        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
-
-        return normalized_reward
-
     # Provide the x/y coordinates of our command center(s)
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
@@ -895,7 +872,7 @@ class DQNModel(nn.Module):
         return transformed_minimap
 
     # This creates a fixed-length vector to store non-spatial data in (metrics and unit metadata)
-    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data):
+    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data, last_five_actions):
 
         # Safety checks
         # Check if units are None or empty
@@ -915,7 +892,6 @@ class DQNModel(nn.Module):
         Y_POS_INDEX = 13
 
         # Number of units and features
-        num_units = len(units)
         num_features = 5  # ['unit_type', 'health', 'x', 'y']
 
         # Create an empty array of shape (300, num_features)
@@ -925,14 +901,26 @@ class DQNModel(nn.Module):
         offset = 0
         if non_spatial_data:
             for key, value in non_spatial_data.items():
-                fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
+                # Input our metric in the first tuple with appropriate padding after
+                fixed_length_units[offset] = [value, 0, 0, 0, 0]
                 offset += 1
-                if offset >= 13:  # Adjust for a maximum 12 non-spatial data entries
+                if offset >= 18:  # Adjust for a maximum 16 non-spatial data entries
                     break
 
+        # Embed the last_five_actions data into our vector
+        # First, replace None elements with 0 and flatten the list
+        actions_cleaned = [action or 0 for action in last_five_actions]
+        # Then, pad the cleaned actions
+        actions_padded = actions_cleaned + [0] * (5 - len(actions_cleaned))
+
+        # Embed the flattened and padded actions
+        fixed_length_units[offset, :5] = actions_padded
+        offset += 1  # Increment the offset by 1, since we've added only one row
+
         # Then store the units information
         # Start after the reserved entries
-        for i in range(min(len(units), 300 - offset)):  # Adjusted based on the reserved non-spatial entries
+        # Adjusted based on the reserved non-spatial entries
+        for i in range(min(len(units), 300 - offset)):
             unit = units[i]
             index = i + offset  # Index into fixed_length_units
             fixed_length_units[index, 0] = unit[UNIT_TYPE_INDEX]
@@ -941,9 +929,11 @@ class DQNModel(nn.Module):
 
             # Transform x, y coordinates as required for normalization
             if not base_top_left:
-                fixed_length_units[index, 3], fixed_length_units[index, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index,
+                                                                 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
             else:
-                fixed_length_units[index, 3], fixed_length_units[index, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index,
+                                                                 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
 
         return fixed_length_units
 
@@ -991,16 +981,25 @@ class DQNAgent(base_agent.BaseAgent):
         self.actual_root_level_steps_taken = 0
         self.in_game_training_iterations = 0
 
+        # TensorBoard Per-Step Reward Metrics
+        # Cumulative Counter for single game
+        self.total_episode_reward = 0
+        # A queue of the last 100 games (used for average tracking)
+        self.total_episode_rewards = deque([0] * 100, maxlen=100)
+        # List of each reward, displayed as a histogram (sampled every 25 games infrequently)
+        self.episode_rewards = []
+
         # Custom Delay Timers
         self.attack_delay_timer = 0
         self.unit_attack_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        self.last_camera_action = None
+        self.last_camera_action = 0
         self.last_supply_depot_built_step = None
 
         # Queue that tracks last actions for exclusionary purposes
-        self.last_three_actions = deque(maxlen=3)
+        # The queue is also fed as current_state info back into the non_spatial FCN
+        self.last_five_actions = deque(maxlen=5)
         self.command_center = []
 
         # Action mapping
@@ -1009,11 +1008,161 @@ class DQNAgent(base_agent.BaseAgent):
         self.top_right_actions = [14, 15, 16, 17]
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
+        self.bottom_right_actions = [22, 23, 24, 25]
 
         if os.path.isfile(_INITIAL_WEIGHTS):
             print("Loading previous model: ", _INITIAL_WEIGHTS)
             self.dqn_model.load_model(_INITIAL_WEIGHTS)
+    
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything above 20k score results in a full score being provided to the model
+        max_score = 20000
+        # Normalize the score to be between 0 and 1 (We use -1 to 1 later)
+        normalized_score = min(score / max_score, 1)
 
+        # --------------------
+        # Action Mapping Reference from self.action_rewards{}
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 0.5
+        attack_adjacent_quadrant = 0.25
+        penalty_home_quadrant = -1
+        penalty_home_expansion = -0.5
+
+        action_reward = 0
+
+        # Incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if previous_action in self.bottom_right_actions:
+            action_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            action_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            action_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            action_reward += penalty_home_expansion
+
+        # Armies of 16 units and over will get the full reward
+        # Trying to encourage attacking with a reasonably large group versus one at a time...
+        max_army_size = 16
+        # Calculate the reward accelerator based on army size. This should give a value between 0.15 and 1.
+        army_size_accelerator = 0.15 + \
+            (0.85 * min(max_army_size,
+             non_spatial_metrics['army_count']) / max_army_size)
+
+        # Include the predefined rewards for the specific action
+        action_reward += self.dqn_model.action_rewards.get(previous_action, 0)
+
+        # Apply the accelerator ONLY to the attack actions that are not penalties
+        # 'attackunit' and 'attack_x_x' actions
+        attack_actions_without_penalty = [4] + list(range(10, 26))
+        non_penalty_attack_actions = list(set(
+            attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
+        # print("Non penalty actions are:" , non_penalty_attack_actions)
+        if previous_action in non_penalty_attack_actions:
+            action_reward *= army_size_accelerator
+            # print("attack action-reward is: ", action_reward)
+
+        # Calculate the total normalized reward by combining the normalized_score and action_reward
+        # Clamp it between -1 and 1 as the LSTM performs better due to its use of tahn
+        # We'll min/max this once again after further rewards below
+        total_reward = min(max(normalized_score + action_reward, -1), 1)
+
+        # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
+
+        # Check repetitiveness in the last_five_actions
+        # Essentially, penalize reptition unless it's for building units (marines or SCVs)
+        action_counts = Counter(last_five_actions)
+        for action, count in action_counts.items():
+            if action not in [3, 5] and count >= 3:
+                total_reward = 0
+
+        # Define a modifier based on the camera position
+        camera_modifier = 1 if non_spatial_metrics['home_base_camera'] else 0.5
+        # print("camera modifier is set to:", camera_modifier, "and last camera action is: ", non_spatial_metrics['camera_location']  )
+
+        # Special Reward handling to ensure Marines are only being built when there is available build_queue
+        if previous_action == 3 and not non_spatial_metrics['barracks_available_queue']:
+            total_reward = 0
+        # Special Reward Handling for supply depot construction and supply starvation
+        elif previous_action == 1:
+            if non_spatial_metrics['supply_free'] < 4:
+                # Reward for building supply during starvation
+                total_reward = camera_modifier * \
+                    (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
+            elif non_spatial_metrics['supply_free'] > 16:
+                # Reduced reward for building a supply depot if there's more than 16 extra supply
+                total_reward = camera_modifier * -0.25
+        # Ensure no reward for unit construction when supply is zero
+        elif (previous_action == 3 or previous_action == 5) and non_spatial_metrics['supply_free'] <= 0:
+            total_reward = 0
+        # Reward Shaping for SCV construction (reduced after 16)
+        elif previous_action == 5 and non_spatial_metrics['worker_supply'] > 16:
+            over_scv = non_spatial_metrics['worker_supply'] - 16
+            total_reward -= camera_modifier * 0.25 * over_scv
+        # Reward Shaping for Barracks construction (reduced after 6)
+        elif non_spatial_metrics['barracks_count'] > 6:
+            over_barracks = non_spatial_metrics['barracks_count'] - 6
+            total_reward -= camera_modifier * 0.25 * over_barracks
+
+        # Penalize certain camera movements if barracks_count is less than 3
+        # Agent should not waste cycles if we haven't built out the base yet
+        if non_spatial_metrics['barracks_count'] < 3 and previous_action in [7, 8, 9]:
+            total_reward = -1
+
+        # Ensure that if the camera is moved, de-incentivize any camera action that is not 6 (return to home base)
+        # 6 is home base and is excluded
+        if non_spatial_metrics['camera_location'] in [7, 8, 9] and previous_action in [7, 8, 9]:
+            total_reward = -1
+
+        # Clamp the final total_reward between -1 and 1
+        total_reward = max(min(total_reward, 1), -1)
+
+        # Update our TensorBoard per-step Reward Metrics
+        self.total_episode_reward += total_reward
+        # print("self.total_episode_reward is set to:", self.total_episode_reward)
+        # Update our Tensorboard per-Game Reward Histogram (if necessary - sampled every 3 games as it's expensive)
+        self.episode_rewards.append(total_reward)
+
+        # DEBUGS
+        #
+        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        # if total_reward < 1:
+        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        #
+        # Return the final total_reward
+        return total_reward
+    
     # BOILER PLATE CODE AGAIN
     def transformLocation(self, x, y):
         # Debug
@@ -1050,7 +1199,6 @@ class DQNAgent(base_agent.BaseAgent):
     
     # CUSTOM
 
-
     # This function checks to see if we can add more marines to our build queue
     # Used to improve our reward system for the agent
     def has_room_in_build_queue(self, obs):
@@ -1064,6 +1212,9 @@ class DQNAgent(base_agent.BaseAgent):
         # print("build queue is: ", obs.observation.build_queue)
         return False
 
+    # A simple normalize function
+    def normalize(self, value, min_value, max_value):
+        return (value - min_value) / (max_value - min_value)
 
     def step(self, obs):
         super(DQNAgent, self).step(obs)
@@ -1108,28 +1259,28 @@ class DQNAgent(base_agent.BaseAgent):
             # Calculate the rolling average reward
             avg_reward = sum(self.last_rewards) / len(self.last_rewards)
 
-            # print("------------------------------------------------------")
+            print("------------------------------------------------------")
             # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            # Checkpoint the model every 200 games
-            # if self.episode_count % 200 == 0:
-            #     print("Checkpointing our model...")
+            # Checkpoint the model every 100 games once we've started training
+            # if self.episode_count % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     print("Saving our model weights (Checkpoint)...")
             #     self.dqn_model.save_model(
-            #         DATA_FILE, self.episode_count, avg_reward)
-
-            # print("Previous average reward was: ",
-            #       self.previous_avg_reward)
-            # print("Our rolling-average reward is: ", avg_reward)
-            # print("Latest game reward was: ", combined_reward)
-            # print("Number of steps were: ", episode_steps)
-            # # Backpropagate the final reward multiplier to previous actions
+            #         _DATA_FILE, self.episode_count, avg_reward)
+
+            print("Previous average reward was: ",
+                  self.previous_avg_reward)
+            print("Our rolling-average reward is: ", avg_reward)
+            print("Latest game reward was: ", combined_reward)
+            print("Number of steps were: ", episode_steps)
+            # Backpropagate the final reward multiplier to previous actions
             # print(
             #     f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
             # self.dqn_model.backpropagate_final_reward(
             #     final_reward_multiplier, self.actual_root_level_steps_taken)
 
-            # # Copy over our deque replay buffer into our list if it's been 10 games
+            # Copy over our deque replay buffer into our list if it's been 10 games
             # if self.episode_count % 10 == 0:
             #     self.dqn_model.transfer_buffer()
 
@@ -1138,27 +1289,52 @@ class DQNAgent(base_agent.BaseAgent):
             #     print("Number of in-game model updates: ",
             #           self.in_game_training_iterations)
             #     print("Training the model after game completion...")
-            # Learn after every game, not just the successful ones:
 
-            # Log our reward over time and training duration if we've started training
-            # Learning disabled for testing
-            # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
-            #     training_start_time = time.time()
-            #     self.dqn_model.learn()
-            #     training_end_time = time.time() - training_start_time
-            #     self.training_time.append(training_end_time)
-            #     avg_training_time = sum(self.training_time) / len(self.training_time)
-            #     print(
-            #         f"This training loop took {training_end_time:.4f} seconds.")
-            #     print("Training complete")
-            #     # Log our average reward to TensorBoard
-            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
-            #         writer.add_scalar(
-            #             'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
-            #     # Log training time
-            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
-            #         writer.add_scalar('Average Training Time in Seconds/train_duration',
-            #                           avg_training_time, self.dqn_model.global_training_steps)
+            # Is our buffer large enough to begin training...
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                # Disabled for training
+                #
+                # training_start_time = time.time()
+                # self.dqn_model.learn(self.target_network)
+                # training_end_time = time.time() - training_start_time
+                # self.training_time.append(training_end_time)
+                # avg_training_time = sum(
+                #     self.training_time) / len(self.training_time)
+                # # Update our target DQN network every 5 games once training has begun
+                # if self.episode_count % 5 == 0:
+                #     self.target_network.load_state_dict(
+                #         self.dqn_model.state_dict())
+                # print(
+                #     f"This training loop took {training_end_time:.4f} seconds.")
+                # print("Training complete")
+                # # Log our average reward to TensorBoard
+                # with SummaryWriter(self.dqn_model.writer_path) as writer:
+                #     writer.add_scalar(
+                #         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+                # # Log training time
+                # with SummaryWriter(self.dqn_model.writer_path) as writer:
+                #     writer.add_scalar('Average Training Time in Seconds/train_duration',
+                #                       avg_training_time, self.dqn_model.global_training_steps)
+
+                # Update our TensorBoard per-step Reward Metrics
+                # Takes an average of the last 100 games (queue length = 100)
+                self.total_episode_rewards.append(self.total_episode_reward)
+                average_total_episode_rewards = sum(
+                    self.total_episode_rewards) / len(self.total_episode_rewards)
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar(
+                        'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
+                # Update our reward histogram every 3 games (sampled due to expense)
+                # In try/except block as these routinely fail/crash simulation
+                # Update our reward histogram every 3 games (sampled due to expense)
+                if self.episode_count % 3 == 0:
+                    try:
+                        with SummaryWriter(self.dqn_model.writer_path) as writer:
+                            # This will log the distribution of rewards per game step
+                            writer.add_histogram(
+                                'Per-Step Rewards', np.array(self.episode_rewards), self.dqn_model.global_training_steps)
+                    except Exception as e:
+                        print(f"Error logging Reward Histogram: {e}")me, self.dqn_model.global_training_steps)
 
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -1171,10 +1347,26 @@ class DQNAgent(base_agent.BaseAgent):
             supply_depot_recently_built = False
             self.last_supply_depot_built_step = None
             # Clear our action queue
-            self.last_three_actions.clear()
-            self.last_camera_action = None
-            for _ in range(3):
-                self.last_three_actions.append(None)
+            self.last_five_actions.clear()
+            self.last_camera_action = 0
+            for _ in range(5):
+                self.last_five_actions.append(None)
+            # TensorBoard Per-Step Reward Metrics
+            # Resetting Cumulative Counter for single game
+            self.total_episode_reward = 0
+            # Resetting List of each reward, displayed as a histogram (sampled every 25 games infrequently)
+            self.episode_rewards = []
+
+            # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
+            if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TESTING_TYPE != "bot"):
+                print(
+                    "----Restarting underlying SC2 Environment for Crash Avoidance!!----")
+                # Raising a custom exception to get caught in our main PySC2 run_loop to trigger restart
+                # This took...a lot of trial and error :(
+                raise CustomRestartException(
+                    "Time to restart the environment!")
+
+            return actions.FunctionCall(_NO_OP, [])
 
         # BOILER PLATE Action-Space Guardrails
         # Used as a way of limiting the potential action space at the beginning of the game for the agent
@@ -1182,6 +1374,7 @@ class DQNAgent(base_agent.BaseAgent):
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
         if obs.first():
+
             # Original logic, doesn't work properly...
             # player_y, player_x = (
             #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
@@ -1195,24 +1388,22 @@ class DQNAgent(base_agent.BaseAgent):
             # Using similar approach to Kane AI to Figure out where our home base is
             # Original reference is here of course:
             # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+            if obs.first():
 
-            player_y, player_x = (obs.observation.feature_minimap.player_relative ==
-                                  features.PlayerRelative.SELF).nonzero()
-            xmean = player_x.mean()
-            ymean = player_y.mean()
+                player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                      features.PlayerRelative.SELF).nonzero()
+                xmean = player_x.mean()
+                ymean = player_y.mean()
 
-            if xmean <= 31 and ymean <= 31:
-                self.base_top_left = True
-            else:
-                self.base_top_left = False
+                if xmean <= 31 and ymean <= 31:
+                    self.base_top_left = True
+                else:
+                    self.base_top_left = False
 
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
 
-        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
-
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
-        # supply_depot_count = int(round(len(depot_y) / 69))
         all_supply_depots = self.dqn_model.get_units_by_type(
             obs, units.Terran.SupplyDepot)
         supply_depot_count = len(
@@ -1266,77 +1457,87 @@ class DQNAgent(base_agent.BaseAgent):
         # Checking to see if we have available build queue for agent state
         barracks_available_build_queue = self.has_room_in_build_queue(obs)
 
+        # We check to see if we need to do any reward shaping to avoid supply starvation...
+        supply_depot_recently_built = False
+        # print("Game step is: ", self.actual_root_level_steps_taken)
+
+        if self.last_supply_depot_built_step is not None:
+            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
+                # print("supply_depot_recently_built = True" )
+                supply_depot_recently_built = True
+            # else:
+                # print("supply_depot_recently_built = False" )
+
         non_spatial_metrics = {
-            'minerals' : self.minerals,
-            'supply_limit' : supply_limit,
-            'supply_used' : supply_used,
-            'supply_free' : supply_free,
-            'army_supply' : army_supply,
-            'worker_supply' : worker_supply,
-            'cc_count' : command_center_count,
-            'barracks_count' : barracks_count,
-            'supply_depot_count' : supply_depot_count,
-            'idle_worker_count' : idle_worker_count,
-            'army_count' : army_count,
-            'home_base_camera' : self.last_camera_action == 6,
-            'barracks_available_queue' : barracks_available_build_queue
+            'minerals': self.minerals,
+            'supply_limit': supply_limit,
+            'supply_used': supply_used,
+            'supply_free': supply_free,
+            'army_supply': army_supply,
+            'worker_supply': worker_supply,
+            'cc_count': command_center_count,
+            'barracks_count': barracks_count,
+            'supply_depot_count': supply_depot_count,
+            'idle_worker_count': idle_worker_count,
+            'army_count': army_count,
+            'home_base_camera': self.last_camera_action == 6,
+            'barracks_available_queue': barracks_available_build_queue,
+            'supply_depot_recently_built': supply_depot_recently_built,
+            'camera_location': self.last_camera_action,
+            'game_step_progress': self.normalize(self.actual_root_level_steps_taken, 1, 1500),
+            'game_score': self.normalize(obs.observation.score_cumulative.score, 1, 12000),
+            # Prevents NaN from being sent to the model, breaking things
+            'previous_action': self.previous_action or 0
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
         # print("Non-spatial barracks count is: ", non_spatial_metrics["barracks_count"])
 
-
         current_state = {
             "non_spatial": np.zeros(300),
             "rgb_minimap": None
         }
         current_state["non_spatial"] = self.dqn_model.transform_non_spatial_to_fixed_length(
-            visible_units, self.base_top_left,non_spatial_metrics)
+            visible_units, self.base_top_left, non_spatial_metrics, self.last_five_actions)
         current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
             rgb_minimap, self.base_top_left)
-        
+
         # print("The output of our spatial data is set to: ", )
-        # for i in range(20):
+        # for i in range(30):
         #     five_tuple = current_state["non_spatial"][i]
         #     print(five_tuple)
 
         ''' Current State should look like this when passed to the model:
-        Non-spatial metrics are set to:  {'minerals': 1585, 'supply_limit': 31, 'supply_used': 31, 'supply_free': 0, 'army_supply': 14, 'worker_supply': 17, 'cc_count': 1, 'barracks_count': 6, 'supply_depot_count': 2, 'idle_worker_count': 8, 'army_count': 12, 'home_base_camera': True}
-        The output of our spatial data is set to: 
-        [1585.    0.    0.    0.    0.]
-        [31.  0.  0.  0.  0.]
-        [31.  0.  0.  0.  0.]
-        [0. 0. 0. 0. 0.]
-        [14.  0.  0.  0.  0.]
-        [17.  0.  0.  0.  0.]
-        [1. 0. 0. 0. 0.]
-        [6. 0. 0. 0. 0.]
-        [2. 0. 0. 0. 0.]
-        [8. 0. 0. 0. 0.]
-        [12.  0.  0.  0.  0.]
-        [1. 0. 0. 0. 0.]
-        [48.  1. 45. 24. 21.]
-        [45.  1. 45. -2. 53.]
-        [1.8e+01 1.0e+00 1.5e+03 7.0e+00 3.1e+01]
-        [  21.    1. 1000.   28.   59.]
-        [45.  1. 45. 31. 42.]
-        [ 19.   1. 400.  -2.  57.]
-        [45.  1. 45.  1. 54.]
-        [48.  1. 45. 15. 53.]
+            Non-spatial metrics are set to:  {'minerals': 105, 'supply_limit': 15, 'supply_used': 12, 'supply_free': 3, 'army_supply': 0, 'worker_supply': 12, 'cc_count': 1, 'barracks_count': 0, 'supply_depot_count': 0, 'idle_worker_count': 0, 'army_count': 0, 'home_base_camera': True, 'camera_location': 6, 'barracks_available_queue': False, 'supply_depot_recently_built': False, 'game_step_progress': 0.0066711140760507, 'game_score': 0.0920076673056088, 'previous_action': 0}
+            The output of our spatial data is set to: 
+            [105.   0.   0.   0.   0.]
+            [15.  0.  0.  0.  0.]
+            [12.  0.  0.  0.  0.]
+            [3. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [12.  0.  0.  0.  0.]
+            [1. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [1. 0. 0. 0. 0.]
+            [6. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0.00667111 0.         0.         0.         0.        ]
+            [0.09200767 0.         0.         0.         0.        ]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 6. 0. 0.]
+            [45.  1. 45. 17. 15.]
+            [45.  1. 45.  8. 23.]
+            [4.83e+02 3.00e+00 1.00e+04 7.00e+00 1.20e+01]
+            [3.42e+02 3.00e+00 1.00e+04 3.30e+01 8.00e+00]
+            [1.8e+01 1.0e+00 1.5e+03 2.2e+01 3.3e+01]
+            [3.41e+02 3.00e+00 1.00e+04 1.70e+01 1.20e+01]
+            [4.83e+02 3.00e+00 1.00e+04 2.10e+01 8.00e+00]
+            [45.  1. 45. 12. 27.]
         '''
-
-
-        # We check to see if we need to do any reward shaping to avoid supply starvation...
-        supply_depot_recently_built = False
-        # print("Game step is: ", self.actual_root_level_steps_taken)
-
-        if self.last_supply_depot_built_step is not None:
-            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
-                # print("supply_depot_recently_built = True" )
-                supply_depot_recently_built = True
-            # else:
-                # print("supply_depot_recently_built = False" )
-
         # Disabled for Testing
         #
         # Push s/a/r/s_next our replay buffer
@@ -1354,7 +1555,7 @@ class DQNAgent(base_agent.BaseAgent):
         #     self.dqn_model.learn(self.target_network)
         ############
 
-               # Simple intent state tracker
+        # Simple intent state tracker
         if not hasattr(self, 'intended_action'):
             self.intended_action = None
 
@@ -1396,18 +1597,17 @@ class DQNAgent(base_agent.BaseAgent):
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
-            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
-
+            # We guard supply depot builds by camera location - need to make sure we're at home base before building
             if supply_free > 4 or self.last_camera_action != 6 or self.minerals < 100:
                 excluded_actions.append(1)
 
-            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            # We guard barracks builds by camera location - need to make sure we're at home base before building
             # Also check to see that we have enough minerals
             if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
 
             # Exclude marines from the build queue
-            if supply_free == 0 or barracks_count < 4 or self.minerals < 50:
+            if supply_free == 0 or barracks_count < 1 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1415,7 +1615,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (army_supply < 16):
+            if (len(enemy_units) == 0) or (army_supply < 10):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1436,21 +1636,27 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(8)
                 excluded_actions.append(9)
 
-            # modified original logic, waits for 16 marines before attacking
+            # modified original logic, waits for 12 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
+            if 4 not in excluded_actions or army_supply < 12 or self.attack_delay_timer < 5:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
 
-            # Prevent the bot from doing nothing in perpetuity
-            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
-            # Safety check to make sure if no other options are available we don't crash...
+            # This section of the code aims to prevent the bot from repeatedly taking the same action over and over again,
+            # which can result in an undesirable behavior like doing nothing for an extended period.
+
+            # If the last five actions taken by the bot are identical...
+            if len(set(self.last_five_actions)) == 1:
 
-            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
-                action_to_exclude = self.last_three_actions[0]
+                # Retrieve the action that was repeated
+                action_to_exclude = self.last_five_actions[0]
+
+                # Check if this action isn't already in the list of excluded actions
+                # and also ensure we aren't excluding too many actions, which could lead to potential crashes
+                # or undesirable behaviors if no actions are left to choose from.
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
@@ -1469,13 +1675,8 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
-            # # Weird error condition where our camera gets locked - purge entries every 100 actions
-            # if self.actual_root_level_steps_taken % 100 == 0:
-            #     # print("zeroing out camera action")
-            #     self.last_camera_action = None
-
-            # Add the action to our tiny 3-element queue
-            self.last_three_actions.append(rl_action)
+            # Add the action to our tiny 5-element queue
+            self.last_five_actions.append(rl_action)
 
             # DEBUG : Print all exclusions!
             # print("Our excluded actions for this step are: ", excluded_actions)
@@ -1484,6 +1685,11 @@ class DQNAgent(base_agent.BaseAgent):
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)
 
+            # Quick reset for first-step as bots have issues with camera state
+            if obs.first():
+                # Set our smart action to our base
+                smart_action == ACTION_RESET_CAMERA
+
             if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
                 unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
 
@@ -1507,6 +1713,8 @@ class DQNAgent(base_agent.BaseAgent):
                 if barracks_y.any():
                     i = random.randint(0, len(barracks_y) - 1)
                     target = [barracks_x[i], barracks_y[i]]
+
+                    self.intended_action = ACTION_BUILD_MARINE
                     # Checks to avoid out-of-bounds crashing
                     # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
                     if 0 <= target[0] < 84 and 0 <= target[1] < 84:
@@ -1529,7 +1737,6 @@ class DQNAgent(base_agent.BaseAgent):
                     target = safe_cc_x, safe_cc_y
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
-
             # To Do:
             # If get_command_center_coordinates returns None, None
             # This means we've probably lost our command center and need to build a new one
@@ -1602,9 +1809,14 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
-            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
-               self.last_supply_depot_built_step = self.actual_root_level_steps_taken
-               if self.cc_y.any():
+
+            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and not _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+
+            elif self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                self.last_supply_depot_built_step = self.actual_root_level_steps_taken
+                if self.cc_y.any():
                     x_padding = random.randint(-30, 30)
                     y_padding = random.randint(-30, 30)
                     target_x = round(self.cc_x.mean()) - 35 + x_padding
@@ -1623,6 +1835,12 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Trying to build a supply depot at:", target)
                     return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
+            elif self.intended_action == ACTION_BUILD_BARRACKS and not _BUILD_BARRACKS in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+                # for action in obs.observation.available_actions:
+                #     print(actions.FUNCTIONS[action])
+
             elif self.intended_action == ACTION_BUILD_BARRACKS and _BUILD_BARRACKS in obs.observation['available_actions']:
                 # print("Obs actions are:", obs.observation['available_actions'])
                 # print("Inside BUILD_BARRACKS")
@@ -1641,7 +1859,7 @@ class DQNAgent(base_agent.BaseAgent):
                         target[0], 83 - BORDER_PADDING))
                     target[1] = max(BORDER_PADDING, min(
                         target[1], 83 - BORDER_PADDING))
-                    
+
                     # print("Trying to build barracks at: ", target)
 
                     # print("Trying to build a barracks at:", target)
@@ -1658,9 +1876,12 @@ class DQNAgent(base_agent.BaseAgent):
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
             # start of boiler plate code (small modifications like delay timers)
-            elif smart_action == ACTION_BUILD_MARINE:
-                if _TRAIN_MARINE in obs.observation['available_actions']:
-                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+            elif self.intended_action == ACTION_BUILD_MARINE and not _TRAIN_MARINE in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+
+            elif self.intended_action == ACTION_BUILD_MARINE and _TRAIN_MARINE in obs.observation['available_actions']:
+                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
 
             elif smart_action == ACTION_ATTACK:
                 do_it = True
@@ -1705,7 +1926,7 @@ class DQNAgent(base_agent.BaseAgent):
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 
             self.intended_action = None
-    
+
         elif self.move_number == 2:
             self.move_number = 0
 
@@ -1728,8 +1949,6 @@ class DQNAgent(base_agent.BaseAgent):
 
         return actions.FunctionCall(_NO_OP, [])
 
-    # end of boiler plate code
-
 def main(args):
     agent1 = DQNAgent()
     agent1_name = "Abel_AI" 

commit adb48364a6991e8ddfbb86af4f4bfd581e8f716f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Sep 22 14:46:54 2023 -0400

    Stochastic replay buffer sampling and formatting changes

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 0f0cb3a7..15a9c862 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -69,6 +69,8 @@ _TRAINING_GUARDRAILS_ENABLED = False
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
+
+
 class CustomRestartException(Exception):
     pass
 
@@ -78,6 +80,7 @@ class CustomRestartException(Exception):
 # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
 # His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
 
+
 ################################## Start of BoilerPlate Code #####################################################
 _NO_OP = actions.FUNCTIONS.no_op.id
 _SELECT_POINT = actions.FUNCTIONS.select_point.id
@@ -318,7 +321,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.00075, gamma=0.90, e_greedy=0.25, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.00075, gamma=0.90, e_greedy=0.15, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -420,7 +423,7 @@ class DQNModel(nn.Module):
         # Not using dropout as it loses our limited/sparse data (unfortunately...)
         # Overfitting hasn't been an issue with this particular setup
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(5, 64), 
+            nn.Linear(5, 64),
             nn.LeakyReLU(),
             nn.Linear(64, 128),
             nn.LeakyReLU(),
@@ -430,20 +433,23 @@ class DQNModel(nn.Module):
             nn.LeakyReLU(),
             nn.Linear(32, 16),
             nn.LeakyReLU(),
-        ).to(self.device) # Moving to GPU if available
+        ).to(self.device)  # Moving to GPU if available
 
         # Decision-making LSTM pass-through (takes concatenated, processed outputs from CNN and FCN, hits the LSTM, then to a single 4k->action_length layer)
         # Offloads to GPU if available
-        self.lstm_decision = nn.LSTM(input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
-        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(self.device)  # to get action probabilities/scores
-
+        self.lstm_decision = nn.LSTM(
+            input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
+        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(
+            self.device)  # to get action probabilities/scores
 
         self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with a static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=5000)
+        # Using a min of 0.00015 to avoid zeroing out and regressing...
+        self.scheduler = CosineAnnealingLR(
+            self.optimizer, T_max=5000, eta_min=0.00015)
 
     #
     def _get_conv_out(self, shape):
@@ -461,10 +467,12 @@ class DQNModel(nn.Module):
             non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
-        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)  # LSTM requires a sequence dimension...
+        # LSTM requires a sequence dimension...
+        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)
 
         lstm_out, _ = self.lstm_decision(combined)
-        lstm_out = lstm_out.squeeze(1)  # And now we remove the sequence dimension
+        # And now we remove the sequence dimension
+        lstm_out = lstm_out.squeeze(1)
 
         return self.fc_after_lstm(lstm_out)
 
@@ -500,12 +508,12 @@ class DQNModel(nn.Module):
         # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
         #####
 
-
     # This function goes back and tweaks the rewards associated with a given game
     # Based on the final tangible reward we get from PySC2
     # Win/loss/draw are multipliers
     # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
     # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
+
     def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
         # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
         steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
@@ -529,14 +537,43 @@ class DQNModel(nn.Module):
         print("Replay buffer currently has: ",
               len(self.training_buffer), "entries")
 
-    # # sample transitions from replay buffer queue for the last game
-    # def sample(self, batch_size):
-    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
-    #     return list(self.buffer)[-batch_size:]
+    # # Sample random transitions from replay buffer
+    # def random_sample(self, batch_size):
+        # return random_sample(self.training_buffer, batch_size)
 
-    # Sample random transitions from replay buffer
     def random_sample(self, batch_size):
-        return random_sample(self.training_buffer, batch_size)
+        """
+        This function implements a biased random sampling strategy.
+        Instead of uniformly sampling the replay buffer, we introduce a bias
+        towards more recent experiences. This has an effect similar to a
+        stochastic prioritization, where more recent experiences have a
+        higher probability of being sampled compared to older experiences.
+
+        The intuition behind this is that more recent experiences might be
+        more relevant to the current policy, and therefore might benefit
+        the agent more during training.
+        """
+
+        # Determine the count for recent experiences
+        # Here, we consider the most recent 20% of the replay buffer.
+        recent_count = int(0.2 * len(self.training_buffer))
+
+        # We take 30% of our batch size from recent experiences
+        # and the rest from the older experiences.
+        num_from_recent = int(0.3 * batch_size)
+        num_from_older = batch_size - num_from_recent
+
+        # Sample from the recent experiences segment.
+        # Larger index values are newer (it's copied directly from a queue, where the newest entries are the last)
+        recent_samples = random.sample(
+            self.training_buffer[-recent_count:], num_from_recent)
+
+        # Sample from the older experiences segment.
+        older_samples = random.sample(
+            self.training_buffer[:-recent_count], num_from_older)
+
+        # Combine and return the samples from both segments.
+        return recent_samples + older_samples
 
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
     # Implementation that Steven Brown (PySC2 Dev) created here:
@@ -552,11 +589,11 @@ class DQNModel(nn.Module):
 
         # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
         self.eval()
-        
+
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(
             non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
-        
+
         # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
@@ -564,21 +601,25 @@ class DQNModel(nn.Module):
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
-            available_actions = [a for a in self.actions if a not in excluded_actions]
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
             # print("Available actions are:", available_actions)
             action = np.random.choice(available_actions)
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
-            conv_output = conv_output.view(conv_output.size(0), -1)  # No need for the third dimension anymore
+            # No need for the third dimension anymore
+            conv_output = conv_output.view(conv_output.size(0), -1)
 
             # Pass the non-spatial data through its layers
             # print("Shape of non_spatial_data_tensor:", non_spatial_data_tensor.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1)  # No need for the third dimension
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1)  # No need for the third dimension
 
             # Concatenate the two outputs along the second dimension (feature axis)
-            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(1)  # Adding sequence dimension for LSTM
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(
+                1)  # Adding sequence dimension for LSTM
             # print("------------Combined tensor shape:", combined_output.shape)
 
             # Pass through LSTM and subsequent fully connected layer
@@ -789,8 +830,8 @@ class DQNModel(nn.Module):
     def get_units_by_type(self, obs, unit_type):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
-    
     # Provide the x/y coordinates of our command center(s)
+
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
@@ -861,7 +902,8 @@ class DQNModel(nn.Module):
         offset = 0
         if non_spatial_data:
             for key, value in non_spatial_data.items():
-                fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
+                # Input our metric in the first tuple with appropriate padding after
+                fixed_length_units[offset] = [value, 0, 0, 0, 0]
                 offset += 1
                 if offset >= 18:  # Adjust for a maximum 16 non-spatial data entries
                     break
@@ -878,7 +920,8 @@ class DQNModel(nn.Module):
 
         # Then store the units information
         # Start after the reserved entries
-        for i in range(min(len(units), 300 - offset)):  # Adjusted based on the reserved non-spatial entries
+        # Adjusted based on the reserved non-spatial entries
+        for i in range(min(len(units), 300 - offset)):
             unit = units[i]
             index = i + offset  # Index into fixed_length_units
             fixed_length_units[index, 0] = unit[UNIT_TYPE_INDEX]
@@ -887,9 +930,11 @@ class DQNModel(nn.Module):
 
             # Transform x, y coordinates as required for normalization
             if not base_top_left:
-                fixed_length_units[index, 3], fixed_length_units[index, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index,
+                                                                 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
             else:
-                fixed_length_units[index, 3], fixed_length_units[index, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index,
+                                                                 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
 
         return fixed_length_units
 
@@ -973,8 +1018,8 @@ class DQNAgent(base_agent.BaseAgent):
         self.dqn_model = self.target_network.to(self.dqn_model.device)
         self.target_network = self.target_network.to(self.dqn_model.device)
 
-    
     # We identify the current per-step reward based on in-game score and normalize it
+
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
@@ -1036,27 +1081,30 @@ class DQNAgent(base_agent.BaseAgent):
         # Trying to encourage attacking with a reasonably large group versus one at a time...
         max_army_size = 16
         # Calculate the reward accelerator based on army size. This should give a value between 0.15 and 1.
-        army_size_accelerator = 0.15 + (0.85 * min(max_army_size, non_spatial_metrics['army_count']) / max_army_size)
+        army_size_accelerator = 0.15 + \
+            (0.85 * min(max_army_size,
+             non_spatial_metrics['army_count']) / max_army_size)
 
         # Include the predefined rewards for the specific action
         action_reward += self.dqn_model.action_rewards.get(previous_action, 0)
 
         # Apply the accelerator ONLY to the attack actions that are not penalties
-        attack_actions_without_penalty = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
-        non_penalty_attack_actions = list(set(attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
+        # 'attackunit' and 'attack_x_x' actions
+        attack_actions_without_penalty = [4] + list(range(10, 26))
+        non_penalty_attack_actions = list(set(
+            attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
         # print("Non penalty actions are:" , non_penalty_attack_actions)
         if previous_action in non_penalty_attack_actions:
             action_reward *= army_size_accelerator
             # print("attack action-reward is: ", action_reward)
 
-
         # Calculate the total normalized reward by combining the normalized_score and action_reward
         # Clamp it between -1 and 1 as the LSTM performs better due to its use of tahn
         # We'll min/max this once again after further rewards below
         total_reward = min(max(normalized_score + action_reward, -1), 1)
 
         # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
-    
+
         # Check repetitiveness in the last_five_actions
         # Essentially, penalize reptition unless it's for building units (marines or SCVs)
         action_counts = Counter(last_five_actions)
@@ -1075,7 +1123,8 @@ class DQNAgent(base_agent.BaseAgent):
         elif previous_action == 1:
             if non_spatial_metrics['supply_free'] < 4:
                 # Reward for building supply during starvation
-                total_reward = camera_modifier * (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
+                total_reward = camera_modifier * \
+                    (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
             elif non_spatial_metrics['supply_free'] > 16:
                 # Reduced reward for building a supply depot if there's more than 16 extra supply
                 total_reward = camera_modifier * -0.25
@@ -1097,9 +1146,10 @@ class DQNAgent(base_agent.BaseAgent):
             total_reward = -1
 
         # Ensure that if the camera is moved, de-incentivize any camera action that is not 6 (return to home base)
-        if non_spatial_metrics['camera_location'] in [7, 8, 9] and previous_action in [7, 8, 9]:  # 6 is home base and is excluded
+        # 6 is home base and is excluded
+        if non_spatial_metrics['camera_location'] in [7, 8, 9] and previous_action in [7, 8, 9]:
             total_reward = -1
-        
+
         # Clamp the final total_reward between -1 and 1
         total_reward = max(min(total_reward, 1), -1)
 
@@ -1113,13 +1163,13 @@ class DQNAgent(base_agent.BaseAgent):
         #
         # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         # if total_reward < 1:
-            # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         #
         # Return the final total_reward
         return total_reward
 
-    
     # BOILER PLATE CODE AGAIN
+
     def transformLocation(self, x, y):
         # Debug
         # print("Before transforming, x and y are set to: ", x , y)
@@ -1152,7 +1202,7 @@ class DQNAgent(base_agent.BaseAgent):
             smart_action, x, y = smart_action.split('_')
 
         return (smart_action, x, y)
-    
+
     # CUSTOM
 
     # This function checks to see if we can add more marines to our build queue
@@ -1166,13 +1216,12 @@ class DQNAgent(base_agent.BaseAgent):
                 if build_queue_length < 5:  # Assuming 5 is the max length.
                     return True
         # print("build queue is: ", obs.observation.build_queue)
-        return False  
-    
+        return False
+
     # A simple normalize function
     def normalize(self, value, min_value, max_value):
         return (value - min_value) / (max_value - min_value)
 
-
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1247,7 +1296,6 @@ class DQNAgent(base_agent.BaseAgent):
                       self.in_game_training_iterations)
                 print("Training the model after game completion...")
 
-
             # Is our buffer large enough to begin training...
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
@@ -1271,11 +1319,12 @@ class DQNAgent(base_agent.BaseAgent):
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar('Average Training Time in Seconds/train_duration',
                                       avg_training_time, self.dqn_model.global_training_steps)
-                
+
                 # Update our TensorBoard per-step Reward Metrics
                 # Takes an average of the last 100 games (queue length = 100)
                 self.total_episode_rewards.append(self.total_episode_reward)
-                average_total_episode_rewards = sum(self.total_episode_rewards) / len(self.total_episode_rewards)
+                average_total_episode_rewards = sum(
+                    self.total_episode_rewards) / len(self.total_episode_rewards)
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar(
                         'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
@@ -1290,7 +1339,7 @@ class DQNAgent(base_agent.BaseAgent):
                                 'Per-Step Rewards', np.array(self.episode_rewards), self.dqn_model.global_training_steps)
                     except Exception as e:
                         print(f"Error logging Reward Histogram: {e}")
-        
+
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
             self.previous_avg_reward = avg_reward
@@ -1314,10 +1363,12 @@ class DQNAgent(base_agent.BaseAgent):
 
             # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
             if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
-                print("----Restarting underlying SC2 Environment for Crash Avoidance!!----")
+                print(
+                    "----Restarting underlying SC2 Environment for Crash Avoidance!!----")
                 # Raising a custom exception to get caught in our main PySC2 run_loop to trigger restart
                 # This took...a lot of trial and error :(
-                raise CustomRestartException("Time to restart the environment!")
+                raise CustomRestartException(
+                    "Time to restart the environment!")
 
             return actions.FunctionCall(_NO_OP, [])
 
@@ -1422,39 +1473,39 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("supply_depot_recently_built = False" )
 
         non_spatial_metrics = {
-            'minerals' : self.minerals,
-            'supply_limit' : supply_limit,
-            'supply_used' : supply_used,
-            'supply_free' : supply_free,
-            'army_supply' : army_supply,
-            'worker_supply' : worker_supply,
-            'cc_count' : command_center_count,
-            'barracks_count' : barracks_count,
-            'supply_depot_count' : supply_depot_count,
-            'idle_worker_count' : idle_worker_count,
-            'army_count' : army_count,
-            'home_base_camera' : self.last_camera_action == 6,
-            'barracks_available_queue' : barracks_available_build_queue,
-            'supply_depot_recently_built' : supply_depot_recently_built,
-            'camera_location' : self.last_camera_action,
-            'game_step_progress' : self.normalize(self.actual_root_level_steps_taken, 1, 1500),
-            'game_score' : self.normalize(obs.observation.score_cumulative.score, 1, 12000),
-            'previous_action' : self.previous_action or 0 # Prevents NaN from being sent to the model, breaking things
+            'minerals': self.minerals,
+            'supply_limit': supply_limit,
+            'supply_used': supply_used,
+            'supply_free': supply_free,
+            'army_supply': army_supply,
+            'worker_supply': worker_supply,
+            'cc_count': command_center_count,
+            'barracks_count': barracks_count,
+            'supply_depot_count': supply_depot_count,
+            'idle_worker_count': idle_worker_count,
+            'army_count': army_count,
+            'home_base_camera': self.last_camera_action == 6,
+            'barracks_available_queue': barracks_available_build_queue,
+            'supply_depot_recently_built': supply_depot_recently_built,
+            'camera_location': self.last_camera_action,
+            'game_step_progress': self.normalize(self.actual_root_level_steps_taken, 1, 1500),
+            'game_score': self.normalize(obs.observation.score_cumulative.score, 1, 12000),
+            # Prevents NaN from being sent to the model, breaking things
+            'previous_action': self.previous_action or 0
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
         # print("Non-spatial barracks count is: ", non_spatial_metrics["barracks_count"])
 
-
         current_state = {
             "non_spatial": np.zeros(300),
             "rgb_minimap": None
         }
         current_state["non_spatial"] = self.dqn_model.transform_non_spatial_to_fixed_length(
-            visible_units, self.base_top_left,non_spatial_metrics, self.last_five_actions)
+            visible_units, self.base_top_left, non_spatial_metrics, self.last_five_actions)
         current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
             rgb_minimap, self.base_top_left)
-        
+
         # print("The output of our spatial data is set to: ", )
         # for i in range(30):
         #     five_tuple = current_state["non_spatial"][i]
@@ -1492,8 +1543,6 @@ class DQNAgent(base_agent.BaseAgent):
             [45.  1. 45. 12. 27.]
         '''
 
-
-
         # Push s/a/r/s_next our replay buffer
         # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
         if self.previous_action is not None:
@@ -1598,17 +1647,17 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
 
-            # This section of the code aims to prevent the bot from repeatedly taking the same action over and over again, 
+            # This section of the code aims to prevent the bot from repeatedly taking the same action over and over again,
             # which can result in an undesirable behavior like doing nothing for an extended period.
 
             # If the last five actions taken by the bot are identical...
-            if len(set(self.last_five_actions)) == 1:  
+            if len(set(self.last_five_actions)) == 1:
 
                 # Retrieve the action that was repeated
                 action_to_exclude = self.last_five_actions[0]
 
                 # Check if this action isn't already in the list of excluded actions
-                # and also ensure we aren't excluding too many actions, which could lead to potential crashes 
+                # and also ensure we aren't excluding too many actions, which could lead to potential crashes
                 # or undesirable behaviors if no actions are left to choose from.
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
@@ -1618,7 +1667,8 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.clear()
 
             # DQN Selects the action
-            rl_action = self.dqn_model.choose_action(current_state, excluded_actions)
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
 
             self.previous_state = current_state
             self.previous_action = rl_action
@@ -1768,8 +1818,8 @@ class DQNAgent(base_agent.BaseAgent):
                 self.move_number = 0
 
             elif self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
-               self.last_supply_depot_built_step = self.actual_root_level_steps_taken
-               if self.cc_y.any():
+                self.last_supply_depot_built_step = self.actual_root_level_steps_taken
+                if self.cc_y.any():
                     x_padding = random.randint(-30, 30)
                     y_padding = random.randint(-30, 30)
                     target_x = round(self.cc_x.mean()) - 35 + x_padding
@@ -1812,7 +1862,7 @@ class DQNAgent(base_agent.BaseAgent):
                         target[0], 83 - BORDER_PADDING))
                     target[1] = max(BORDER_PADDING, min(
                         target[1], 83 - BORDER_PADDING))
-                    
+
                     # print("Trying to build barracks at: ", target)
 
                     # print("Trying to build a barracks at:", target)
@@ -1834,7 +1884,7 @@ class DQNAgent(base_agent.BaseAgent):
                 self.move_number = 0
 
             elif self.intended_action == ACTION_BUILD_MARINE and _TRAIN_MARINE in obs.observation['available_actions']:
-                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+                return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
 
             elif smart_action == ACTION_ATTACK:
                 do_it = True
@@ -1879,7 +1929,7 @@ class DQNAgent(base_agent.BaseAgent):
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 
             self.intended_action = None
-    
+
         elif self.move_number == 2:
             self.move_number = 0
 
@@ -1934,7 +1984,8 @@ def main(args):
                 map_name="Simple64",
                 players=[
                     sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
-                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TRAINING_TYPE == "bot" else sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
+                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TRAINING_TYPE == "bot" else sc2_env.Agent(
+                        sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,
@@ -1961,5 +2012,6 @@ def main(args):
             # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
             continue
 
+
 if __name__ == "__main__":
     app.run(main)

commit 11fce77fc436a0bf65f2cd34c33d7a26b8ce9a99
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Sep 22 11:59:01 2023 -0400

    New histograms, reward refactor, camera fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 5189c4e2..0f0cb3a7 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -109,9 +109,9 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-b.pt_episode_200_reward_-1.00.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-c.pt_episode_400_reward_-1.00.pt'
 # Specify Naming Scheme of Future Model Checkpoints
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-b.pt'
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-c.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -318,7 +318,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.70, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.00075, gamma=0.90, e_greedy=0.25, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -345,7 +345,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-b-very_easy-bot-NGR'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-c-very_easy-bot-NGR'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
@@ -443,7 +443,7 @@ class DQNModel(nn.Module):
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with a static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10000)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=5000)
 
     #
     def _get_conv_out(self, shape):
@@ -789,136 +789,7 @@ class DQNModel(nn.Module):
     def get_units_by_type(self, obs, unit_type):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
-    # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
-        # Extract the cumulative score from the observation
-        score = obs.observation.score_cumulative.score
-        # Anything above 20k score results in a full score being provided to the model
-        max_score = 20000
-        # Normalize the score to be between 0 and 1 (We use -1 to 1 later)
-        normalized_score = min(score / max_score, 1)
-
-        # --------------------
-        # Action Mapping Reference from self.action_rewards{}
-        # 0: 'donothing'
-        # 1: 'buildsupplydepot'
-        # 2: 'buildbarracks'
-        # 3: 'buildmarine'
-        # 4: 'attackunit'
-        # 5: 'buildscv'
-        # 6: 'resetcamera'
-        # 7: 'mcselfexpansion'
-        # 8: 'mcenemyexpansion'
-        # 9: 'mcenemyprimary'
-        # 10: 'attack_4_4'
-        # 11: 'attack_12_4'
-        # 12: 'attack_4_12'
-        # 13: 'attack_12_12'
-        # 14: 'attack_36_4'
-        # 15: 'attack_44_4'
-        # 16: 'attack_36_12'
-        # 17: 'attack_44_12'
-        # 18: 'attack_4_36'
-        # 19: 'attack_12_36'
-        # 20: 'attack_4_44'
-        # 21: 'attack_12_44'
-        # 22: 'attack_36_36'
-        # 23: 'attack_44_36'
-        # 24: 'attack_36_44'
-        # 25: 'attack_44_44'
-        # --------------------
-
-        # Incentive multipliers for attack logic
-        attack_opposite_quadrant = 0.5
-        attack_adjacent_quadrant = 0.25
-        penalty_home_quadrant = -1
-        penalty_home_expansion = -0.5
-
-        action_reward = 0
-
-        # Incentivize attacks to opposite quadrant & their expansion
-        # Also de-incentivize attacking home base & expansion
-        if previous_action in self.bottom_right_actions:
-            action_reward += attack_opposite_quadrant
-        elif previous_action in self.bottom_left_actions:
-            action_reward += attack_adjacent_quadrant
-        elif previous_action in self.top_left_actions:
-            action_reward += penalty_home_quadrant
-        elif previous_action in self.top_right_actions:
-            action_reward += penalty_home_expansion
-
-        # Armies of 16 units and over will get the full reward
-        # Trying to encourage attacking with a reasonably large group versus one at a time...
-        max_army_size = 16
-        # Calculate the reward accelerator based on army size. This should give a value between 0.15 and 1.
-        army_size_accelerator = 0.15 + (0.85 * min(max_army_size, non_spatial_metrics['army_count']) / max_army_size)
-
-        # Include the predefined rewards for the specific action
-        action_reward += self.action_rewards.get(previous_action, 0)
-
-        # Apply the accelerator ONLY to the attack actions that are not penalties
-        attack_actions_without_penalty = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
-        non_penalty_attack_actions = list(set(attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
-        # print("Non penalty actions are:" , non_penalty_attack_actions)
-        if previous_action in non_penalty_attack_actions:
-            action_reward *= army_size_accelerator
-            # print("attack action-reward is: ", action_reward)
-
-
-        # Calculate the total normalized reward by combining the normalized_score and action_reward
-        # Clamp it between -1 and 1 as the LSTM performs better due to its use of tahn
-        total_reward = min(max(normalized_score + action_reward, -1), 1)
-
-        # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
     
-        # Check repetitiveness in the last_five_actions
-        # Essentially, penalize reptition unless it's for building units (marines or SCVs)
-        action_counts = Counter(last_five_actions)
-        for action, count in action_counts.items():
-            if action not in [3, 5] and count >= 3:
-                total_reward = 0
-
-        # Define a modifier based on the camera position
-        camera_modifier = 1 if non_spatial_metrics['home_base_camera'] else 0.5
-
-        # Special Reward handling to ensure Marines are only being built when there is available build_queue
-        if previous_action == 3 and not non_spatial_metrics['barracks_available_queue']:
-            total_reward = 0
-        # Special Reward Handling for supply depot construction and supply starvation
-        elif previous_action == 1:
-            if non_spatial_metrics['supply_free'] < 4:
-                # Reward for building supply during starvation
-                total_reward = camera_modifier * (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
-            elif non_spatial_metrics['supply_free'] > 16:
-                # Zero reward for building a supply depot if there's more than 16 extra supply
-                total_reward = camera_modifier * -0.25
-        # Ensure no reward for unit construction when supply is zero
-        elif (previous_action == 3 or previous_action == 5) and non_spatial_metrics['supply_free'] <= 0:
-            total_reward = 0
-        # Reward Shaping for SCV construction (reduced after 16)
-        elif previous_action == 5 and non_spatial_metrics['worker_supply'] > 16:
-            over_scv = non_spatial_metrics['worker_supply'] - 16
-            total_reward -= camera_modifier * 0.25 * over_scv
-        # Reward Shaping for Barracks construction (reduced after 6)
-        elif non_spatial_metrics['barracks_count'] > 6:
-            over_barracks = non_spatial_metrics['barracks_count'] - 6
-            total_reward -= camera_modifier * 0.25 * over_barracks
-
-        # Update our TensorBoard per-step Reward Metrics
-        self.total_episode_reward += total_reward
-        # Update our Tensorboard per-Game Reward Histogram (if necessary - sampled every 25 games as it's expensive)
-        if self.episode_count % 25 == 0:
-                self.episode_rewards.append(total_reward)
-
-        # DEBUGS
-        #
-        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
-        # if total_reward < 1:
-            # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
-        #
-        # Return the final total_reward
-        return total_reward
-
     # Provide the x/y coordinates of our command center(s)
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
@@ -1102,6 +973,152 @@ class DQNAgent(base_agent.BaseAgent):
         self.dqn_model = self.target_network.to(self.dqn_model.device)
         self.target_network = self.target_network.to(self.dqn_model.device)
 
+    
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything above 20k score results in a full score being provided to the model
+        max_score = 20000
+        # Normalize the score to be between 0 and 1 (We use -1 to 1 later)
+        normalized_score = min(score / max_score, 1)
+
+        # --------------------
+        # Action Mapping Reference from self.action_rewards{}
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 0.5
+        attack_adjacent_quadrant = 0.25
+        penalty_home_quadrant = -1
+        penalty_home_expansion = -0.5
+
+        action_reward = 0
+
+        # Incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if previous_action in self.bottom_right_actions:
+            action_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            action_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            action_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            action_reward += penalty_home_expansion
+
+        # Armies of 16 units and over will get the full reward
+        # Trying to encourage attacking with a reasonably large group versus one at a time...
+        max_army_size = 16
+        # Calculate the reward accelerator based on army size. This should give a value between 0.15 and 1.
+        army_size_accelerator = 0.15 + (0.85 * min(max_army_size, non_spatial_metrics['army_count']) / max_army_size)
+
+        # Include the predefined rewards for the specific action
+        action_reward += self.dqn_model.action_rewards.get(previous_action, 0)
+
+        # Apply the accelerator ONLY to the attack actions that are not penalties
+        attack_actions_without_penalty = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
+        non_penalty_attack_actions = list(set(attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
+        # print("Non penalty actions are:" , non_penalty_attack_actions)
+        if previous_action in non_penalty_attack_actions:
+            action_reward *= army_size_accelerator
+            # print("attack action-reward is: ", action_reward)
+
+
+        # Calculate the total normalized reward by combining the normalized_score and action_reward
+        # Clamp it between -1 and 1 as the LSTM performs better due to its use of tahn
+        # We'll min/max this once again after further rewards below
+        total_reward = min(max(normalized_score + action_reward, -1), 1)
+
+        # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
+    
+        # Check repetitiveness in the last_five_actions
+        # Essentially, penalize reptition unless it's for building units (marines or SCVs)
+        action_counts = Counter(last_five_actions)
+        for action, count in action_counts.items():
+            if action not in [3, 5] and count >= 3:
+                total_reward = 0
+
+        # Define a modifier based on the camera position
+        camera_modifier = 1 if non_spatial_metrics['home_base_camera'] else 0.5
+        # print("camera modifier is set to:", camera_modifier, "and last camera action is: ", non_spatial_metrics['camera_location']  )
+
+        # Special Reward handling to ensure Marines are only being built when there is available build_queue
+        if previous_action == 3 and not non_spatial_metrics['barracks_available_queue']:
+            total_reward = 0
+        # Special Reward Handling for supply depot construction and supply starvation
+        elif previous_action == 1:
+            if non_spatial_metrics['supply_free'] < 4:
+                # Reward for building supply during starvation
+                total_reward = camera_modifier * (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
+            elif non_spatial_metrics['supply_free'] > 16:
+                # Reduced reward for building a supply depot if there's more than 16 extra supply
+                total_reward = camera_modifier * -0.25
+        # Ensure no reward for unit construction when supply is zero
+        elif (previous_action == 3 or previous_action == 5) and non_spatial_metrics['supply_free'] <= 0:
+            total_reward = 0
+        # Reward Shaping for SCV construction (reduced after 16)
+        elif previous_action == 5 and non_spatial_metrics['worker_supply'] > 16:
+            over_scv = non_spatial_metrics['worker_supply'] - 16
+            total_reward -= camera_modifier * 0.25 * over_scv
+        # Reward Shaping for Barracks construction (reduced after 6)
+        elif non_spatial_metrics['barracks_count'] > 6:
+            over_barracks = non_spatial_metrics['barracks_count'] - 6
+            total_reward -= camera_modifier * 0.25 * over_barracks
+
+        # Penalize certain camera movements if barracks_count is less than 3
+        # Agent should not waste cycles if we haven't built out the base yet
+        if non_spatial_metrics['barracks_count'] < 3 and previous_action in [7, 8, 9]:
+            total_reward = -1
+
+        # Ensure that if the camera is moved, de-incentivize any camera action that is not 6 (return to home base)
+        if non_spatial_metrics['camera_location'] in [7, 8, 9] and previous_action in [7, 8, 9]:  # 6 is home base and is excluded
+            total_reward = -1
+        
+        # Clamp the final total_reward between -1 and 1
+        total_reward = max(min(total_reward, 1), -1)
+
+        # Update our TensorBoard per-step Reward Metrics
+        self.total_episode_reward += total_reward
+        # print("self.total_episode_reward is set to:", self.total_episode_reward)
+        # Update our Tensorboard per-Game Reward Histogram (if necessary - sampled every 3 games as it's expensive)
+        self.episode_rewards.append(total_reward)
+
+        # DEBUGS
+        #
+        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        # if total_reward < 1:
+            # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        #
+        # Return the final total_reward
+        return total_reward
+
+    
     # BOILER PLATE CODE AGAIN
     def transformLocation(self, x, y):
         # Debug
@@ -1230,25 +1247,7 @@ class DQNAgent(base_agent.BaseAgent):
                       self.in_game_training_iterations)
                 print("Training the model after game completion...")
 
-            # Update our TensorBoard per-step Reward Metrics
-            # Takes an average of the last 100 games (queue length = 100)
-            self.total_episode_rewards.append(self.total_episode_reward)
-            average_total_episode_rewards = sum(self.total_episode_rewards) / len(self.total_episode_rewards)
-            with SummaryWriter(self.dqn_model.writer_path) as writer:
-                writer.add_scalar(
-                    'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
-            # Update our reward histogram every 25 games (sampled due to expense)
-            # In try/except block as these routinely fail/crash simulation
-            # Update our reward histogram every 25 games (sampled due to expense)
-            if self.episode_count % 25 == 0:
-                try:
-                    with SummaryWriter(self.writer_path) as writer:
-                        # This will log the distribution of rewards per game step
-                        writer.add_histogram(
-                            'Per-Step Rewards', np.array(self.episode_rewards), self.global_training_steps)
-                except Exception as e:
-                    print(f"Error logging Reward Histogram: {e}")
-    
+
             # Is our buffer large enough to begin training...
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
@@ -1272,7 +1271,26 @@ class DQNAgent(base_agent.BaseAgent):
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar('Average Training Time in Seconds/train_duration',
                                       avg_training_time, self.dqn_model.global_training_steps)
-
+                
+                # Update our TensorBoard per-step Reward Metrics
+                # Takes an average of the last 100 games (queue length = 100)
+                self.total_episode_rewards.append(self.total_episode_reward)
+                average_total_episode_rewards = sum(self.total_episode_rewards) / len(self.total_episode_rewards)
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar(
+                        'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
+                # Update our reward histogram every 3 games (sampled due to expense)
+                # In try/except block as these routinely fail/crash simulation
+                # Update our reward histogram every 3 games (sampled due to expense)
+                if self.episode_count % 3 == 0:
+                    try:
+                        with SummaryWriter(self.dqn_model.writer_path) as writer:
+                            # This will log the distribution of rewards per game step
+                            writer.add_histogram(
+                                'Per-Step Rewards', np.array(self.episode_rewards), self.dqn_model.global_training_steps)
+                    except Exception as e:
+                        print(f"Error logging Reward Histogram: {e}")
+        
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
             self.previous_avg_reward = avg_reward
@@ -1289,9 +1307,9 @@ class DQNAgent(base_agent.BaseAgent):
             for _ in range(5):
                 self.last_five_actions.append(None)
             # TensorBoard Per-Step Reward Metrics
-            # Cumulative Counter for single game
+            # Resetting Cumulative Counter for single game
             self.total_episode_reward = 0
-            # List of each reward, displayed as a histogram (sampled every 25 games infrequently)
+            # Resetting List of each reward, displayed as a histogram (sampled every 25 games infrequently)
             self.episode_rewards = []
 
             # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
@@ -1480,9 +1498,9 @@ class DQNAgent(base_agent.BaseAgent):
         # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
         if self.previous_action is not None:
             self.actual_root_level_steps_taken += 1
-            # print("Pushing to the replay buffer: ", self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics))
+            # print("Pushing to the replay buffer: ", self.previous_action, self.get_normalized_reward(obs, self.previous_action, non_spatial_metrics))
             self.dqn_model.store_transition(self.previous_state,
-                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics, self.last_five_actions), current_state)
+                                            self.previous_action, self.get_normalized_reward(obs, self.previous_action, non_spatial_metrics, self.last_five_actions), current_state)
 
         # Do in-game training of the model for every 100 root actions the agent takes
         if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
@@ -1619,6 +1637,11 @@ class DQNAgent(base_agent.BaseAgent):
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)
 
+            # Quick reset for first-step as bots have issues with camera state
+            if obs.first():
+                # Set our smart action to our base
+                smart_action == ACTION_RESET_CAMERA
+
             if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
                 unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
 

commit 9c371ea47dc434c61cf99ff7fab66d62cb4b9c90
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Sep 22 06:41:39 2023 -0400

    Large Reward and Logging Update

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 2461ff11..5189c4e2 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -40,6 +40,8 @@ import torch.optim as optim
 import torch.nn.functional as F
 # queue used for replay buffer
 from collections import deque
+# from action-reptition detection in our rewards queue (and tensorboard metrics...)
+from collections import Counter
 
 # Using TensorBoard for model performance tracking & visualizations
 from torch.utils.tensorboard import SummaryWriter
@@ -48,7 +50,6 @@ from torch.cuda.amp import autocast, GradScaler
 # Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
 # The static learning rate of 0.01 was...quite poor excellent.
 from torch.optim.lr_scheduler import CosineAnnealingLR
-from collections import Counter
 # Using itertools to slice deque's efficiently
 import itertools
 
@@ -108,7 +109,7 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-a.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-b.pt_episode_200_reward_-1.00.pt'
 # Specify Naming Scheme of Future Model Checkpoints
 _DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-b.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
@@ -564,6 +565,7 @@ class DQNModel(nn.Module):
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
             available_actions = [a for a in self.actions if a not in excluded_actions]
+            # print("Available actions are:", available_actions)
             action = np.random.choice(available_actions)
         else:
             # Pass the minimap data through the CNN
@@ -788,7 +790,7 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics):
+    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, last_five_actions):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
         # Anything above 20k score results in a full score being provided to the model
@@ -868,23 +870,45 @@ class DQNModel(nn.Module):
         total_reward = min(max(normalized_score + action_reward, -1), 1)
 
         # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
+    
+        # Check repetitiveness in the last_five_actions
+        # Essentially, penalize reptition unless it's for building units (marines or SCVs)
+        action_counts = Counter(last_five_actions)
+        for action, count in action_counts.items():
+            if action not in [3, 5] and count >= 3:
+                total_reward = 0
+
+        # Define a modifier based on the camera position
+        camera_modifier = 1 if non_spatial_metrics['home_base_camera'] else 0.5
 
         # Special Reward handling to ensure Marines are only being built when there is available build_queue
-        # Otherwise, agent spams it unnecessarily
-        if previous_action == 3:  
-            if not non_spatial_metrics['barracks_available_queue']:
-                return 0
+        if previous_action == 3 and not non_spatial_metrics['barracks_available_queue']:
+            total_reward = 0
         # Special Reward Handling for supply depot construction and supply starvation
-        elif previous_action == 1:  
+        elif previous_action == 1:
             if non_spatial_metrics['supply_free'] < 4:
                 # Reward for building supply during starvation
-                return 1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward
+                total_reward = camera_modifier * (1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward)
             elif non_spatial_metrics['supply_free'] > 16:
                 # Zero reward for building a supply depot if there's more than 16 extra supply
-                return -0.25
-        # Ensure we do not reward construction of units when supply is zero
-        elif previous_action == 3 or previous_action == 5 and non_spatial_metrics['supply_free'] <= 0:
-            return 0        
+                total_reward = camera_modifier * -0.25
+        # Ensure no reward for unit construction when supply is zero
+        elif (previous_action == 3 or previous_action == 5) and non_spatial_metrics['supply_free'] <= 0:
+            total_reward = 0
+        # Reward Shaping for SCV construction (reduced after 16)
+        elif previous_action == 5 and non_spatial_metrics['worker_supply'] > 16:
+            over_scv = non_spatial_metrics['worker_supply'] - 16
+            total_reward -= camera_modifier * 0.25 * over_scv
+        # Reward Shaping for Barracks construction (reduced after 6)
+        elif non_spatial_metrics['barracks_count'] > 6:
+            over_barracks = non_spatial_metrics['barracks_count'] - 6
+            total_reward -= camera_modifier * 0.25 * over_barracks
+
+        # Update our TensorBoard per-step Reward Metrics
+        self.total_episode_reward += total_reward
+        # Update our Tensorboard per-Game Reward Histogram (if necessary - sampled every 25 games as it's expensive)
+        if self.episode_count % 25 == 0:
+                self.episode_rewards.append(total_reward)
 
         # DEBUGS
         #
@@ -892,6 +916,7 @@ class DQNModel(nn.Module):
         # if total_reward < 1:
             # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         #
+        # Return the final total_reward
         return total_reward
 
     # Provide the x/y coordinates of our command center(s)
@@ -1040,6 +1065,14 @@ class DQNAgent(base_agent.BaseAgent):
         self.actual_root_level_steps_taken = 0
         self.in_game_training_iterations = 0
 
+        # TensorBoard Per-Step Reward Metrics
+        # Cumulative Counter for single game
+        self.total_episode_reward = 0
+        # A queue of the last 100 games (used for average tracking)
+        self.total_episode_rewards = deque([0] * 100, maxlen=100)
+        # List of each reward, displayed as a histogram (sampled every 25 games infrequently)
+        self.episode_rewards = []
+
         # Custom Delay Timers
         self.attack_delay_timer = 0
         self.unit_attack_delay_timer = 0
@@ -1196,9 +1229,27 @@ class DQNAgent(base_agent.BaseAgent):
                 print("Number of in-game model updates: ",
                       self.in_game_training_iterations)
                 print("Training the model after game completion...")
-            # Learn after every game, not just the successful ones:
 
-            # Log our reward over time and training duration if we've started training
+            # Update our TensorBoard per-step Reward Metrics
+            # Takes an average of the last 100 games (queue length = 100)
+            self.total_episode_rewards.append(self.total_episode_reward)
+            average_total_episode_rewards = sum(self.total_episode_rewards) / len(self.total_episode_rewards)
+            with SummaryWriter(self.dqn_model.writer_path) as writer:
+                writer.add_scalar(
+                    'Total Per-Step Shaped Reward/value', average_total_episode_rewards, self.dqn_model.global_training_steps)
+            # Update our reward histogram every 25 games (sampled due to expense)
+            # In try/except block as these routinely fail/crash simulation
+            # Update our reward histogram every 25 games (sampled due to expense)
+            if self.episode_count % 25 == 0:
+                try:
+                    with SummaryWriter(self.writer_path) as writer:
+                        # This will log the distribution of rewards per game step
+                        writer.add_histogram(
+                            'Per-Step Rewards', np.array(self.episode_rewards), self.global_training_steps)
+                except Exception as e:
+                    print(f"Error logging Reward Histogram: {e}")
+    
+            # Is our buffer large enough to begin training...
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
                 self.dqn_model.learn(self.target_network)
@@ -1206,7 +1257,7 @@ class DQNAgent(base_agent.BaseAgent):
                 self.training_time.append(training_end_time)
                 avg_training_time = sum(
                     self.training_time) / len(self.training_time)
-                # Update our target network every 5 games once training has begun
+                # Update our target DQN network every 5 games once training has begun
                 if self.episode_count % 5 == 0:
                     self.target_network.load_state_dict(
                         self.dqn_model.state_dict())
@@ -1237,6 +1288,11 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_camera_action = 0
             for _ in range(5):
                 self.last_five_actions.append(None)
+            # TensorBoard Per-Step Reward Metrics
+            # Cumulative Counter for single game
+            self.total_episode_reward = 0
+            # List of each reward, displayed as a histogram (sampled every 25 games infrequently)
+            self.episode_rewards = []
 
             # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
             if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
@@ -1426,7 +1482,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.actual_root_level_steps_taken += 1
             # print("Pushing to the replay buffer: ", self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics))
             self.dqn_model.store_transition(self.previous_state,
-                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics), current_state)
+                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics, self.last_five_actions), current_state)
 
         # Do in-game training of the model for every 100 root actions the agent takes
         if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:

commit 22d560a54aa3ab2255f2274905aa69693de71247
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Sep 21 21:17:39 2023 -0400

    state machine fixes for NGR

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 6c3bb07a..2461ff11 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -64,7 +64,7 @@ _TRAINING_TYPE = "bot"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
 _BOT_DIFFICULTY = "easy"
 # Enable/Disable Training Guardrails
-_TRAINING_GUARDRAILS_ENABLED = True
+_TRAINING_GUARDRAILS_ENABLED = False
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
@@ -108,9 +108,9 @@ ACTION_ATTACK = 'attack'
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 # Specify Previous Network-Compatable Weights:
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-d.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-22-a.pt'
 # Specify Naming Scheme of Future Model Checkpoints
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-a.pt'
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-b.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -317,7 +317,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.70, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -328,7 +328,7 @@ class DQNModel(nn.Module):
         self.final_epsilon = 0.01
         # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 1500000
+            self.epsilon - self.final_epsilon) / 1000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -344,7 +344,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-a-easy-rush-bot'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-b-very_easy-bot-NGR'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
@@ -882,6 +882,9 @@ class DQNModel(nn.Module):
             elif non_spatial_metrics['supply_free'] > 16:
                 # Zero reward for building a supply depot if there's more than 16 extra supply
                 return -0.25
+        # Ensure we do not reward construction of units when supply is zero
+        elif previous_action == 3 or previous_action == 5 and non_spatial_metrics['supply_free'] <= 0:
+            return 0        
 
         # DEBUGS
         #
@@ -1491,7 +1494,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (army_supply < 16):
+            if (len(enemy_units) == 0) or (army_supply < 10):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1512,11 +1515,11 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(8)
                 excluded_actions.append(9)
 
-            # modified original logic, waits for 16 marines before attacking
+            # modified original logic, waits for 12 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
+            if 4 not in excluded_actions or army_supply < 12 or self.attack_delay_timer < 5:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
@@ -1583,6 +1586,8 @@ class DQNAgent(base_agent.BaseAgent):
                 if barracks_y.any():
                     i = random.randint(0, len(barracks_y) - 1)
                     target = [barracks_x[i], barracks_y[i]]
+
+                    self.intended_action = ACTION_BUILD_MARINE
                     # Checks to avoid out-of-bounds crashing
                     # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
                     if 0 <= target[0] < 84 and 0 <= target[1] < 84:
@@ -1678,7 +1683,12 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
-            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+
+            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and not _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+
+            elif self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
                self.last_supply_depot_built_step = self.actual_root_level_steps_taken
                if self.cc_y.any():
                     x_padding = random.randint(-30, 30)
@@ -1699,6 +1709,12 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Trying to build a supply depot at:", target)
                     return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
+            elif self.intended_action == ACTION_BUILD_BARRACKS and not _BUILD_BARRACKS in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+                # for action in obs.observation.available_actions:
+                #     print(actions.FUNCTIONS[action])
+
             elif self.intended_action == ACTION_BUILD_BARRACKS and _BUILD_BARRACKS in obs.observation['available_actions']:
                 # print("Obs actions are:", obs.observation['available_actions'])
                 # print("Inside BUILD_BARRACKS")
@@ -1734,8 +1750,11 @@ class DQNAgent(base_agent.BaseAgent):
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
             # start of boiler plate code (small modifications like delay timers)
-            elif smart_action == ACTION_BUILD_MARINE:
-                if _TRAIN_MARINE in obs.observation['available_actions']:
+            elif self.intended_action == ACTION_BUILD_MARINE and not _TRAIN_MARINE in obs.observation['available_actions']:
+                # print("Resetting back to square one")
+                self.move_number = 0
+
+            elif self.intended_action == ACTION_BUILD_MARINE and _TRAIN_MARINE in obs.observation['available_actions']:
                     return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
 
             elif smart_action == ACTION_ATTACK:

commit 752961a36c1d5c0da46d92147fbb32d087603f29
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Sep 21 16:02:06 2023 -0400

    Significant increase in current_state[non-spatial] information

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index cfd922cf..6c3bb07a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -60,7 +60,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-_TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+_TRAINING_TYPE = "bot"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
 _BOT_DIFFICULTY = "easy"
 # Enable/Disable Training Guardrails
@@ -107,8 +107,10 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-c.pt'
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-21-d.pt'
+# Specify Previous Network-Compatable Weights:
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-d.pt'
+# Specify Naming Scheme of Future Model Checkpoints
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-22-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -119,7 +121,7 @@ ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
 ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
 ###### END OF GLOBAL CUSTOM CODE #####
 
-# Steven Brown also used smart_actions although for this bot I've expanded them considerably
+# Steven Brown created the PySC2 concept of smart_actions although for this bot I've expanded them considerably
 smart_actions = [
     ACTION_DO_NOTHING,
     ACTION_BUILD_SUPPLY_DEPOT,
@@ -342,7 +344,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v21-d-KaneAI'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v22-a-easy-rush-bot'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
@@ -786,7 +788,7 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, supply_depot_recently_built):
+    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
         # Anything above 20k score results in a full score being provided to the model
@@ -876,13 +878,14 @@ class DQNModel(nn.Module):
         elif previous_action == 1:  
             if non_spatial_metrics['supply_free'] < 4:
                 # Reward for building supply during starvation
-                return 1 if not supply_depot_recently_built else total_reward
+                return 1 if not non_spatial_metrics['supply_depot_recently_built'] else total_reward
             elif non_spatial_metrics['supply_free'] > 16:
                 # Zero reward for building a supply depot if there's more than 16 extra supply
                 return -0.25
 
         # DEBUGS
         #
+        # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         # if total_reward < 1:
             # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         #
@@ -930,7 +933,7 @@ class DQNModel(nn.Module):
         return transformed_minimap
 
     # This creates a fixed-length vector to store non-spatial data in (metrics and unit metadata)
-    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data):
+    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data, last_five_actions):
 
         # Safety checks
         # Check if units are None or empty
@@ -950,7 +953,6 @@ class DQNModel(nn.Module):
         Y_POS_INDEX = 13
 
         # Number of units and features
-        num_units = len(units)
         num_features = 5  # ['unit_type', 'health', 'x', 'y']
 
         # Create an empty array of shape (300, num_features)
@@ -962,9 +964,19 @@ class DQNModel(nn.Module):
             for key, value in non_spatial_data.items():
                 fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
                 offset += 1
-                if offset >= 13:  # Adjust for a maximum 13 non-spatial data entries
+                if offset >= 18:  # Adjust for a maximum 16 non-spatial data entries
                     break
 
+        # Embed the last_five_actions data into our vector
+        # First, replace None elements with 0 and flatten the list
+        actions_cleaned = [action or 0 for action in last_five_actions]
+        # Then, pad the cleaned actions
+        actions_padded = actions_cleaned + [0] * (5 - len(actions_cleaned))
+
+        # Embed the flattened and padded actions
+        fixed_length_units[offset, :5] = actions_padded
+        offset += 1  # Increment the offset by 1, since we've added only one row
+
         # Then store the units information
         # Start after the reserved entries
         for i in range(min(len(units), 300 - offset)):  # Adjusted based on the reserved non-spatial entries
@@ -1030,11 +1042,12 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        self.last_camera_action = None
+        self.last_camera_action = 0
         self.last_supply_depot_built_step = None
 
         # Queue that tracks last actions for exclusionary purposes
-        self.last_three_actions = deque(maxlen=3)
+        # The queue is also fed as current_state info back into the non_spatial FCN
+        self.last_five_actions = deque(maxlen=5)
         self.command_center = []
 
         # Action mapping
@@ -1087,6 +1100,8 @@ class DQNAgent(base_agent.BaseAgent):
 
         return (smart_action, x, y)
     
+    # CUSTOM
+
     # This function checks to see if we can add more marines to our build queue
     # Used to improve our reward system for the agent
     def has_room_in_build_queue(self, obs):
@@ -1098,11 +1113,13 @@ class DQNAgent(base_agent.BaseAgent):
                 if build_queue_length < 5:  # Assuming 5 is the max length.
                     return True
         # print("build queue is: ", obs.observation.build_queue)
-        return False
+        return False  
     
+    # A simple normalize function
+    def normalize(self, value, min_value, max_value):
+        return (value - min_value) / (max_value - min_value)
 
 
-    # CUSTOM
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1213,13 +1230,13 @@ class DQNAgent(base_agent.BaseAgent):
             supply_depot_recently_built = False
             self.last_supply_depot_built_step = None
             # Clear our action queue
-            self.last_three_actions.clear()
-            self.last_camera_action = None
-            for _ in range(3):
-                self.last_three_actions.append(None)
+            self.last_five_actions.clear()
+            self.last_camera_action = 0
+            for _ in range(5):
+                self.last_five_actions.append(None)
 
             # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
-            if self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0:
+            if (self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0) and (_TRAINING_TYPE != "bot"):
                 print("----Restarting underlying SC2 Environment for Crash Avoidance!!----")
                 # Raising a custom exception to get caught in our main PySC2 run_loop to trigger restart
                 # This took...a lot of trial and error :(
@@ -1313,9 +1330,20 @@ class DQNAgent(base_agent.BaseAgent):
 
         supply_free = supply_limit - supply_used
 
-        
+        # Checking to see if we have available build queue for agent state
         barracks_available_build_queue = self.has_room_in_build_queue(obs)
 
+        # We check to see if we need to do any reward shaping to avoid supply starvation...
+        supply_depot_recently_built = False
+        # print("Game step is: ", self.actual_root_level_steps_taken)
+
+        if self.last_supply_depot_built_step is not None:
+            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
+                # print("supply_depot_recently_built = True" )
+                supply_depot_recently_built = True
+            # else:
+                # print("supply_depot_recently_built = False" )
+
         non_spatial_metrics = {
             'minerals' : self.minerals,
             'supply_limit' : supply_limit,
@@ -1329,7 +1357,12 @@ class DQNAgent(base_agent.BaseAgent):
             'idle_worker_count' : idle_worker_count,
             'army_count' : army_count,
             'home_base_camera' : self.last_camera_action == 6,
-            'barracks_available_queue' : barracks_available_build_queue
+            'barracks_available_queue' : barracks_available_build_queue,
+            'supply_depot_recently_built' : supply_depot_recently_built,
+            'camera_location' : self.last_camera_action,
+            'game_step_progress' : self.normalize(self.actual_root_level_steps_taken, 1, 1500),
+            'game_score' : self.normalize(obs.observation.score_cumulative.score, 1, 12000),
+            'previous_action' : self.previous_action or 0 # Prevents NaN from being sent to the model, breaking things
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
@@ -1341,59 +1374,56 @@ class DQNAgent(base_agent.BaseAgent):
             "rgb_minimap": None
         }
         current_state["non_spatial"] = self.dqn_model.transform_non_spatial_to_fixed_length(
-            visible_units, self.base_top_left,non_spatial_metrics)
+            visible_units, self.base_top_left,non_spatial_metrics, self.last_five_actions)
         current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
             rgb_minimap, self.base_top_left)
         
         # print("The output of our spatial data is set to: ", )
-        # for i in range(20):
+        # for i in range(30):
         #     five_tuple = current_state["non_spatial"][i]
         #     print(five_tuple)
 
         ''' Current State should look like this when passed to the model:
-        Non-spatial metrics are set to:  {'minerals': 1585, 'supply_limit': 31, 'supply_used': 31, 'supply_free': 0, 'army_supply': 14, 'worker_supply': 17, 'cc_count': 1, 'barracks_count': 6, 'supply_depot_count': 2, 'idle_worker_count': 8, 'army_count': 12, 'home_base_camera': True}
-        The output of our spatial data is set to: 
-        [1585.    0.    0.    0.    0.]
-        [31.  0.  0.  0.  0.]
-        [31.  0.  0.  0.  0.]
-        [0. 0. 0. 0. 0.]
-        [14.  0.  0.  0.  0.]
-        [17.  0.  0.  0.  0.]
-        [1. 0. 0. 0. 0.]
-        [6. 0. 0. 0. 0.]
-        [2. 0. 0. 0. 0.]
-        [8. 0. 0. 0. 0.]
-        [12.  0.  0.  0.  0.]
-        [1. 0. 0. 0. 0.]
-        [48.  1. 45. 24. 21.]
-        [45.  1. 45. -2. 53.]
-        [1.8e+01 1.0e+00 1.5e+03 7.0e+00 3.1e+01]
-        [  21.    1. 1000.   28.   59.]
-        [45.  1. 45. 31. 42.]
-        [ 19.   1. 400.  -2.  57.]
-        [45.  1. 45.  1. 54.]
-        [48.  1. 45. 15. 53.]
+            Non-spatial metrics are set to:  {'minerals': 105, 'supply_limit': 15, 'supply_used': 12, 'supply_free': 3, 'army_supply': 0, 'worker_supply': 12, 'cc_count': 1, 'barracks_count': 0, 'supply_depot_count': 0, 'idle_worker_count': 0, 'army_count': 0, 'home_base_camera': True, 'camera_location': 6, 'barracks_available_queue': False, 'supply_depot_recently_built': False, 'game_step_progress': 0.0066711140760507, 'game_score': 0.0920076673056088, 'previous_action': 0}
+            The output of our spatial data is set to: 
+            [105.   0.   0.   0.   0.]
+            [15.  0.  0.  0.  0.]
+            [12.  0.  0.  0.  0.]
+            [3. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [12.  0.  0.  0.  0.]
+            [1. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [1. 0. 0. 0. 0.]
+            [6. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 0. 0. 0.]
+            [0.00667111 0.         0.         0.         0.        ]
+            [0.09200767 0.         0.         0.         0.        ]
+            [0. 0. 0. 0. 0.]
+            [0. 0. 6. 0. 0.]
+            [45.  1. 45. 17. 15.]
+            [45.  1. 45.  8. 23.]
+            [4.83e+02 3.00e+00 1.00e+04 7.00e+00 1.20e+01]
+            [3.42e+02 3.00e+00 1.00e+04 3.30e+01 8.00e+00]
+            [1.8e+01 1.0e+00 1.5e+03 2.2e+01 3.3e+01]
+            [3.41e+02 3.00e+00 1.00e+04 1.70e+01 1.20e+01]
+            [4.83e+02 3.00e+00 1.00e+04 2.10e+01 8.00e+00]
+            [45.  1. 45. 12. 27.]
         '''
 
 
-        # We check to see if we need to do any reward shaping to avoid supply starvation...
-        supply_depot_recently_built = False
-        # print("Game step is: ", self.actual_root_level_steps_taken)
-
-        if self.last_supply_depot_built_step is not None:
-            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
-                # print("supply_depot_recently_built = True" )
-                supply_depot_recently_built = True
-            # else:
-                # print("supply_depot_recently_built = False" )
 
         # Push s/a/r/s_next our replay buffer
         # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
         if self.previous_action is not None:
             self.actual_root_level_steps_taken += 1
-            # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+            # print("Pushing to the replay buffer: ", self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics))
             self.dqn_model.store_transition(self.previous_state,
-                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics,supply_depot_recently_built), current_state)
+                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics), current_state)
 
         # Do in-game training of the model for every 100 root actions the agent takes
         if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
@@ -1494,11 +1524,11 @@ class DQNAgent(base_agent.BaseAgent):
             # This section of the code aims to prevent the bot from repeatedly taking the same action over and over again, 
             # which can result in an undesirable behavior like doing nothing for an extended period.
 
-            # If the last three actions taken by the bot are identical...
-            if len(set(self.last_three_actions)) == 1:  
+            # If the last five actions taken by the bot are identical...
+            if len(set(self.last_five_actions)) == 1:  
 
                 # Retrieve the action that was repeated
-                action_to_exclude = self.last_three_actions[0]
+                action_to_exclude = self.last_five_actions[0]
 
                 # Check if this action isn't already in the list of excluded actions
                 # and also ensure we aren't excluding too many actions, which could lead to potential crashes 
@@ -1520,8 +1550,8 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
-            # Add the action to our tiny 3-element queue
-            self.last_three_actions.append(rl_action)
+            # Add the action to our tiny 5-element queue
+            self.last_five_actions.append(rl_action)
 
             # DEBUG : Print all exclusions!
             # print("Our excluded actions for this step are: ", excluded_actions)

commit 46b50af41acf1c64528624bc17675d27f851f51f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 20 18:48:32 2023 -0400

    added build queue

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index bffb509b..75a5e188 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -873,7 +873,7 @@ class DQNModel(nn.Module):
                     f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
                 return None, None
         else:
-            print("Command Center not found.")
+            # print("Command Center not found.")
             return None, None
 
     # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
@@ -927,7 +927,7 @@ class DQNModel(nn.Module):
             for key, value in non_spatial_data.items():
                 fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
                 offset += 1
-                if offset >= 12:  # Adjust for a maximum 12 non-spatial data entries
+                if offset >= 13:  # Adjust for a maximum 12 non-spatial data entries
                     break
 
         # Then store the units information
@@ -1047,9 +1047,24 @@ class DQNAgent(base_agent.BaseAgent):
             smart_action, x, y = smart_action.split('_')
 
         return (smart_action, x, y)
-
+    
     # CUSTOM
 
+
+    # This function checks to see if we can add more marines to our build queue
+    # Used to improve our reward system for the agent
+    def has_room_in_build_queue(self, obs):
+        for unit in obs.observation.feature_units:
+            if unit.unit_type == _TERRAN_BARRACKS:
+                # Get the length of the build_queue tensor.
+                build_queue_length = len(obs.observation.build_queue)
+                # Check if there's room in the build queue.
+                if build_queue_length < 5:  # Assuming 5 is the max length.
+                    return True
+        # print("build queue is: ", obs.observation.build_queue)
+        return False
+
+
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1248,7 +1263,8 @@ class DQNAgent(base_agent.BaseAgent):
 
         supply_free = supply_limit - supply_used
 
-
+        # Checking to see if we have available build queue for agent state
+        barracks_available_build_queue = self.has_room_in_build_queue(obs)
 
         non_spatial_metrics = {
             'minerals' : self.minerals,
@@ -1262,7 +1278,8 @@ class DQNAgent(base_agent.BaseAgent):
             'supply_depot_count' : supply_depot_count,
             'idle_worker_count' : idle_worker_count,
             'army_count' : army_count,
-            'home_base_camera' : self.last_camera_action == 6
+            'home_base_camera' : self.last_camera_action == 6,
+            'barracks_available_queue' : barracks_available_build_queue
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
@@ -1742,7 +1759,7 @@ def main(args):
                 map_name="Simple64",
                 players=[
                     sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
-                    sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
+                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TESTING_TYPE == "bot" else sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,

commit 533669118e802742e879f57a621f14c22a3aa43b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 20 18:45:47 2023 -0400

    Added barracks queue

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 3d8893a5..cfd922cf 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -60,9 +60,9 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-_TRAINING_TYPE = "bot"  # Possible values: "self", "kane", "bot"
+_TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
-_BOT_DIFFICULTY = "very_easy"
+_BOT_DIFFICULTY = "easy"
 # Enable/Disable Training Guardrails
 _TRAINING_GUARDRAILS_ENABLED = True
 
@@ -107,8 +107,8 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'nonexistentweights'
-_DATA_FILE = 'ddqn-cnn-lstm-agent-model-21-a.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-c.pt'
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-21-d.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -315,7 +315,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.15, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -324,9 +324,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.01
-        # We decay over 2M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 2000000
+            self.epsilon - self.final_epsilon) / 1500000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -342,19 +342,19 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v21-a'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v21-d-KaneAI'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 100000
+        self.training_buffer_requirement = 30000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
             0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as the AI sees it as a 'safe' move in perpetuity
-            1: 1,     # 'buildsupplydepot'
-            2: 0.95,  # 'buildbarracks'
-            3: 0.8,   # 'buildmarine'
-            4: 0.8,   # 'attackunit' - High reward, attack visible units
-            5: 0.8,   # 'buildscv'
-            6: 0.8,   # 'resetcamera' - High reward as is required when available
+            1: 0.9,     # 'buildsupplydepot'
+            2: 0.9,  # 'buildbarracks'
+            3: 1,   # 'buildmarine'
+            4: 0.4,   # 'attackunit' - High reward, attack visible units
+            5: 0.9,   # 'buildscv'
+            6: 0.4,   # 'resetcamera' - High reward as is required when available
             7: 0.05,  # 'mcselfexpansion' - moves camera - checks our expansion
             8: 0.1,   # 'mcenemyexpansion' - moves camera - checks enemy expansion
             9: 0.15,  # 'mcenemyprimary' - moves camera - checks enemy primary base
@@ -429,7 +429,7 @@ class DQNModel(nn.Module):
             nn.LeakyReLU(),
         ).to(self.device) # Moving to GPU if available
 
-        # Decision-making LSTM (takes concatenated, processed outputs from CNN and FCN and gives us a decision)
+        # Decision-making LSTM pass-through (takes concatenated, processed outputs from CNN and FCN, hits the LSTM, then to a single 4k->action_length layer)
         # Offloads to GPU if available
         self.lstm_decision = nn.LSTM(input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
         self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(self.device)  # to get action probabilities/scores
@@ -789,9 +789,9 @@ class DQNModel(nn.Module):
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, supply_depot_recently_built):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
-        # Anything above 15k score results in a full score being provided to the model
-        max_score = 15000
-        # Normalize the score to be between 0 and 1
+        # Anything above 20k score results in a full score being provided to the model
+        max_score = 20000
+        # Normalize the score to be between 0 and 1 (We use -1 to 1 later)
         normalized_score = min(score / max_score, 1)
 
         # --------------------
@@ -825,8 +825,8 @@ class DQNModel(nn.Module):
         # --------------------
 
         # Incentive multipliers for attack logic
-        attack_opposite_quadrant = 1
-        attack_adjacent_quadrant = 0.5
+        attack_opposite_quadrant = 0.5
+        attack_adjacent_quadrant = 0.25
         penalty_home_quadrant = -1
         penalty_home_expansion = -0.5
 
@@ -852,33 +852,39 @@ class DQNModel(nn.Module):
         # Include the predefined rewards for the specific action
         action_reward += self.action_rewards.get(previous_action, 0)
 
-        # Apply the accelerator to the attack actions
-        attack_actions = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
-        if previous_action in attack_actions:
+        # Apply the accelerator ONLY to the attack actions that are not penalties
+        attack_actions_without_penalty = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
+        non_penalty_attack_actions = list(set(attack_actions_without_penalty) - set(self.top_left_actions) - set(self.top_right_actions))
+        # print("Non penalty actions are:" , non_penalty_attack_actions)
+        if previous_action in non_penalty_attack_actions:
             action_reward *= army_size_accelerator
+            # print("attack action-reward is: ", action_reward)
+
 
         # Calculate the total normalized reward by combining the normalized_score and action_reward
-        # Clamp it between 0 and 1
-        total_reward = min(max(normalized_score + action_reward, 0), 1)
+        # Clamp it between -1 and 1 as the LSTM performs better due to its use of tahn
+        total_reward = min(max(normalized_score + action_reward, -1), 1)
 
-        # Special Reward Handling for supply depot construction and supply starvation
-        if non_spatial_metrics['supply_free'] < 4:
-            if previous_action == 1:  # If action is 'buildsupplydepot'
-                # print("Rewarding Supply construction during starvation")
-                return 1
-            # If a supply depot is already under construction, do not penalize/reward further actions
-            elif supply_depot_recently_built:
-                return total_reward
-            else:
-                # print("Penalizing lack of supply depot construction during starvation")
+        # print("Total reward is set to:", total_reward, "with the last action being: ", previous_action)
+
+        # Special Reward handling to ensure Marines are only being built when there is available build_queue
+        # Otherwise, agent spams it unnecessarily
+        if previous_action == 3:  
+            if not non_spatial_metrics['barracks_available_queue']:
                 return 0
-        elif non_spatial_metrics['supply_free'] > 16 and previous_action == 1:  # If there's more than 16 extra supply
-            # Zero out the reward for building a supply depot if there's more than 16 extra supply
-            return 0
+        # Special Reward Handling for supply depot construction and supply starvation
+        elif previous_action == 1:  
+            if non_spatial_metrics['supply_free'] < 4:
+                # Reward for building supply during starvation
+                return 1 if not supply_depot_recently_built else total_reward
+            elif non_spatial_metrics['supply_free'] > 16:
+                # Zero reward for building a supply depot if there's more than 16 extra supply
+                return -0.25
 
         # DEBUGS
         #
-        #print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        # if total_reward < 1:
+            # print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
         #
         return total_reward
 
@@ -956,7 +962,7 @@ class DQNModel(nn.Module):
             for key, value in non_spatial_data.items():
                 fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
                 offset += 1
-                if offset >= 12:  # Adjust for a maximum 12 non-spatial data entries
+                if offset >= 13:  # Adjust for a maximum 13 non-spatial data entries
                     break
 
         # Then store the units information
@@ -1080,9 +1086,23 @@ class DQNAgent(base_agent.BaseAgent):
             smart_action, x, y = smart_action.split('_')
 
         return (smart_action, x, y)
+    
+    # This function checks to see if we can add more marines to our build queue
+    # Used to improve our reward system for the agent
+    def has_room_in_build_queue(self, obs):
+        for unit in obs.observation.feature_units:
+            if unit.unit_type == _TERRAN_BARRACKS:
+                # Get the length of the build_queue tensor.
+                build_queue_length = len(obs.observation.build_queue)
+                # Check if there's room in the build queue.
+                if build_queue_length < 5:  # Assuming 5 is the max length.
+                    return True
+        # print("build queue is: ", obs.observation.build_queue)
+        return False
+    
 
-    # CUSTOM
 
+    # CUSTOM
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -1293,7 +1313,8 @@ class DQNAgent(base_agent.BaseAgent):
 
         supply_free = supply_limit - supply_used
 
-
+        
+        barracks_available_build_queue = self.has_room_in_build_queue(obs)
 
         non_spatial_metrics = {
             'minerals' : self.minerals,
@@ -1307,7 +1328,8 @@ class DQNAgent(base_agent.BaseAgent):
             'supply_depot_count' : supply_depot_count,
             'idle_worker_count' : idle_worker_count,
             'army_count' : army_count,
-            'home_base_camera' : self.last_camera_action == 6
+            'home_base_camera' : self.last_camera_action == 6,
+            'barracks_available_queue' : barracks_available_build_queue
         }
 
         # print("Non-spatial metrics are set to: ", non_spatial_metrics)
@@ -1431,7 +1453,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(2)
 
             # Exclude marines from the build queue
-            if supply_free == 0 or barracks_count < 4 or self.minerals < 50:
+            if supply_free == 0 or barracks_count < 1 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1784,7 +1806,7 @@ def main(args):
                 map_name="Simple64",
                 players=[
                     sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
-                    sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
+                    sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[_BOT_DIFFICULTY]) if _TRAINING_TYPE == "bot" else sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,

commit a7551d46426cd02c218da4d05b0069ebeec3c297
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 20 10:31:36 2023 -0400

    Migration to LSTM for testing

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index a87db7f2..bffb509b 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -88,7 +88,6 @@ class CustomRestartException(Exception):
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -121,7 +120,7 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a-TEST.pt'
+_INITIAL_WEIGHTS = 'ddqn-cnn-lstm-agent-model-21-a.pt'
 _DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
@@ -360,6 +359,7 @@ class DQNModel(nn.Module):
         self.training_buffer_requirement = 150000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        # Some rewards are further shaped based on game state, such as buildsupplydepot
         self.action_rewards = {
             0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
             1: 1,  # 'buildsupplydepot'
@@ -403,50 +403,49 @@ class DQNModel(nn.Module):
 
         # CNN for RGB minimap with normalisation
         self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=3, out_channels=32,
+            nn.Conv2d(in_channels=3, out_channels=16,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(32),
-            nn.ReLU(),
+            nn.BatchNorm2d(16),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
 
-            nn.Conv2d(in_channels=32, out_channels=64,
+            nn.Conv2d(in_channels=16, out_channels=32,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(64),
-            nn.ReLU(),
+            nn.BatchNorm2d(32),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
 
-            nn.Conv2d(in_channels=64, out_channels=128,
+            nn.Conv2d(in_channels=32, out_channels=64,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(128),
-            nn.ReLU(),
+            nn.BatchNorm2d(64),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
         ).to(self.device)  # Moving to GPU if available
 
         # Calculate the size of our flattened output after convolutional layers
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
-        # Fully Connected Network (FCN) for non-spatial data with normalization
+        # Fully Connected Network (FCN) for non-spatial data
+        # Not using dropout as it loses our limited/sparse data (unfortunately...)
+        # Overfitting hasn't been an issue with this particular setup
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
-            nn.ReLU(),
-            nn.Linear(128, 256),
-            nn.BatchNorm1d(300),
-            nn.ReLU()
-        ).to(self.device)  # Moving to GPU if available
-
-        # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
-        self.fc_decision = nn.Sequential(
-            nn.Linear(84992, 2048),
-            nn.ReLU(),
-
-            nn.Linear(2048, 4096),
-            nn.ReLU(),
+            nn.Linear(5, 64), 
+            nn.LeakyReLU(),
+            nn.Linear(64, 128),
+            nn.LeakyReLU(),
+            nn.Linear(128, 64),
+            nn.LeakyReLU(),
+            nn.Linear(64, 32),
+            nn.LeakyReLU(),
+            nn.Linear(32, 16),
+            nn.LeakyReLU(),
+        ).to(self.device) # Moving to GPU if available
+
+        # Decision-making LSTM pass-through (takes concatenated, processed outputs from CNN and FCN, hits the LSTM, then to a single 4k->action_length layer)
+        # Offloads to GPU if available
+        self.lstm_decision = nn.LSTM(input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
+        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(self.device)  # to get action probabilities/scores
 
-            nn.Linear(4096, 2048),
-            nn.ReLU(),
-
-            nn.Linear(2048, len(self.actions))
-        ).to(self.device)  # Moving to GPU if available
 
         self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
@@ -460,7 +459,7 @@ class DQNModel(nn.Module):
         o = self.conv(torch.zeros(1, *shape).to(self.device))
         return int(np.prod(o.size()))
 
-    # This is an implicit PyTorch function that's called automatically
+    # This is an implicit PyTorch function that's called automatically to feed output between networks
     def forward(self, non_spatial_data, minimap):
         non_spatial_data = non_spatial_data.to(self.device)
         minimap = minimap.to(self.device)
@@ -471,9 +470,12 @@ class DQNModel(nn.Module):
             non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
-        combined = torch.cat((conv_out, fc_out), dim=1)
+        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)  # LSTM requires a sequence dimension...
 
-        return self.fc_decision(combined)
+        lstm_out, _ = self.lstm_decision(combined)
+        lstm_out = lstm_out.squeeze(1)  # And now we remove the sequence dimension
+
+        return self.fc_after_lstm(lstm_out)
 
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
 
@@ -561,53 +563,38 @@ class DQNModel(nn.Module):
 
         # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
         self.eval()
+        
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(
             non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        
         # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
 
-        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
-
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
-            available_actions = [
-                a for a in self.actions if a not in excluded_actions]
+            available_actions = [a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
-
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
-            # Adjusting the shape to [batch_size, feature_size, 1]
-            conv_output = conv_output.view(conv_output.size(0), -1, 1)
+            conv_output = conv_output.view(conv_output.size(0), -1)  # No need for the third dimension anymore
 
             # Pass the non-spatial data through its layers
+            # print("Shape of non_spatial_data_tensor:", non_spatial_data_tensor.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            # Adjusting the shape to [batch_size, feature_size, 1]
-            non_spatial_output = non_spatial_output.view(
-                non_spatial_output.size(0), -1, 1)
-
-            # Pass the non-spatial data through its layers
-            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
-            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            non_spatial_output = non_spatial_output.view(
-                non_spatial_output.size(0), -1, 1)
-            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
-
-            # print("Conv shape is: ", conv_output.shape)
-            # print("Non spatial shape is: ", non_spatial_output.shape)
+            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1)  # No need for the third dimension
 
             # Concatenate the two outputs along the second dimension (feature axis)
-            combined_output = torch.cat(
-                (conv_output, non_spatial_output), dim=1)
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(1)  # Adding sequence dimension for LSTM
+            # print("------------Combined tensor shape:", combined_output.shape)
 
-            # Squeeze out the dummy third dimension
-            combined_output = combined_output.squeeze(2)
-
-            # Finally, pass through the decision-making layers
-            q_values = self.fc_decision(combined_output)
+            # Pass through LSTM and subsequent fully connected layer
+            lstm_out, _ = self.lstm_decision(combined_output)
+            lstm_out = lstm_out.squeeze(1)  # Removing sequence dimension
+            q_values = self.fc_after_lstm(lstm_out)
 
             # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
             for action in excluded_actions:
@@ -619,10 +606,7 @@ class DQNModel(nn.Module):
         self.epsilon -= self.epsilon_decay_rate
         self.epsilon = max(self.final_epsilon, self.epsilon)
 
-        # print("The action we chose was: ", action)
-        # print("The excluded actions were: ", excluded_actions)
-
-        # Turn training mode back on
+        # Disabled Turn training mode due to 'testing'
         # self.train()
         return action
 
@@ -1026,9 +1010,9 @@ class DQNAgent(base_agent.BaseAgent):
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
 
-        if os.path.isfile(DATA_FILE):
-            print("Loading previous model: ", DATA_FILE)
-            self.dqn_model.load_model(DATA_FILE)
+        if os.path.isfile(_INITIAL_WEIGHTS):
+            print("Loading previous model: ", _INITIAL_WEIGHTS)
+            self.dqn_model.load_model(_INITIAL_WEIGHTS)
 
     # BOILER PLATE CODE AGAIN
     def transformLocation(self, x, y):
@@ -1743,7 +1727,7 @@ def main(args):
         agent2_name = "Kane_AI"
     elif _TESTING_TYPE == "bot":
         agent2 = sc2_env.Bot(sc2_env.Race.terran,
-                             sc2_env.Difficulty[_BOT_DIFFICULTY.upper()])
+                             sc2_env.Difficulty[_BOT_DIFFICULTY])
         agent2_name = f"{_BOT_DIFFICULTY}_bot"
     else:
         raise ValueError(f"Unknown TRAINING_TYPE: {_TESTING_TYPE}")

commit 5d0dff28e696b291363aa008ac2cced4ab606adf
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 20 08:00:55 2023 -0400

    Grammar/comments

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index b827d426..3d8893a5 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -1,5 +1,5 @@
 '''
-Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
+Abel AI - A Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
@@ -308,14 +308,14 @@ print("--------------------")
 
 
 # Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
-# Technically I believe this architecture could be called a "Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
+# Technically I believe this architecture could be called a "Double DQN with Dual Input, Temporal LSTM Processing, Heuristic-Guided Mixed Precision, CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
 # 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
 # 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.25, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.15, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -414,6 +414,8 @@ class DQNModel(nn.Module):
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
         # Fully Connected Network (FCN) for non-spatial data
+        # Not using dropout as it loses our limited/sparse data (unfortunately...)
+        # Overfitting hasn't been an issue with this particular setup
         self.fc_non_spatial = nn.Sequential(
             nn.Linear(5, 64), 
             nn.LeakyReLU(),

commit 95422a6ef2b250766a2801c1078767c62be516f9
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 20 07:49:34 2023 -0400

    Overhauled Network, added LSTM, fixed FCN

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 18be13ab..b827d426 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -7,6 +7,8 @@ Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by
 
 To modify training behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TRAINING_TYPE Global knob on line 61)
 
+To debug (outside of the PySC2 Run Loop Environment) to see Python Tracebacks, use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_abel_ai.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 ---- add to readme:
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
 craig		soft	nofile		8192
@@ -58,7 +60,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-_TRAINING_TYPE = "self"  # Possible values: "self", "kane", "bot"
+_TRAINING_TYPE = "bot"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
 _BOT_DIFFICULTY = "very_easy"
 # Enable/Disable Training Guardrails
@@ -105,8 +107,8 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-20-b.pt'
+_INITIAL_WEIGHTS = 'nonexistentweights'
+_DATA_FILE = 'ddqn-cnn-lstm-agent-model-21-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -313,7 +315,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=512):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.25, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -340,7 +342,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v20-self-c'
+        self.writer_path = 'runs/ddqn-cnn-lstm-agent-model-v21-a'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 100000
         # This is our custom reward/action mapping dictionary
@@ -389,56 +391,53 @@ class DQNModel(nn.Module):
 
         # CNN for RGB minimap with normalisation
         self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=3, out_channels=32,
+            nn.Conv2d(in_channels=3, out_channels=16,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(32),
-            nn.ReLU(),
+            nn.BatchNorm2d(16),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
 
-            nn.Conv2d(in_channels=32, out_channels=64,
+            nn.Conv2d(in_channels=16, out_channels=32,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(64),
-            nn.ReLU(),
+            nn.BatchNorm2d(32),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
 
-            nn.Conv2d(in_channels=64, out_channels=128,
+            nn.Conv2d(in_channels=32, out_channels=64,
                       kernel_size=3, stride=1, padding=1),
-            nn.BatchNorm2d(128),
-            nn.ReLU(),
+            nn.BatchNorm2d(64),
+            nn.LeakyReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
         ).to(self.device)  # Moving to GPU if available
 
         # Calculate the size of our flattened output after convolutional layers
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
-        # Fully Connected Network (FCN) for non-spatial data with normalization
+        # Fully Connected Network (FCN) for non-spatial data
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
-            nn.ReLU(),
-            nn.Linear(128, 256),
-            nn.BatchNorm1d(300),
-            nn.ReLU()
-        ).to(self.device)  # Moving to GPU if available
-
-        # Decision-making layers (takes concatenated, processed outputs from CNN and FCN and gives us a decision)
-        self.fc_decision = nn.Sequential(
-            nn.Linear(84992, 2048),
-            nn.ReLU(),
-
-            nn.Linear(2048, 4096),
-            nn.ReLU(),
+            nn.Linear(5, 64), 
+            nn.LeakyReLU(),
+            nn.Linear(64, 128),
+            nn.LeakyReLU(),
+            nn.Linear(128, 64),
+            nn.LeakyReLU(),
+            nn.Linear(64, 32),
+            nn.LeakyReLU(),
+            nn.Linear(32, 16),
+            nn.LeakyReLU(),
+        ).to(self.device) # Moving to GPU if available
+
+        # Decision-making LSTM (takes concatenated, processed outputs from CNN and FCN and gives us a decision)
+        # Offloads to GPU if available
+        self.lstm_decision = nn.LSTM(input_size=8896, hidden_size=4096, num_layers=1, batch_first=True).to(self.device)
+        self.fc_after_lstm = nn.Linear(4096, len(self.actions)).to(self.device)  # to get action probabilities/scores
 
-            nn.Linear(4096, 2048),
-            nn.ReLU(),
-
-            nn.Linear(2048, len(self.actions))
-        ).to(self.device)  # Moving to GPU if available
 
         self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
-        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
+        # The goal is to help the model move out of local minima (this happened a lot with a static learning rate)
         self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10000)
 
     #
@@ -446,7 +445,7 @@ class DQNModel(nn.Module):
         o = self.conv(torch.zeros(1, *shape).to(self.device))
         return int(np.prod(o.size()))
 
-    # This is an implicit PyTorch function that's called automatically
+    # This is an implicit PyTorch function that's called automatically to feed output between networks
     def forward(self, non_spatial_data, minimap):
         non_spatial_data = non_spatial_data.to(self.device)
         minimap = minimap.to(self.device)
@@ -457,9 +456,12 @@ class DQNModel(nn.Module):
             non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
-        combined = torch.cat((conv_out, fc_out), dim=1)
+        combined = torch.cat((conv_out, fc_out), dim=1).unsqueeze(1)  # LSTM requires a sequence dimension...
 
-        return self.fc_decision(combined)
+        lstm_out, _ = self.lstm_decision(combined)
+        lstm_out = lstm_out.squeeze(1)  # And now we remove the sequence dimension
+
+        return self.fc_after_lstm(lstm_out)
 
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
     def get_writer(self):
@@ -545,53 +547,38 @@ class DQNModel(nn.Module):
 
         # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
         self.eval()
+        
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(
             non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        
         # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
 
-        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
-
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
-            available_actions = [
-                a for a in self.actions if a not in excluded_actions]
+            available_actions = [a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
-
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
-            # Adjusting the shape to [batch_size, feature_size, 1]
-            conv_output = conv_output.view(conv_output.size(0), -1, 1)
-
-            # Pass the non-spatial data through its layers
-            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            # Adjusting the shape to [batch_size, feature_size, 1]
-            non_spatial_output = non_spatial_output.view(
-                non_spatial_output.size(0), -1, 1)
+            conv_output = conv_output.view(conv_output.size(0), -1)  # No need for the third dimension anymore
 
             # Pass the non-spatial data through its layers
-            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
+            # print("Shape of non_spatial_data_tensor:", non_spatial_data_tensor.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            non_spatial_output = non_spatial_output.view(
-                non_spatial_output.size(0), -1, 1)
-            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
-
-            # print("Conv shape is: ", conv_output.shape)
-            # print("Non spatial shape is: ", non_spatial_output.shape)
+            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1)  # No need for the third dimension
 
             # Concatenate the two outputs along the second dimension (feature axis)
-            combined_output = torch.cat(
-                (conv_output, non_spatial_output), dim=1)
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1).unsqueeze(1)  # Adding sequence dimension for LSTM
+            # print("------------Combined tensor shape:", combined_output.shape)
 
-            # Squeeze out the dummy third dimension
-            combined_output = combined_output.squeeze(2)
-
-            # Finally, pass through the decision-making layers
-            q_values = self.fc_decision(combined_output)
+            # Pass through LSTM and subsequent fully connected layer
+            lstm_out, _ = self.lstm_decision(combined_output)
+            lstm_out = lstm_out.squeeze(1)  # Removing sequence dimension
+            q_values = self.fc_after_lstm(lstm_out)
 
             # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
             for action in excluded_actions:
@@ -603,9 +590,6 @@ class DQNModel(nn.Module):
         self.epsilon -= self.epsilon_decay_rate
         self.epsilon = max(self.final_epsilon, self.epsilon)
 
-        # print("The action we chose was: ", action)
-        # print("The excluded actions were: ", excluded_actions)
-
         # Turn training mode back on
         self.train()
         return action
@@ -803,13 +787,13 @@ class DQNModel(nn.Module):
     def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, supply_depot_recently_built):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
-        # Anything about 12k score results in a full score being provided to the model
-        max_score = 12000
+        # Anything above 15k score results in a full score being provided to the model
+        max_score = 15000
         # Normalize the score to be between 0 and 1
-        normalized_reward = min(score / max_score, 1)
+        normalized_score = min(score / max_score, 1)
 
         # --------------------
-        # Action Mapping Reference
+        # Action Mapping Reference from self.action_rewards{}
         # 0: 'donothing'
         # 1: 'buildsupplydepot'
         # 2: 'buildbarracks'
@@ -844,37 +828,57 @@ class DQNModel(nn.Module):
         penalty_home_quadrant = -1
         penalty_home_expansion = -0.5
 
+        action_reward = 0
+
         # Incentivize attacks to opposite quadrant & their expansion
         # Also de-incentivize attacking home base & expansion
         if previous_action in self.bottom_right_actions:
-            normalized_reward += attack_opposite_quadrant
+            action_reward += attack_opposite_quadrant
         elif previous_action in self.bottom_left_actions:
-            normalized_reward += attack_adjacent_quadrant
+            action_reward += attack_adjacent_quadrant
         elif previous_action in self.top_left_actions:
-            normalized_reward += penalty_home_quadrant
+            action_reward += penalty_home_quadrant
         elif previous_action in self.top_right_actions:
-            normalized_reward += penalty_home_expansion
+            action_reward += penalty_home_expansion
+
+        # Armies of 16 units and over will get the full reward
+        # Trying to encourage attacking with a reasonably large group versus one at a time...
+        max_army_size = 16
+        # Calculate the reward accelerator based on army size. This should give a value between 0.15 and 1.
+        army_size_accelerator = 0.15 + (0.85 * min(max_army_size, non_spatial_metrics['army_count']) / max_army_size)
+
+        # Include the predefined rewards for the specific action
+        action_reward += self.action_rewards.get(previous_action, 0)
+
+        # Apply the accelerator to the attack actions
+        attack_actions = [4] + list(range(10, 26))  # 'attackunit' and 'attack_x_x' actions
+        if previous_action in attack_actions:
+            action_reward *= army_size_accelerator
 
-        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
-        action_reward = self.action_rewards.get(previous_action, 0)
-        # Add the action reward, ensuring the total reward stays between 0 and 1
-        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+        # Calculate the total normalized reward by combining the normalized_score and action_reward
+        # Clamp it between 0 and 1
+        total_reward = min(max(normalized_score + action_reward, 0), 1)
 
-        # Special Reward Handling for supply starvation
+        # Special Reward Handling for supply depot construction and supply starvation
         if non_spatial_metrics['supply_free'] < 4:
             if previous_action == 1:  # If action is 'buildsupplydepot'
                 # print("Rewarding Supply construction during starvation")
                 return 1
             # If a supply depot is already under construction, do not penalize/reward further actions
             elif supply_depot_recently_built:
-                return normalized_reward
+                return total_reward
             else:
                 # print("Penalizing lack of supply depot construction during starvation")
                 return 0
+        elif non_spatial_metrics['supply_free'] > 16 and previous_action == 1:  # If there's more than 16 extra supply
+            # Zero out the reward for building a supply depot if there's more than 16 extra supply
+            return 0
 
-        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
-
-        return normalized_reward
+        # DEBUGS
+        #
+        #print("normalized reward is set to: ", total_reward, " and previous_action was: ", previous_action)
+        #
+        return total_reward
 
     # Provide the x/y coordinates of our command center(s)
     def get_command_center_coordinates(self, obs):
@@ -896,7 +900,7 @@ class DQNModel(nn.Module):
                     f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
                 return None, None
         else:
-            print("Command Center not found.")
+            # print("Command Center not found.")
             return None, None
 
     # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
@@ -1415,12 +1419,11 @@ class DQNAgent(base_agent.BaseAgent):
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
-            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
-
+            # We guard supply depot builds by camera location - need to make sure we're at home base before building
             if supply_free > 4 or self.last_camera_action != 6 or self.minerals < 100:
                 excluded_actions.append(1)
 
-            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            # We guard barracks builds by camera location - need to make sure we're at home base before building
             # Also check to see that we have enough minerals
             if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
@@ -1464,23 +1467,27 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
 
-            # Prevent the bot from doing nothing in perpetuity
-            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
-            # Safety check to make sure if no other options are available we don't crash...
+            # This section of the code aims to prevent the bot from repeatedly taking the same action over and over again, 
+            # which can result in an undesirable behavior like doing nothing for an extended period.
 
-            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
+            # If the last three actions taken by the bot are identical...
+            if len(set(self.last_three_actions)) == 1:  
+
+                # Retrieve the action that was repeated
                 action_to_exclude = self.last_three_actions[0]
+
+                # Check if this action isn't already in the list of excluded actions
+                # and also ensure we aren't excluding too many actions, which could lead to potential crashes 
+                # or undesirable behaviors if no actions are left to choose from.
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
-
             # If our Guardrails are disabled, allow any action to be chosen
             if _TRAINING_GUARDRAILS_ENABLED == False:
                 excluded_actions.clear()
 
             # DQN Selects the action
-            rl_action = self.dqn_model.choose_action(
-                current_state, excluded_actions)
+            rl_action = self.dqn_model.choose_action(current_state, excluded_actions)
 
             self.previous_state = current_state
             self.previous_action = rl_action
@@ -1489,11 +1496,6 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
-            # # Weird error condition where our camera gets locked - purge entries every 100 actions
-            # if self.actual_root_level_steps_taken % 100 == 0:
-            #     # print("zeroing out camera action")
-            #     self.last_camera_action = None
-
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)
 
@@ -1765,7 +1767,7 @@ def main(args):
         agent2_name = "Kane_AI"
     elif _TRAINING_TYPE == "bot":
         agent2 = sc2_env.Bot(sc2_env.Race.terran,
-                             sc2_env.Difficulty[_BOT_DIFFICULTY.upper()])
+                             sc2_env.Difficulty[_BOT_DIFFICULTY])
         agent2_name = f"{_BOT_DIFFICULTY}_bot"
     else:
         raise ValueError(f"Unknown TRAINING_TYPE: {_TRAINING_TYPE}")

commit 366ec091b3b9ce5a251fc718fcf05fd312d71584
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Sep 19 13:05:32 2023 -0400

    adding important replays

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/abel-beats-kane.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/abel-beats-kane.SC2Replay
new file mode 100644
index 00000000..e6bb9893
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/abel-beats-kane.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/another-abel-vs-kane-win.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/another-abel-vs-kane-win.SC2Replay
new file mode 100644
index 00000000..2deae027
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/another-abel-vs-kane-win.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/early-abel-draws-kane(should-have-won).SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/early-abel-draws-kane(should-have-won).SC2Replay
new file mode 100644
index 00000000..e1822e54
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/early-abel-draws-kane(should-have-won).SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/kane-vs-abel-early.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/kane-vs-abel-early.SC2Replay
new file mode 100644
index 00000000..1f16969e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/dqn-replays-posterity/kane-vs-abel-early.SC2Replay differ

commit 90b9b38257a009314f52fd9174a133e7eb18bc59
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Sep 19 11:29:22 2023 -0400

    Guardrails and agent naming for replays

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index dda101ea..18be13ab 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -58,10 +58,11 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-TRAINING_TYPE = "self"  # Possible values: "self", "kane", "bot"
-# Possible values: "very_easy", "easy", "medium", "hard" (only used if TRAINING_TYPE = "bot")
-BOT_DIFFICULTY = "very_easy"
-
+_TRAINING_TYPE = "self"  # Possible values: "self", "kane", "bot"
+# Possible values: "very_easy", "easy", "medium", "hard" (only used if _TRAINING_TYPE = "bot")
+_BOT_DIFFICULTY = "very_easy"
+# Enable/Disable Training Guardrails
+_TRAINING_GUARDRAILS_ENABLED = True
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
@@ -104,8 +105,8 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a-TEST.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-20-b.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -339,7 +340,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v20-self-b'
+        self.writer_path = 'runs/dqn-cnn-agent-v20-self-c'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 100000
         # This is our custom reward/action mapping dictionary
@@ -1472,6 +1473,11 @@ class DQNAgent(base_agent.BaseAgent):
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
+
+            # If our Guardrails are disabled, allow any action to be chosen
+            if _TRAINING_GUARDRAILS_ENABLED == False:
+                excluded_actions.clear()
+
             # DQN Selects the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
@@ -1642,8 +1648,8 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Inside BUILD_BARRACKS")
                 if self.cc_y.any():
                     # print("Inside SCV Check")
-                    x_padding = random.randint(-30, 30)
-                    y_padding = random.randint(-30, 30)
+                    x_padding = random.randint(-35, 35)
+                    y_padding = random.randint(-35, 35)
                     target_x = round(self.cc_x.mean()) - 35 + x_padding
                     target_y = round(self.cc_y.mean()) + y_padding
 
@@ -1747,17 +1753,22 @@ class DQNAgent(base_agent.BaseAgent):
 
 def main(args):
     agent1 = DQNAgent()
+    agent1_name = "Abel_AI"
+
     # Depending on the training type, create agent2
     # Self-Reinforcement Learning
-    if TRAINING_TYPE == "self":
+    if _TRAINING_TYPE == "self":
         agent2 = test_DQN_Agent()
-    elif TRAINING_TYPE == "kane":
+        agent2_name = "Abel_AI"
+    elif _TRAINING_TYPE == "kane":
         agent2 = KaneAI()
-    elif TRAINING_TYPE == "bot":
+        agent2_name = "Kane_AI"
+    elif _TRAINING_TYPE == "bot":
         agent2 = sc2_env.Bot(sc2_env.Race.terran,
-                             sc2_env.Difficulty[BOT_DIFFICULTY.upper()])
+                             sc2_env.Difficulty[_BOT_DIFFICULTY.upper()])
+        agent2_name = f"{_BOT_DIFFICULTY}_bot"
     else:
-        raise ValueError(f"Unknown TRAINING_TYPE: {TRAINING_TYPE}")
+        raise ValueError(f"Unknown TRAINING_TYPE: {_TRAINING_TYPE}")
 
     USE_FEATURE_UNITS = True
     RGB_SCREEN_SIZE = 84
@@ -1768,8 +1779,8 @@ def main(args):
             with sc2_env.SC2Env(
                 map_name="Simple64",
                 players=[
-                    sc2_env.Agent(sc2_env.Race.terran),
-                    sc2_env.Agent(sc2_env.Race.terran),
+                    sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
+                    sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,

commit 2d4ca4e9a851fd058981523a351bab99d9efaef9
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Sep 19 11:28:38 2023 -0400

    Guardrail config and bot naming

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 992f7897..a87db7f2 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -5,7 +5,7 @@ This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
 Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
-To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TESTING_TYPE Global knob on line 61)
+To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the _TESTING_TYPE Global knob on line 61)
 
 ---- add to readme:
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
@@ -69,9 +69,11 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+_TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
-BOT_DIFFICULTY = "very_easy"
+_BOT_DIFFICULTY = "very_easy"
+# Enable/Disable Training Guardrails
+_TRAINING_GUARDRAILS_ENABLED = True
 
 # There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
 _MAX_GAMES_BEFORE_RESTART = 500
@@ -86,7 +88,7 @@ class CustomRestartException(Exception):
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-20-a.pt_episode_800_reward_0.61.pt'
+DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -1107,8 +1109,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Calculate the rolling average reward
             avg_reward = sum(self.last_rewards) / len(self.last_rewards)
 
-            print("------------------------------------------------------")
-            print("Combined reward is set to: ", combined_reward)
+            # print("------------------------------------------------------")
+            # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
             # Checkpoint the model every 200 games
@@ -1117,11 +1119,11 @@ class DQNAgent(base_agent.BaseAgent):
             #     self.dqn_model.save_model(
             #         DATA_FILE, self.episode_count, avg_reward)
 
-            print("Previous average reward was: ",
-                  self.previous_avg_reward)
-            print("Our rolling-average reward is: ", avg_reward)
-            print("Latest game reward was: ", combined_reward)
-            print("Number of steps were: ", episode_steps)
+            # print("Previous average reward was: ",
+            #       self.previous_avg_reward)
+            # print("Our rolling-average reward is: ", avg_reward)
+            # print("Latest game reward was: ", combined_reward)
+            # print("Number of steps were: ", episode_steps)
             # # Backpropagate the final reward multiplier to previous actions
             # print(
             #     f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
@@ -1451,8 +1453,9 @@ class DQNAgent(base_agent.BaseAgent):
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
-            # Testing
-            # excluded_actions.clear()
+            # If our Guardrails are disabled, allow any action to be chosen
+            if _TRAINING_GUARDRAILS_ENABLED == False:
+                excluded_actions.clear()
 
             # DQN Selects the action
             rl_action = self.dqn_model.choose_action(
@@ -1624,8 +1627,8 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Inside BUILD_BARRACKS")
                 if self.cc_y.any():
                     # print("Inside SCV Check")
-                    x_padding = random.randint(-30, 30)
-                    y_padding = random.randint(-30, 30)
+                    x_padding = random.randint(-35, 35)
+                    y_padding = random.randint(-35, 35)
                     target_x = round(self.cc_x.mean()) - 35 + x_padding
                     target_y = round(self.cc_y.mean()) + y_padding
 
@@ -1728,17 +1731,22 @@ class DQNAgent(base_agent.BaseAgent):
 
 def main(args):
     agent1 = DQNAgent()
+    agent1_name = "Abel_AI" 
+
     # Depending on the training type, create agent2
     # Self-Reinforcement Learning
-    if TESTING_TYPE == "self":
+    if _TESTING_TYPE == "self":
         agent2 = DQNAgent()
-    elif TESTING_TYPE == "kane":
+        agent2_name = "Abel_AI"
+    elif _TESTING_TYPE == "kane":
         agent2 = KaneAI()
-    elif TESTING_TYPE == "bot":
+        agent2_name = "Kane_AI"
+    elif _TESTING_TYPE == "bot":
         agent2 = sc2_env.Bot(sc2_env.Race.terran,
-                             sc2_env.Difficulty[BOT_DIFFICULTY.upper()])
+                             sc2_env.Difficulty[_BOT_DIFFICULTY.upper()])
+        agent2_name = f"{_BOT_DIFFICULTY}_bot"
     else:
-        raise ValueError(f"Unknown TESTING_TYPE: {TESTING_TYPE}")
+        raise ValueError(f"Unknown TRAINING_TYPE: {_TESTING_TYPE}")
 
     USE_FEATURE_UNITS = True
     RGB_SCREEN_SIZE = 84
@@ -1749,8 +1757,8 @@ def main(args):
             with sc2_env.SC2Env(
                 map_name="Simple64",
                 players=[
-                    sc2_env.Agent(sc2_env.Race.terran),
-                    sc2_env.Agent(sc2_env.Race.terran),
+                    sc2_env.Agent(sc2_env.Race.terran, name=agent1_name),
+                    sc2_env.Agent(sc2_env.Race.terran, name=agent2_name),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,

commit acd949a4da46d8b7fc4a949c414c060b903698c1
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 20:23:43 2023 -0400

    better camera fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 749696f2..dda101ea 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -1187,7 +1187,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
-            self.last_camera_action = 6
+            self.last_camera_action = None
             for _ in range(3):
                 self.last_three_actions.append(None)
 
@@ -1483,10 +1483,10 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
-            # Weird error condition where our camera gets locked - purge entries every 100 actions
-            if self.actual_root_level_steps_taken % 100 == 0:
-                # print("zeroing out camera action")
-                self.last_camera_action = 6
+            # # Weird error condition where our camera gets locked - purge entries every 100 actions
+            # if self.actual_root_level_steps_taken % 100 == 0:
+            #     # print("zeroing out camera action")
+            #     self.last_camera_action = None
 
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)

commit 645eeee8f6618eab4f5507facc78ab018b824956
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 20:23:19 2023 -0400

    better camera fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 30e4dbec..992f7897 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -1010,7 +1010,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        self.last_camera_action = 6
+        self.last_camera_action = None
         self.last_supply_depot_built_step = None
 
         # Queue that tracks last actions for exclusionary purposes
@@ -1171,7 +1171,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
-            self.last_camera_action = 6
+            self.last_camera_action = None
             for _ in range(3):
                 self.last_three_actions.append(None)
 
@@ -1465,10 +1465,10 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
-            # Weird error condition where our camera gets locked - purge entries every 100 actions
-            if self.actual_root_level_steps_taken % 100 == 0:
-                # print("zeroing out camera action")
-                self.last_camera_action = 6
+            # # Weird error condition where our camera gets locked - purge entries every 100 actions
+            # if self.actual_root_level_steps_taken % 100 == 0:
+            #     # print("zeroing out camera action")
+            #     self.last_camera_action = None
 
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)

commit 606b01ca22cca1febf869fca55f2ecbf21db1509
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 19:47:46 2023 -0400

    fixed camera glitch

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index c0c4fc05..749696f2 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -3,7 +3,7 @@ Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, w
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
-Out of the 1700~ Lines-of-Code, approximately 1400~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
 To modify training behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TRAINING_TYPE Global knob on line 61)
 
@@ -1187,7 +1187,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
-            self.last_camera_action = None
+            self.last_camera_action = 6
             for _ in range(3):
                 self.last_three_actions.append(None)
 
@@ -1486,7 +1486,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Weird error condition where our camera gets locked - purge entries every 100 actions
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("zeroing out camera action")
-                self.last_camera_action = 0
+                self.last_camera_action = 6
 
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)

commit ad71bc5c78e9ceb400f81cbcbcd6901e73b5bd1a
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 19:47:26 2023 -0400

    fixed camera glitch

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 5c33a43e..30e4dbec 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -1010,7 +1010,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        self.last_camera_action = None
+        self.last_camera_action = 6
         self.last_supply_depot_built_step = None
 
         # Queue that tracks last actions for exclusionary purposes
@@ -1171,7 +1171,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
-            self.last_camera_action = None
+            self.last_camera_action = 6
             for _ in range(3):
                 self.last_three_actions.append(None)
 
@@ -1451,6 +1451,9 @@ class DQNAgent(base_agent.BaseAgent):
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
+            # Testing
+            # excluded_actions.clear()
+
             # DQN Selects the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
@@ -1465,14 +1468,14 @@ class DQNAgent(base_agent.BaseAgent):
             # Weird error condition where our camera gets locked - purge entries every 100 actions
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("zeroing out camera action")
-                self.last_camera_action = 0
+                self.last_camera_action = 6
 
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)
 
             # DEBUG : Print all exclusions!
-            print("Our excluded actions for this step are: ", excluded_actions)
-            print("The model chose: ", rl_action)
+            # print("Our excluded actions for this step are: ", excluded_actions)
+            # print("The model chose: ", rl_action)
 
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)

commit 578794ad8013a60f4be63f933a55c759cef9ccc4
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 19:37:16 2023 -0400

    Update test_abel_ai.py

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index e2af9a49..5c33a43e 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -69,7 +69,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-TESTING_TYPE = "self"  # Possible values: "self", "kane", "bot"
+TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
 BOT_DIFFICULTY = "very_easy"
 
@@ -975,8 +975,12 @@ class DQNAgent(base_agent.BaseAgent):
         print("Available actions are set to:", initial_actions)
         print("State Size:", STATE_SIZE)
 
+        # Our primary DQN
         self.dqn_model = DQNModel(
             actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+        # Our target network used for Q-Value Calculations
+        self.target_network = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
 
         self.previous_action = None
         self.previous_state = {

commit a2d92a27931e24f5e054284f0f5b43fad05c9cdf
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 19:34:03 2023 -0400

    updating choose_action()

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 20c9ba39..e2af9a49 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -69,7 +69,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+TESTING_TYPE = "self"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
 BOT_DIFFICULTY = "very_easy"
 
@@ -570,16 +570,11 @@ class DQNModel(nn.Module):
         # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
 
         # Epsilon-based exploration
-        # Disabled for testing
         if np.random.uniform() < self.epsilon:
             available_actions = [
                 a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
 
-        #     # This is where we keep the logic for epsilon decay (linear)
-        #     self.action_counter += 1
-        #     self.epsilon -= self.epsilon_decay_rate
-        #     self.epsilon = max(self.final_epsilon, self.epsilon)
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
@@ -617,10 +612,16 @@ class DQNModel(nn.Module):
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
 
-            # print("The action we chose was: ", action)
-            # print("The excluded actions were: ", excluded_actions)
+        # This is where we keep the logic for epsilon decay (linear)
+        self.action_counter += 1
+        self.epsilon -= self.epsilon_decay_rate
+        self.epsilon = max(self.final_epsilon, self.epsilon)
+
+        # print("The action we chose was: ", action)
+        # print("The excluded actions were: ", excluded_actions)
 
         # Turn training mode back on
+        # self.train()
         return action
 
     # This is where we train the model
@@ -1466,8 +1467,8 @@ class DQNAgent(base_agent.BaseAgent):
             self.last_three_actions.append(rl_action)
 
             # DEBUG : Print all exclusions!
-            # print("Our excluded actions for this step are: ", excluded_actions)
-            # print("The model chose: ", rl_action)
+            print("Our excluded actions for this step are: ", excluded_actions)
+            print("The model chose: ", rl_action)
 
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)

commit 6b76d23da104eb9911c13a0dfd1575d1d6cb82ef
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 19:26:04 2023 -0400

    huge refactor for testing framework

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 1dd2f867..20c9ba39 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -1,13 +1,26 @@
-# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
-# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
-# The RL Algorithm can properly interact with StarCraft II / PySC2
-# All of the RL algorithms were implemented by me
+'''
+Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
+
+This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
+The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
+Out of the 1800~ Lines-of-Code, approximately 1500~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+
+To modify testing behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TESTING_TYPE Global knob on line 61)
+
+---- add to readme:
+file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
+craig		soft	nofile		8192
+craig 	hard	nofile		1048576
+Additionally, tensorboard needs to be run as root (due to higher FD limits):
+ulimit -n 1048576
+tensorboard --logdir=runs
+
+Validated with: ulimit -s -H && ulimit -n -S
+required as after 1k episodes we run out of FD's:
+FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+'''
 
-# Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 
-# Test against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 # file-descriptor limits need to be artificially raised in /etc/security/limits.conf
 # craig		soft	nofile		8192
 # craig 	hard	nofile		1048576
@@ -19,16 +32,23 @@
 # required as after 1k episodes we run out of FD's:
 # FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
 
+# Import our other agents
+from Kane_AI import KaneAI
+
 import random
 from random import sample as random_sample
 import time
 import math
+
+from absl import app
 import os
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as F
+# queue used for replay buffer
+from collections import deque
 
 # Using TensorBoard for model performance tracking & visualizations
 from torch.utils.tensorboard import SummaryWriter
@@ -45,8 +65,18 @@ from pysc2.agents import base_agent
 from pysc2.lib import actions
 from pysc2.lib import features
 from pysc2.lib import units
-# queue used for replay buffer
-from collections import deque
+from pysc2.env import sc2_env
+from pysc2.env import run_loop
+
+# Global Configuration Knobs
+TESTING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+# Possible values: "very_easy", "easy", "medium", "hard" (only used if TESTING_TYPE = "bot")
+BOT_DIFFICULTY = "very_easy"
+
+# There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
+_MAX_GAMES_BEFORE_RESTART = 500
+class CustomRestartException(Exception):
+    pass
 
 # BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
 # https://github.com/skjb/pysc2-tutorial/tree/master
@@ -56,7 +86,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-17.pt_episode_15200_reward_-0.06.pt'
+DATA_FILE = 'dqn-cnn-agent-model-20-a.pt_episode_800_reward_0.61.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -68,12 +98,8 @@ _TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
 _SELECT_ARMY = actions.FUNCTIONS.select_army.id
 _ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
 _HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
-# CUSTOM
-_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
-# END CUSTOM
 
 _UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
-# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
 
 _TERRAN_COMMANDCENTER = 18
 _TERRAN_SCV = 45
@@ -90,15 +116,22 @@ ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
 ACTION_BUILD_BARRACKS = 'buildbarracks'
 ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
-# CUSTOM
+####### CUSTOM CODE ######
+#
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a-TEST.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+
 ACTION_BUILD_SCV = 'buildscv'
 ACTION_ATTACK_UNIT = 'attackunit'
 ACTION_RESET_CAMERA = 'resetcamera'
 ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
 ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
 ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
-# END OF CUSTOM
+###### END OF GLOBAL CUSTOM CODE #####
 
+# Steven Brown also used smart_actions although for this bot I've expanded them considerably
 smart_actions = [
     ACTION_DO_NOTHING,
     ACTION_BUILD_SUPPLY_DEPOT,
@@ -112,7 +145,7 @@ smart_actions = [
     ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
 ]
 
-# DQN Non-Partial State size
+# DQN Non-Spatial State size
 STATE_SIZE = 2
 
 ################################## End of BoilerPlate Code #####################################################
@@ -182,44 +215,63 @@ quadrants = [
 
 # Calculate the offset points for each quadrant
 # This is used in the smart_attack functions (16 locations with offsets)
-# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
 
 
 def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
     corner_offset = 3
     mini_quadrant_size = 16  # Each quadrant is divided further into 4
-    
+
     if quadrant == "top-left":
         points = [
-            (top_left_x + corner_offset, top_left_y + corner_offset),                      # Top-left of top-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + corner_offset), # Top-left of top-right mini quadrant
-            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-left of bottom-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-left of bottom-right mini quadrant
+            # Top-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             corner_offset),  # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + \
+             corner_offset),  # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-left of bottom-right mini quadrant
         ]
     elif quadrant == "top-right":
         points = [
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + corner_offset),                   # Top-right of top-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + corner_offset),               # Top-right of top-right mini quadrant
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-right of bottom-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-right of bottom-right mini quadrant
+            # Top-right of top-left mini quadrant
+            (top_left_x + mini_quadrant_size - \
+             corner_offset, top_left_y + corner_offset),
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset),  # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-right of bottom-right mini quadrant
         ]
     elif quadrant == "bottom-left":
         points = [
-            (top_left_x + corner_offset, top_left_y + mini_quadrant_size - corner_offset),                   # Bottom-left of top-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-left of top-right mini quadrant
-            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset),               # Bottom-left of bottom-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-left of bottom-right mini quadrant
+            # Bottom-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - \
+             corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-left of bottom-right mini quadrant
         ]
     elif quadrant == "bottom-right":
         points = [
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-right of top-right mini quadrant
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset), # Bottom-right of bottom-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-right of bottom-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y +
+             mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset),  # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-right of bottom-right mini quadrant
         ]
 
     return points
 
+
 # For each quadrant, calculate the offset points and append the attack action
 quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
 for i, quad in enumerate(quadrants):
@@ -855,8 +907,8 @@ class DQNModel(nn.Module):
 
         return transformed_minimap
 
-    # This creates a fixed-length vector to store visible units in
-    def transform_units_to_fixed_length(self, units, base_top_left):
+    # This creates a fixed-length vector to store non-spatial data in (metrics and unit metadata)
+    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data):
 
         # Safety checks
         # Check if units are None or empty
@@ -882,20 +934,29 @@ class DQNModel(nn.Module):
         # Create an empty array of shape (300, num_features)
         fixed_length_units = np.zeros((300, num_features))
 
-        # Fill the array with the actual values
-        for i in range(min(num_units, 300)):
+        # Embed the non-spatial game state information into the first entries
+        offset = 0
+        if non_spatial_data:
+            for key, value in non_spatial_data.items():
+                fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
+                offset += 1
+                if offset >= 12:  # Adjust for a maximum 12 non-spatial data entries
+                    break
+
+        # Then store the units information
+        # Start after the reserved entries
+        for i in range(min(len(units), 300 - offset)):  # Adjusted based on the reserved non-spatial entries
             unit = units[i]
-            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
-            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
-            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+            index = i + offset  # Index into fixed_length_units
+            fixed_length_units[index, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[index, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[index, 2] = unit[HEALTH_INDEX]
 
             # Transform x, y coordinates as required for normalization
             if not base_top_left:
-                fixed_length_units[i, 3], fixed_length_units[i,
-                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
             else:
-                fixed_length_units[i, 3], fixed_length_units[i,
-                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
 
         return fixed_length_units
 
@@ -927,6 +988,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.move_number = 0
 
+        # Minerals
         self.minerals = 0
 
         # Used for tracking rewards for use in model saving/checkpointing
@@ -941,14 +1003,13 @@ class DQNAgent(base_agent.BaseAgent):
         # Custom Delay Timers
         self.attack_delay_timer = 0
         self.unit_attack_delay_timer = 0
-        self.supply_delay_timer = 0
-        self.barracks_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
         self.last_camera_action = None
+        self.last_supply_depot_built_step = None
+
         # Queue that tracks last actions for exclusionary purposes
         self.last_three_actions = deque(maxlen=3)
-
         self.command_center = []
 
         # Action mapping
@@ -1003,12 +1064,8 @@ class DQNAgent(base_agent.BaseAgent):
         # Using delay timers to avoid duplicate commands being issued by the AI
         self.attack_delay_timer += 1
         self.unit_attack_delay_timer += 1
-        self.supply_delay_timer += 1
-        self.barracks_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_move_timer += 1
-        # Minerals
-        self.minerals = obs.observation['player'][1]
 
         # Visuals
         rgb_minimap = obs.observation["rgb_minimap"]
@@ -1016,8 +1073,6 @@ class DQNAgent(base_agent.BaseAgent):
         # Check our current score, just for debugging
         # print("Current score is: ", self.get_normalized_reward(obs))
 
-        # print("self.cc is: ", self.cc_y)
-
         # If this is our last step
         if obs.last():
             self.episode_count += 1
@@ -1048,7 +1103,7 @@ class DQNAgent(base_agent.BaseAgent):
             avg_reward = sum(self.last_rewards) / len(self.last_rewards)
 
             print("------------------------------------------------------")
-            # print("Combined reward is set to: ", combined_reward)
+            print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
             # Checkpoint the model every 200 games
@@ -1057,11 +1112,11 @@ class DQNAgent(base_agent.BaseAgent):
             #     self.dqn_model.save_model(
             #         DATA_FILE, self.episode_count, avg_reward)
 
-            # print("Previous average reward was: ",
-            #       self.previous_avg_reward)
-            # print("Our rolling-average reward is: ", avg_reward)
-            # print("Latest game reward was: ", combined_reward)
-            # print("Number of steps were: ", episode_steps)
+            print("Previous average reward was: ",
+                  self.previous_avg_reward)
+            print("Our rolling-average reward is: ", avg_reward)
+            print("Latest game reward was: ", combined_reward)
+            print("Number of steps were: ", episode_steps)
             # # Backpropagate the final reward multiplier to previous actions
             # print(
             #     f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
@@ -1073,10 +1128,10 @@ class DQNAgent(base_agent.BaseAgent):
             #     self.dqn_model.transfer_buffer()
 
             # Print statements if our buffer is large enough to train on...
-            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
-                print("Number of in-game model updates: ",
-                      self.in_game_training_iterations)
-                print("Training the model after game completion...")
+            # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     print("Number of in-game model updates: ",
+            #           self.in_game_training_iterations)
+            #     print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
 
             # Log our reward over time and training duration if we've started training
@@ -1107,13 +1162,14 @@ class DQNAgent(base_agent.BaseAgent):
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
             self.in_game_training_iterations = 0
+            supply_depot_recently_built = False
+            self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
+            self.last_camera_action = None
             for _ in range(3):
                 self.last_three_actions.append(None)
 
-            return actions.FunctionCall(_NO_OP, [])
-
         # BOILER PLATE Action-Space Guardrails
         # Used as a way of limiting the potential action space at the beginning of the game for the agent
 
@@ -1181,79 +1237,122 @@ class DQNAgent(base_agent.BaseAgent):
         # if self.command_center:
         #     print("Command centers are set to:", self.command_center)
 
+        # Minerals
+        self.minerals = obs.observation['player'][1]
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
         army_supply = obs.observation['player'][5]
         worker_supply = obs.observation['player'][6]
-        # BEGIN CUSTOM CODE
+        idle_worker_count = obs.observation['player'][7]
+        army_count = obs.observation['player'][8]
+
         # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
-        # print("army_supply is set to: ", army_supply)
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
-        # To do - prioritize enemy units over structures...
-        # enemy_terran_structures = [
-        #     unit for unit in obs.observation.feature_units
-        #     if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
-        # ]
-        # if len(enemy_terran_structures) > 0:
-        #     print("Enemy structures are set to: ", enemy_terran_structures)
-
-        # self_units = [
-        #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
-        # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
-        # END CUSTOM CODE
-
         visible_units = [unit for unit in obs.observation.feature_units]
         # print("Visible Units are set to: ", visible_units)
         # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
 
         supply_free = supply_limit - supply_used
 
+
+
+        non_spatial_metrics = {
+            'minerals' : self.minerals,
+            'supply_limit' : supply_limit,
+            'supply_used' : supply_used,
+            'supply_free' : supply_free,
+            'army_supply' : army_supply,
+            'worker_supply' : worker_supply,
+            'cc_count' : command_center_count,
+            'barracks_count' : barracks_count,
+            'supply_depot_count' : supply_depot_count,
+            'idle_worker_count' : idle_worker_count,
+            'army_count' : army_count,
+            'home_base_camera' : self.last_camera_action == 6
+        }
+
+        # print("Non-spatial metrics are set to: ", non_spatial_metrics)
+        # print("Non-spatial barracks count is: ", non_spatial_metrics["barracks_count"])
+
+
+        current_state = {
+            "non_spatial": np.zeros(300),
+            "rgb_minimap": None
+        }
+        current_state["non_spatial"] = self.dqn_model.transform_non_spatial_to_fixed_length(
+            visible_units, self.base_top_left,non_spatial_metrics)
+        current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+            rgb_minimap, self.base_top_left)
+        
+        # print("The output of our spatial data is set to: ", )
+        # for i in range(20):
+        #     five_tuple = current_state["non_spatial"][i]
+        #     print(five_tuple)
+
+        ''' Current State should look like this when passed to the model:
+        Non-spatial metrics are set to:  {'minerals': 1585, 'supply_limit': 31, 'supply_used': 31, 'supply_free': 0, 'army_supply': 14, 'worker_supply': 17, 'cc_count': 1, 'barracks_count': 6, 'supply_depot_count': 2, 'idle_worker_count': 8, 'army_count': 12, 'home_base_camera': True}
+        The output of our spatial data is set to: 
+        [1585.    0.    0.    0.    0.]
+        [31.  0.  0.  0.  0.]
+        [31.  0.  0.  0.  0.]
+        [0. 0. 0. 0. 0.]
+        [14.  0.  0.  0.  0.]
+        [17.  0.  0.  0.  0.]
+        [1. 0. 0. 0. 0.]
+        [6. 0. 0. 0. 0.]
+        [2. 0. 0. 0. 0.]
+        [8. 0. 0. 0. 0.]
+        [12.  0.  0.  0.  0.]
+        [1. 0. 0. 0. 0.]
+        [48.  1. 45. 24. 21.]
+        [45.  1. 45. -2. 53.]
+        [1.8e+01 1.0e+00 1.5e+03 7.0e+00 3.1e+01]
+        [  21.    1. 1000.   28.   59.]
+        [45.  1. 45. 31. 42.]
+        [ 19.   1. 400.  -2.  57.]
+        [45.  1. 45.  1. 54.]
+        [48.  1. 45. 15. 53.]
+        '''
+
+
+        # We check to see if we need to do any reward shaping to avoid supply starvation...
+        supply_depot_recently_built = False
+        # print("Game step is: ", self.actual_root_level_steps_taken)
+
+        if self.last_supply_depot_built_step is not None:
+            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
+                # print("supply_depot_recently_built = True" )
+                supply_depot_recently_built = True
+            # else:
+                # print("supply_depot_recently_built = False" )
+
+        # Disabled for Testing
+        #
+        # Push s/a/r/s_next our replay buffer
+        # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+        # if self.previous_action is not None:
+        #     self.actual_root_level_steps_taken += 1
+        #     # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+        #     self.dqn_model.store_transition(self.previous_state,
+        #                                     self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics,supply_depot_recently_built), current_state)
+
+        # # Do in-game training of the model for every 100 root actions the agent takes
+        # if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+        #     # print("Beginning in-game training for the model.")
+        #     self.in_game_training_iterations += 1
+        #     self.dqn_model.learn(self.target_network)
+        ############
+
+               # Simple intent state tracker
+        if not hasattr(self, 'intended_action'):
+            self.intended_action = None
+
         if self.move_number == 0:
             self.move_number += 1
 
-            current_state = {
-                "non_spatial": np.zeros(300),
-                "rgb_minimap": None
-            }
-            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
-                visible_units, self.base_top_left)
-            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
-                rgb_minimap, self.base_top_left)
-
-            # current_state["non_spatial"][0] = supply_depot_count
-            # current_state["non_spatial"][1] = barracks_count
-            # current_state["non_spatial"][2] = army_supply
-            # # Custom State Add-on
-            # current_state["non_spatial"][3] = command_center_count
-            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
-            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
-            # # A check to see where the camera is looking
-            # current_state["non_spatial"][6] = self.last_camera_action
-            # # spawn location boolean seems critical for learning
-            # current_state["non_spatial"][7] = self.base_top_left
-
-            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
-
-            # print("State at the end of the step is set to: ", current_state)
-
-            # Push s/a/r/s_next our replay buffer
-            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
-            if self.previous_action is not None:
-                self.actual_root_level_steps_taken += 1
-                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
-                self.dqn_model.store_transition(self.previous_state,
-                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
-
-            # Do in-game training of the model for every 100 root actions the agent takes:
-            # Learning disabled for testing
-            # if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
-            #     # print("Beginning in-game training for the model.")
-            #     self.in_game_training_iterations += 1
-            #     self.dqn_model.learn()
-
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []
             # print("excluded_actions at the start are set to: ", excluded_actions)
@@ -1289,20 +1388,18 @@ class DQNAgent(base_agent.BaseAgent):
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
-            # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
-            # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
+            if supply_free > 4 or self.last_camera_action != 6 or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
             # Also check to see that we have enough minerals
-            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150:
+            if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
 
-            # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
+            # Exclude marines from the build queue
+            if supply_free == 0 or barracks_count < 4 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1315,7 +1412,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(4)
 
             # SCV Checks
-            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
+            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 4:
                 excluded_actions.append(5)
 
             # Camera reset handling
@@ -1331,7 +1428,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(8)
                 excluded_actions.append(9)
 
-            # modified original logic, waits for 8 marines before attacking
+            # modified original logic, waits for 16 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
@@ -1339,43 +1436,26 @@ class DQNAgent(base_agent.BaseAgent):
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
-                
-            #  Excludes attacking home base
-            # These actions always target the home base after x/y transformation
-            # ---- Required for bootstrapping, but not after training due to reward structure ----
-            #
-            # for action in self.top_left_actions:
-            #     if action not in excluded_actions:
-            #         excluded_actions.append(action)
 
             # Prevent the bot from doing nothing in perpetuity
             # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
             # Safety check to make sure if no other options are available we don't crash...
 
-            # Old Logic - was specific to '0' but now we check for everything instead
-            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
-            #     # print("Excluding DO NOTHING")
-            #     excluded_actions.append(0)
-
             if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
                 action_to_exclude = self.last_three_actions[0]
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
-            # Updated for DQN - let the model  select the action
+            # DQN Selects the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
 
-            # print("Agent action choice: ", rl_action)
-            # print("Excluded actions were: ", excluded_actions)
-
             self.previous_state = current_state
             self.previous_action = rl_action
 
             # Set our last camera action:
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
-                # print("self.last_camera_action is:", self.last_camera_action)
 
             # Weird error condition where our camera gets locked - purge entries every 100 actions
             if self.actual_root_level_steps_taken % 100 == 0:
@@ -1392,8 +1472,6 @@ class DQNAgent(base_agent.BaseAgent):
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)
 
-            # print("Smart action is set to: ", smart_action)
-
             if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
                 unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
 
@@ -1404,13 +1482,15 @@ class DQNAgent(base_agent.BaseAgent):
                     # Checks to avoid out-of-bounds crashing
                     # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
                     if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        # Update Intent State Tracking
+                        if smart_action == ACTION_BUILD_BARRACKS:
+                            self.intended_action = ACTION_BUILD_BARRACKS
+                        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                            self.intended_action = ACTION_BUILD_SUPPLY_DEPOT
                         return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
                     else:
                         print(f"SCV coordinates out of range: {target}")
 
-                else:
-                    print("Failed unit.y check")
-
             elif smart_action == ACTION_BUILD_MARINE:
                 if barracks_y.any():
                     i = random.randint(0, len(barracks_y) - 1)
@@ -1510,60 +1590,59 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
-            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
-                # Sleep to avoid duplicate actions
-                self.supply_delay_timer = 0
-                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
-                    if self.cc_y.any():
-                        x_padding = random.randint(-30, 30)
-                        y_padding = random.randint(-30, 30)
-                        target_x = round(self.cc_x.mean()) - 35 + x_padding
-                        target_y = round(self.cc_y.mean()) + y_padding
-
-                        target = self.transformLocation(target_x, target_y)
-
-                        # Ensure the coordinates are within valid bounds after transformation
-                        target[0] = max(BORDER_PADDING, min(
-                            target[0], 83 - BORDER_PADDING))
-                        target[1] = max(BORDER_PADDING, min(
-                            target[1], 83 - BORDER_PADDING))
-
-                        # print("Trying to build a supply depot at:", target)
-
-                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
-
-            elif smart_action == ACTION_BUILD_BARRACKS:
-                # sleep to avoid loops
-                self.barracks_delay_timer = 0
-
-                if _BUILD_BARRACKS in obs.observation['available_actions']:
-                    if self.cc_y.any():
-                        x_padding = random.randint(-30, 30)
-                        y_padding = random.randint(-30, 30)
-                        target_x = round(self.cc_x.mean()) - 35 + x_padding
-                        target_y = round(self.cc_y.mean()) + y_padding
-
-                        target = self.transformLocation(target_x, target_y)
-
-                        # Ensure the coordinates are within valid bounds after transformation
-                        # Assuming screen size is 84x84
-                        target[0] = max(BORDER_PADDING, min(
-                            target[0], 83 - BORDER_PADDING))
-                        target[1] = max(BORDER_PADDING, min(
-                            target[1], 83 - BORDER_PADDING))
-
-                        # print("Trying to build a barracks at:", target)
-
-                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+               self.last_supply_depot_built_step = self.actual_root_level_steps_taken
+               if self.cc_y.any():
+                    x_padding = random.randint(-30, 30)
+                    y_padding = random.randint(-30, 30)
+                    target_x = round(self.cc_x.mean()) - 35 + x_padding
+                    target_y = round(self.cc_y.mean()) + y_padding
+
+                    target = self.transformLocation(target_x, target_y)
+
+                    # Ensure the coordinates are within valid bounds after transformation
+                    target[0] = max(BORDER_PADDING, min(
+                        target[0], 83 - BORDER_PADDING))
+                    target[1] = max(BORDER_PADDING, min(
+                        target[1], 83 - BORDER_PADDING))
+
+                    # Start our timer for reward shaping
+
+                    # print("Trying to build a supply depot at:", target)
+                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif self.intended_action == ACTION_BUILD_BARRACKS and _BUILD_BARRACKS in obs.observation['available_actions']:
+                # print("Obs actions are:", obs.observation['available_actions'])
+                # print("Inside BUILD_BARRACKS")
+                if self.cc_y.any():
+                    # print("Inside SCV Check")
+                    x_padding = random.randint(-30, 30)
+                    y_padding = random.randint(-30, 30)
+                    target_x = round(self.cc_x.mean()) - 35 + x_padding
+                    target_y = round(self.cc_y.mean()) + y_padding
+
+                    target = self.transformLocation(target_x, target_y)
+
+                    # Ensure the coordinates are within valid bounds after transformation
+                    # Assuming screen size is 84x84
+                    target[0] = max(BORDER_PADDING, min(
+                        target[0], 83 - BORDER_PADDING))
+                    target[1] = max(BORDER_PADDING, min(
+                        target[1], 83 - BORDER_PADDING))
+                    
+                    # print("Trying to build barracks at: ", target)
+
+                    # print("Trying to build a barracks at:", target)
+                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
             elif smart_action == ACTION_BUILD_SCV:
                 # print("SCV Smart Action Set")
-
                 # Zero out our build timer
                 self.scv_delay_timer = 0
                 if _TRAIN_SCV in obs.observation['available_actions']:
                     # print("Trying to train an SCV")
+
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
             # start of boiler plate code (small modifications like delay timers)
@@ -1609,9 +1688,12 @@ class DQNAgent(base_agent.BaseAgent):
                         # Reset our timers to 0
                         self.unit_attack_delay_timer = 0
                         self.attack_delay_timer = 0
+                        self.last_camera_action = 0
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 
+            self.intended_action = None
+    
         elif self.move_number == 2:
             self.move_number = 0
 
@@ -1635,3 +1717,57 @@ class DQNAgent(base_agent.BaseAgent):
         return actions.FunctionCall(_NO_OP, [])
 
     # end of boiler plate code
+
+def main(args):
+    agent1 = DQNAgent()
+    # Depending on the training type, create agent2
+    # Self-Reinforcement Learning
+    if TESTING_TYPE == "self":
+        agent2 = DQNAgent()
+    elif TESTING_TYPE == "kane":
+        agent2 = KaneAI()
+    elif TESTING_TYPE == "bot":
+        agent2 = sc2_env.Bot(sc2_env.Race.terran,
+                             sc2_env.Difficulty[BOT_DIFFICULTY.upper()])
+    else:
+        raise ValueError(f"Unknown TESTING_TYPE: {TESTING_TYPE}")
+
+    USE_FEATURE_UNITS = True
+    RGB_SCREEN_SIZE = 84
+    RGB_MINIMAP_SIZE = 64
+
+    while True:
+        try:
+            with sc2_env.SC2Env(
+                map_name="Simple64",
+                players=[
+                    sc2_env.Agent(sc2_env.Race.terran),
+                    sc2_env.Agent(sc2_env.Race.terran),
+                ],
+                agent_interface_format=features.AgentInterfaceFormat(
+                    action_space=actions.ActionSpace.RGB,
+                    use_feature_units=USE_FEATURE_UNITS,
+                    rgb_dimensions=features.Dimensions(
+                        screen=RGB_SCREEN_SIZE, minimap=RGB_MINIMAP_SIZE),
+                    feature_dimensions=features.Dimensions(
+                        screen=84, minimap=64)
+                ),
+                step_mul=16,
+                game_steps_per_episode=21000,
+                visualize=False
+            ) as env:
+                run_loop.run_loop([agent1, agent2], env)
+
+        except CustomRestartException:
+            print("Caught restart signal. Restarting environment...")
+            continue
+        except KeyboardInterrupt:
+            break
+        except Exception as e:
+            print(f"An error occurred: {e}")
+            print("Attempting to restart the SC2 environment.")
+            # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
+            continue
+
+if __name__ == "__main__":
+    app.run(main)

commit 37fa114b7b5da610d5b4f391b802c1f4950f8174
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 14:56:13 2023 -0400

    update to reward structure and state machine

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 1107bbc1..c0c4fc05 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -104,7 +104,7 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-19-b.pt_episode_3400_reward_0.84.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-20-a-TEST.pt'
 _DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
@@ -339,9 +339,9 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v20-self-a'
+        self.writer_path = 'runs/dqn-cnn-agent-v20-self-b'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 50000
+        self.training_buffer_requirement = 100000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -799,7 +799,7 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action):
+    def get_normalized_reward(self, obs, previous_action, non_spatial_metrics, supply_depot_recently_built):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
         # Anything about 12k score results in a full score being provided to the model
@@ -859,6 +859,18 @@ class DQNModel(nn.Module):
         # Add the action reward, ensuring the total reward stays between 0 and 1
         normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
 
+        # Special Reward Handling for supply starvation
+        if non_spatial_metrics['supply_free'] < 4:
+            if previous_action == 1:  # If action is 'buildsupplydepot'
+                # print("Rewarding Supply construction during starvation")
+                return 1
+            # If a supply depot is already under construction, do not penalize/reward further actions
+            elif supply_depot_recently_built:
+                return normalized_reward
+            else:
+                # print("Penalizing lack of supply depot construction during starvation")
+                return 0
+
         # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
 
         return normalized_reward
@@ -937,7 +949,7 @@ class DQNModel(nn.Module):
             for key, value in non_spatial_data.items():
                 fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
                 offset += 1
-                if offset >= 11:  # Adjust for a maximum 11 non-spatial data entries
+                if offset >= 12:  # Adjust for a maximum 12 non-spatial data entries
                     break
 
         # Then store the units information
@@ -1006,9 +1018,10 @@ class DQNAgent(base_agent.BaseAgent):
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
         self.last_camera_action = None
+        self.last_supply_depot_built_step = None
+
         # Queue that tracks last actions for exclusionary purposes
         self.last_three_actions = deque(maxlen=3)
-
         self.command_center = []
 
         # Action mapping
@@ -1170,6 +1183,8 @@ class DQNAgent(base_agent.BaseAgent):
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
             self.in_game_training_iterations = 0
+            supply_depot_recently_built = False
+            self.last_supply_depot_built_step = None
             # Clear our action queue
             self.last_three_actions.clear()
             self.last_camera_action = None
@@ -1285,9 +1300,12 @@ class DQNAgent(base_agent.BaseAgent):
             'supply_depot_count' : supply_depot_count,
             'idle_worker_count' : idle_worker_count,
             'army_count' : army_count,
+            'home_base_camera' : self.last_camera_action == 6
         }
 
-        print("Non-spatial metrics are set to: ", non_spatial_metrics)
+        # print("Non-spatial metrics are set to: ", non_spatial_metrics)
+        # print("Non-spatial barracks count is: ", non_spatial_metrics["barracks_count"])
+
 
         current_state = {
             "non_spatial": np.zeros(300),
@@ -1298,26 +1316,47 @@ class DQNAgent(base_agent.BaseAgent):
         current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
             rgb_minimap, self.base_top_left)
         
-        print("The output of our spatial data is set to: ", )
-        for i in range(20):
-            five_tuple = current_state["non_spatial"][i]
-            print(five_tuple)
-
-        # current_state["non_spatial"][0] = supply_depot_count
-        # current_state["non_spatial"][1] = barracks_count
-        # current_state["non_spatial"][2] = army_supply
-        # # Custom State Add-on
-        # current_state["non_spatial"][3] = command_center_count
-        # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
-        # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
-        # # A check to see where the camera is looking
-        # current_state["non_spatial"][6] = self.last_camera_action
-        # # spawn location boolean seems critical for learning
-        # current_state["non_spatial"][7] = self.base_top_left
-
-        # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
-
-        # print("State at the end of the step is set to: ", current_state)
+        # print("The output of our spatial data is set to: ", )
+        # for i in range(20):
+        #     five_tuple = current_state["non_spatial"][i]
+        #     print(five_tuple)
+
+        ''' Current State should look like this when passed to the model:
+        Non-spatial metrics are set to:  {'minerals': 1585, 'supply_limit': 31, 'supply_used': 31, 'supply_free': 0, 'army_supply': 14, 'worker_supply': 17, 'cc_count': 1, 'barracks_count': 6, 'supply_depot_count': 2, 'idle_worker_count': 8, 'army_count': 12, 'home_base_camera': True}
+        The output of our spatial data is set to: 
+        [1585.    0.    0.    0.    0.]
+        [31.  0.  0.  0.  0.]
+        [31.  0.  0.  0.  0.]
+        [0. 0. 0. 0. 0.]
+        [14.  0.  0.  0.  0.]
+        [17.  0.  0.  0.  0.]
+        [1. 0. 0. 0. 0.]
+        [6. 0. 0. 0. 0.]
+        [2. 0. 0. 0. 0.]
+        [8. 0. 0. 0. 0.]
+        [12.  0.  0.  0.  0.]
+        [1. 0. 0. 0. 0.]
+        [48.  1. 45. 24. 21.]
+        [45.  1. 45. -2. 53.]
+        [1.8e+01 1.0e+00 1.5e+03 7.0e+00 3.1e+01]
+        [  21.    1. 1000.   28.   59.]
+        [45.  1. 45. 31. 42.]
+        [ 19.   1. 400.  -2.  57.]
+        [45.  1. 45.  1. 54.]
+        [48.  1. 45. 15. 53.]
+        '''
+
+
+        # We check to see if we need to do any reward shaping to avoid supply starvation...
+        supply_depot_recently_built = False
+        # print("Game step is: ", self.actual_root_level_steps_taken)
+
+        if self.last_supply_depot_built_step is not None:
+            if (self.actual_root_level_steps_taken - self.last_supply_depot_built_step) <= 60:
+                # print("supply_depot_recently_built = True" )
+                supply_depot_recently_built = True
+            # else:
+                # print("supply_depot_recently_built = False" )
 
         # Push s/a/r/s_next our replay buffer
         # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
@@ -1325,7 +1364,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.actual_root_level_steps_taken += 1
             # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
             self.dqn_model.store_transition(self.previous_state,
-                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, non_spatial_metrics,supply_depot_recently_built), current_state)
 
         # Do in-game training of the model for every 100 root actions the agent takes
         if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
@@ -1333,6 +1372,10 @@ class DQNAgent(base_agent.BaseAgent):
             self.in_game_training_iterations += 1
             self.dqn_model.learn(self.target_network)
 
+        # Simple intent state tracker
+        if not hasattr(self, 'intended_action'):
+            self.intended_action = None
+
         if self.move_number == 0:
             self.move_number += 1
 
@@ -1371,11 +1414,9 @@ class DQNAgent(base_agent.BaseAgent):
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
-            # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
-            # We also get the agent to build at least three barracks before building >1 supply depot
 
-            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
+            if supply_free > 4 or self.last_camera_action != 6 or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
@@ -1383,8 +1424,8 @@ class DQNAgent(base_agent.BaseAgent):
             if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
 
-            # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
+            # Exclude marines from the build queue
+            if supply_free == 0 or barracks_count < 4 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1397,7 +1438,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(4)
 
             # SCV Checks
-            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
+            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 4:
                 excluded_actions.append(5)
 
             # Camera reset handling
@@ -1422,23 +1463,10 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
 
-            #  Excludes attacking home base
-            # These actions always target the home base after x/y transformation
-            # ---- Required for bootstrapping, but not after training due to reward structure ----
-            #
-            # for action in self.top_left_actions:
-            #     if action not in excluded_actions:
-            #         excluded_actions.append(action)
-
             # Prevent the bot from doing nothing in perpetuity
             # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
             # Safety check to make sure if no other options are available we don't crash...
 
-            # Old Logic - was specific to '0' but now we check for everything instead
-            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
-            #     # print("Excluding DO NOTHING")
-            #     excluded_actions.append(0)
-
             if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
                 action_to_exclude = self.last_three_actions[0]
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
@@ -1480,6 +1508,11 @@ class DQNAgent(base_agent.BaseAgent):
                     # Checks to avoid out-of-bounds crashing
                     # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
                     if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        # Update Intent State Tracking
+                        if smart_action == ACTION_BUILD_BARRACKS:
+                            self.intended_action = ACTION_BUILD_BARRACKS
+                        elif smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                            self.intended_action = ACTION_BUILD_SUPPLY_DEPOT
                         return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
                     else:
                         print(f"SCV coordinates out of range: {target}")
@@ -1583,46 +1616,50 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
-            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
-                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
-                    if self.cc_y.any():
-                        x_padding = random.randint(-30, 30)
-                        y_padding = random.randint(-30, 30)
-                        target_x = round(self.cc_x.mean()) - 35 + x_padding
-                        target_y = round(self.cc_y.mean()) + y_padding
-
-                        target = self.transformLocation(target_x, target_y)
-
-                        # Ensure the coordinates are within valid bounds after transformation
-                        target[0] = max(BORDER_PADDING, min(
-                            target[0], 83 - BORDER_PADDING))
-                        target[1] = max(BORDER_PADDING, min(
-                            target[1], 83 - BORDER_PADDING))
-
-                        # print("Trying to build a supply depot at:", target)
-                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
-
-            elif smart_action == ACTION_BUILD_BARRACKS:
-                # sleep to avoid loops
-
-                if _BUILD_BARRACKS in obs.observation['available_actions']:
-                    if self.cc_y.any():
-                        x_padding = random.randint(-30, 30)
-                        y_padding = random.randint(-30, 30)
-                        target_x = round(self.cc_x.mean()) - 35 + x_padding
-                        target_y = round(self.cc_y.mean()) + y_padding
-
-                        target = self.transformLocation(target_x, target_y)
-
-                        # Ensure the coordinates are within valid bounds after transformation
-                        # Assuming screen size is 84x84
-                        target[0] = max(BORDER_PADDING, min(
-                            target[0], 83 - BORDER_PADDING))
-                        target[1] = max(BORDER_PADDING, min(
-                            target[1], 83 - BORDER_PADDING))
-
-                        # print("Trying to build a barracks at:", target)
-                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+            if self.intended_action == ACTION_BUILD_SUPPLY_DEPOT and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+               self.last_supply_depot_built_step = self.actual_root_level_steps_taken
+               if self.cc_y.any():
+                    x_padding = random.randint(-30, 30)
+                    y_padding = random.randint(-30, 30)
+                    target_x = round(self.cc_x.mean()) - 35 + x_padding
+                    target_y = round(self.cc_y.mean()) + y_padding
+
+                    target = self.transformLocation(target_x, target_y)
+
+                    # Ensure the coordinates are within valid bounds after transformation
+                    target[0] = max(BORDER_PADDING, min(
+                        target[0], 83 - BORDER_PADDING))
+                    target[1] = max(BORDER_PADDING, min(
+                        target[1], 83 - BORDER_PADDING))
+
+                    # Start our timer for reward shaping
+
+                    # print("Trying to build a supply depot at:", target)
+                    return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif self.intended_action == ACTION_BUILD_BARRACKS and _BUILD_BARRACKS in obs.observation['available_actions']:
+                # print("Obs actions are:", obs.observation['available_actions'])
+                # print("Inside BUILD_BARRACKS")
+                if self.cc_y.any():
+                    # print("Inside SCV Check")
+                    x_padding = random.randint(-30, 30)
+                    y_padding = random.randint(-30, 30)
+                    target_x = round(self.cc_x.mean()) - 35 + x_padding
+                    target_y = round(self.cc_y.mean()) + y_padding
+
+                    target = self.transformLocation(target_x, target_y)
+
+                    # Ensure the coordinates are within valid bounds after transformation
+                    # Assuming screen size is 84x84
+                    target[0] = max(BORDER_PADDING, min(
+                        target[0], 83 - BORDER_PADDING))
+                    target[1] = max(BORDER_PADDING, min(
+                        target[1], 83 - BORDER_PADDING))
+                    
+                    # print("Trying to build barracks at: ", target)
+
+                    # print("Trying to build a barracks at:", target)
+                    return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
             elif smart_action == ACTION_BUILD_SCV:
@@ -1681,6 +1718,8 @@ class DQNAgent(base_agent.BaseAgent):
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 
+            self.intended_action = None
+    
         elif self.move_number == 2:
             self.move_number = 0
 

commit b34af81332dd2847112207c8bf9df395d2d90bff
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 18 09:16:09 2023 -0400

    Adding more non-spatial metrics, moved current_state out of root level actions

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index cf649643..1107bbc1 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -104,8 +104,8 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-17.pt_episode_15200_reward_-0.06.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-19-b.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-19-b.pt_episode_3400_reward_0.84.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-20-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -339,7 +339,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v19-self-b'
+        self.writer_path = 'runs/dqn-cnn-agent-v20-self-a'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 50000
         # This is our custom reward/action mapping dictionary
@@ -904,8 +904,8 @@ class DQNModel(nn.Module):
 
         return transformed_minimap
 
-    # This creates a fixed-length vector to store visible units in
-    def transform_units_to_fixed_length(self, units, base_top_left):
+    # This creates a fixed-length vector to store non-spatial data in (metrics and unit metadata)
+    def transform_non_spatial_to_fixed_length(self, units, base_top_left, non_spatial_data):
 
         # Safety checks
         # Check if units are None or empty
@@ -931,24 +931,32 @@ class DQNModel(nn.Module):
         # Create an empty array of shape (300, num_features)
         fixed_length_units = np.zeros((300, num_features))
 
-        # Fill the array with the actual values
-        for i in range(min(num_units, 300)):
+        # Embed the non-spatial game state information into the first entries
+        offset = 0
+        if non_spatial_data:
+            for key, value in non_spatial_data.items():
+                fixed_length_units[offset] = [value, 0, 0, 0, 0] # Input our metric in the first tuple with appropriate padding after
+                offset += 1
+                if offset >= 11:  # Adjust for a maximum 11 non-spatial data entries
+                    break
+
+        # Then store the units information
+        # Start after the reserved entries
+        for i in range(min(len(units), 300 - offset)):  # Adjusted based on the reserved non-spatial entries
             unit = units[i]
-            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
-            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
-            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+            index = i + offset  # Index into fixed_length_units
+            fixed_length_units[index, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[index, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[index, 2] = unit[HEALTH_INDEX]
 
             # Transform x, y coordinates as required for normalization
             if not base_top_left:
-                fixed_length_units[i, 3], fixed_length_units[i,
-                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
             else:
-                fixed_length_units[i, 3], fixed_length_units[i,
-                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+                fixed_length_units[index, 3], fixed_length_units[index, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
 
         return fixed_length_units
 
-
 # Agent Implementation
 
 
@@ -1243,10 +1251,15 @@ class DQNAgent(base_agent.BaseAgent):
         # if self.command_center:
         #     print("Command centers are set to:", self.command_center)
 
+        # Minerals
+        self.minerals = obs.observation['player'][1]
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
         army_supply = obs.observation['player'][5]
         worker_supply = obs.observation['player'][6]
+        idle_worker_count = obs.observation['player'][7]
+        army_count = obs.observation['player'][8]
+
         # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
         enemy_units = [
@@ -1258,51 +1271,71 @@ class DQNAgent(base_agent.BaseAgent):
 
         supply_free = supply_limit - supply_used
 
-        # Minerals
-        self.minerals = obs.observation['player'][1]
+
+
+        non_spatial_metrics = {
+            'minerals' : self.minerals,
+            'supply_limit' : supply_limit,
+            'supply_used' : supply_used,
+            'supply_free' : supply_free,
+            'army_supply' : army_supply,
+            'worker_supply' : worker_supply,
+            'cc_count' : command_center_count,
+            'barracks_count' : barracks_count,
+            'supply_depot_count' : supply_depot_count,
+            'idle_worker_count' : idle_worker_count,
+            'army_count' : army_count,
+        }
+
+        print("Non-spatial metrics are set to: ", non_spatial_metrics)
+
+        current_state = {
+            "non_spatial": np.zeros(300),
+            "rgb_minimap": None
+        }
+        current_state["non_spatial"] = self.dqn_model.transform_non_spatial_to_fixed_length(
+            visible_units, self.base_top_left,non_spatial_metrics)
+        current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+            rgb_minimap, self.base_top_left)
+        
+        print("The output of our spatial data is set to: ", )
+        for i in range(20):
+            five_tuple = current_state["non_spatial"][i]
+            print(five_tuple)
+
+        # current_state["non_spatial"][0] = supply_depot_count
+        # current_state["non_spatial"][1] = barracks_count
+        # current_state["non_spatial"][2] = army_supply
+        # # Custom State Add-on
+        # current_state["non_spatial"][3] = command_center_count
+        # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
+        # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
+        # # A check to see where the camera is looking
+        # current_state["non_spatial"][6] = self.last_camera_action
+        # # spawn location boolean seems critical for learning
+        # current_state["non_spatial"][7] = self.base_top_left
+
+        # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
+
+        # print("State at the end of the step is set to: ", current_state)
+
+        # Push s/a/r/s_next our replay buffer
+        # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+        if self.previous_action is not None:
+            self.actual_root_level_steps_taken += 1
+            # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+            self.dqn_model.store_transition(self.previous_state,
+                                            self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+
+        # Do in-game training of the model for every 100 root actions the agent takes
+        if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            # print("Beginning in-game training for the model.")
+            self.in_game_training_iterations += 1
+            self.dqn_model.learn(self.target_network)
 
         if self.move_number == 0:
             self.move_number += 1
 
-            current_state = {
-                "non_spatial": np.zeros(300),
-                "rgb_minimap": None
-            }
-            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
-                visible_units, self.base_top_left)
-            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
-                rgb_minimap, self.base_top_left)
-
-            # current_state["non_spatial"][0] = supply_depot_count
-            # current_state["non_spatial"][1] = barracks_count
-            # current_state["non_spatial"][2] = army_supply
-            # # Custom State Add-on
-            # current_state["non_spatial"][3] = command_center_count
-            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
-            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
-            # # A check to see where the camera is looking
-            # current_state["non_spatial"][6] = self.last_camera_action
-            # # spawn location boolean seems critical for learning
-            # current_state["non_spatial"][7] = self.base_top_left
-
-            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
-
-            # print("State at the end of the step is set to: ", current_state)
-
-            # Push s/a/r/s_next our replay buffer
-            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
-            if self.previous_action is not None:
-                self.actual_root_level_steps_taken += 1
-                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
-                self.dqn_model.store_transition(self.previous_state,
-                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
-
-            # Do in-game training of the model for every 100 root actions the agent takes
-            if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
-                # print("Beginning in-game training for the model.")
-                self.in_game_training_iterations += 1
-                self.dqn_model.learn(self.target_network)
-
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []
             # print("excluded_actions at the start are set to: ", excluded_actions)
@@ -1411,7 +1444,7 @@ class DQNAgent(base_agent.BaseAgent):
                 if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
                     excluded_actions.append(action_to_exclude)
 
-            # Updated for DQN - let the model  select the action
+            # DQN Selects the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
 
@@ -1717,7 +1750,7 @@ def main(args):
             print("Caught restart signal. Restarting environment...")
             continue
         except KeyboardInterrupt:
-            pass
+            break
         except Exception as e:
             print(f"An error occurred: {e}")
             print("Attempting to restart the SC2 environment.")

commit e5dc0ecbc253e7cea73b4899ec3ac9003beebb0f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 17 13:37:06 2023 -0400

    updating crash handling

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index b73355cc..cf649643 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -63,8 +63,10 @@ TRAINING_TYPE = "self"  # Possible values: "self", "kane", "bot"
 BOT_DIFFICULTY = "very_easy"
 
 
-# There is a predictable SC2 crash after every 986th game in multi-custom-agent environments, this code is a workaround
-_MAX_GAMES_BEFORE_RESTART = 1
+# There is a predictable SC2 SegFault after every 986th game in multi-custom-agent environments, this code is a workaround
+_MAX_GAMES_BEFORE_RESTART = 500
+class CustomRestartException(Exception):
+    pass
 
 # BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
 # https://github.com/skjb/pysc2-tutorial/tree/master
@@ -302,7 +304,7 @@ print("--------------------")
 # --------------------
 
 
-# Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 2048 (confirmed optimal via hyperparameter search)
+# Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 512 (confirmed optimal via hyperparameter search)
 # Technically I believe this architecture could be called a "Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
 # 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
@@ -310,7 +312,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -318,10 +320,10 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.001
-        # We decay over 3M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.final_epsilon = 0.01
+        # We decay over 2M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 3000000
+            self.epsilon - self.final_epsilon) / 2000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -337,9 +339,9 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v19-self-a'
+        self.writer_path = 'runs/dqn-cnn-agent-v19-self-b'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 250000
+        self.training_buffer_requirement = 50000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -1166,6 +1168,13 @@ class DQNAgent(base_agent.BaseAgent):
             for _ in range(3):
                 self.last_three_actions.append(None)
 
+            # There is a predictable SC2 Segfault after every 986th game currently, this code is a workaround
+            if self.episode_count % _MAX_GAMES_BEFORE_RESTART == 0:
+                print("----Restarting underlying SC2 Environment for Crash Avoidance!!----")
+                # Raising a custom exception to get caught in our main PySC2 run_loop to trigger restart
+                # This took...a lot of trial and error :(
+                raise CustomRestartException("Time to restart the environment!")
+
             return actions.FunctionCall(_NO_OP, [])
 
         # BOILER PLATE Action-Space Guardrails
@@ -1682,8 +1691,8 @@ def main(args):
     RGB_SCREEN_SIZE = 84
     RGB_MINIMAP_SIZE = 64
 
-    try:
-        while True:
+    while True:
+        try:
             with sc2_env.SC2Env(
                 map_name="Simple64",
                 players=[
@@ -1703,19 +1712,17 @@ def main(args):
                 visualize=False
             ) as env:
                 run_loop.run_loop([agent1, agent2], env)
-                # There is a predictable crash after every 986th game currently, this code is a workaround
-                if agent1.episode_count % _MAX_GAMES_BEFORE_RESTART == 0:
-                    print(f"Games played is: {agent1.episode_count}")
-                    print("Restarting underlying SC2 Environment!")
-                    # the full SC2 environment will be recreated at the start of the next iteration of the main loop
-
-    except KeyboardInterrupt:
-        pass
-    except Exception as e:
-        print(f"An error occurred: {e}")
-        print("Attempting to restart the SC2 environment.")
-        # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
 
+        except CustomRestartException:
+            print("Caught restart signal. Restarting environment...")
+            continue
+        except KeyboardInterrupt:
+            pass
+        except Exception as e:
+            print(f"An error occurred: {e}")
+            print("Attempting to restart the SC2 environment.")
+            # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
+            continue
 
 if __name__ == "__main__":
     app.run(main)

commit 2719783f85bbba8833ce49303b68d9234a0dee53
Author: Craig Dods <cdods@meta.com>
Date:   Sun Sep 17 13:06:29 2023 -0400

    diagram of kane added

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 9b7119ae..b73355cc 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -321,7 +321,7 @@ class DQNModel(nn.Module):
         self.final_epsilon = 0.001
         # We decay over 3M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 300000
+            self.epsilon - self.final_epsilon) / 3000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size

commit bb46610300a9c1e03cf66b1fada8450a0c2ba906
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 17 07:54:10 2023 -0400

    updating agents

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
index 6dabcf6e..1dd2f867 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -56,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-17.pt_episode_14200_reward_-0.17.pt'
+DATA_FILE = 'dqn-cnn-agent-model-17.pt_episode_15200_reward_-0.06.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -1366,8 +1366,8 @@ class DQNAgent(base_agent.BaseAgent):
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
 
-            print("Agent action choice: ", rl_action)
-            print("Excluded actions were: ", excluded_actions)
+            # print("Agent action choice: ", rl_action)
+            # print("Excluded actions were: ", excluded_actions)
 
             self.previous_state = current_state
             self.previous_action = rl_action
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 00ae6980..9b7119ae 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -58,7 +58,7 @@ from pysc2.env import sc2_env
 from pysc2.env import run_loop
 
 # Global Configuration Knobs
-TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+TRAINING_TYPE = "self"  # Possible values: "self", "kane", "bot"
 # Possible values: "very_easy", "easy", "medium", "hard" (only used if TRAINING_TYPE = "bot")
 BOT_DIFFICULTY = "very_easy"
 
@@ -102,8 +102,8 @@ ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 #
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-18.pt_episode_600_reward_-0.56.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-19-a.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-17.pt_episode_15200_reward_-0.06.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-19-b.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -337,7 +337,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v18-kane-a'
+        self.writer_path = 'runs/dqn-cnn-agent-v19-self-a'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
@@ -490,6 +490,7 @@ class DQNModel(nn.Module):
         # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
         #####
 
+
     # This function goes back and tweaks the rewards associated with a given game
     # Based on the final tangible reward we get from PySC2
     # Win/loss/draw are multipliers

commit 861ee97f966ac5fa88cdf88aade65ca2828cfa71
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 17 07:45:23 2023 -0400

    Global training config knobs instead of argparsing

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
index 7ed3bce9..00ae6980 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -3,16 +3,9 @@ Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, w
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
-Out of the 1600~ Lines-of-Code, approximately 1300~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+Out of the 1700~ Lines-of-Code, approximately 1400~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
 
-To begin training against a built-in Starcraft II bot (very_easy recommended to begin), use this string:
-python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush
-
-To begin Self-Training (Self-Play reinforcement Learning), use this string:
-python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
-
-To train against Kane-AI, use this string:
-python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
+To modify training behaviour (Self-Reinforcement, Built-in Bots, or Kane, modify the TRAINING_TYPE Global knob on line 61)
 
 ---- add to readme:
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
@@ -27,8 +20,7 @@ required as after 1k episodes we run out of FD's:
 FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
 '''
 
-
-# Import our other agent
+# Import our other agents
 from Kane_AI import KaneAI
 from test_abel_ai import DQNAgent as test_DQN_Agent
 
@@ -36,7 +28,6 @@ import random
 from random import sample as random_sample
 import time
 import math
-import argparse
 
 from absl import app
 import os
@@ -65,17 +56,15 @@ from pysc2.lib import features
 from pysc2.lib import units
 from pysc2.env import sc2_env
 from pysc2.env import run_loop
-# Environment Setup
-# There is a predictable crash after every 986th game currently, this code is a workaround
-_MAX_GAMES_BEFORE_RESTART = 1
 
-# Command line parsing
-def parse_arguments():
-    parser = argparse.ArgumentParser(description="SC2 Agent Trainer")
-    parser.add_argument("training_type", choices=["self", "kane", "bot"], help="Choose training scenario: self, kane or bot")
-    parser.add_argument("--difficulty", choices=["very_easy", "easy", "medium", "hard"], default="very_easy", help="Bot difficulty level")
-    return parser.parse_args()
+# Global Configuration Knobs
+TRAINING_TYPE = "kane"  # Possible values: "self", "kane", "bot"
+# Possible values: "very_easy", "easy", "medium", "hard" (only used if TRAINING_TYPE = "bot")
+BOT_DIFFICULTY = "very_easy"
+
 
+# There is a predictable SC2 crash after every 986th game in multi-custom-agent environments, this code is a workaround
+_MAX_GAMES_BEFORE_RESTART = 1
 
 # BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
 # https://github.com/skjb/pysc2-tutorial/tree/master
@@ -83,7 +72,6 @@ def parse_arguments():
 # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
 # His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
 
-
 ################################## Start of BoilerPlate Code #####################################################
 _NO_OP = actions.FUNCTIONS.no_op.id
 _SELECT_POINT = actions.FUNCTIONS.select_point.id
@@ -1678,12 +1666,16 @@ class DQNAgent(base_agent.BaseAgent):
 def main(args):
     agent1 = DQNAgent()
     # Depending on the training type, create agent2
-    if args.training_type == "self":
+    # Self-Reinforcement Learning
+    if TRAINING_TYPE == "self":
         agent2 = test_DQN_Agent()
-    elif args.training_type == "kane":
+    elif TRAINING_TYPE == "kane":
         agent2 = KaneAI()
-    else:  # "bot"
-        agent2 = sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[args.difficulty.upper()])
+    elif TRAINING_TYPE == "bot":
+        agent2 = sc2_env.Bot(sc2_env.Race.terran,
+                             sc2_env.Difficulty[BOT_DIFFICULTY.upper()])
+    else:
+        raise ValueError(f"Unknown TRAINING_TYPE: {TRAINING_TYPE}")
 
     USE_FEATURE_UNITS = True
     RGB_SCREEN_SIZE = 84
@@ -1695,13 +1687,15 @@ def main(args):
                 map_name="Simple64",
                 players=[
                     sc2_env.Agent(sc2_env.Race.terran),
-                    sc2_env.Agent(sc2_env.Race.terran), 
+                    sc2_env.Agent(sc2_env.Race.terran),
                 ],
                 agent_interface_format=features.AgentInterfaceFormat(
                     action_space=actions.ActionSpace.RGB,
                     use_feature_units=USE_FEATURE_UNITS,
-                    rgb_dimensions=features.Dimensions(screen=RGB_SCREEN_SIZE, minimap=RGB_MINIMAP_SIZE),
-                    feature_dimensions=features.Dimensions(screen=84, minimap=64)
+                    rgb_dimensions=features.Dimensions(
+                        screen=RGB_SCREEN_SIZE, minimap=RGB_MINIMAP_SIZE),
+                    feature_dimensions=features.Dimensions(
+                        screen=84, minimap=64)
                 ),
                 step_mul=16,
                 game_steps_per_episode=21000,
@@ -1714,7 +1708,6 @@ def main(args):
                     print("Restarting underlying SC2 Environment!")
                     # the full SC2 environment will be recreated at the start of the next iteration of the main loop
 
-
     except KeyboardInterrupt:
         pass
     except Exception as e:
@@ -1722,6 +1715,6 @@ def main(args):
         print("Attempting to restart the SC2 environment.")
         # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
 
+
 if __name__ == "__main__":
-    args = parse_arguments()
-    app.run(main(args))
\ No newline at end of file
+    app.run(main)

commit 0ccb18b045b2f442dc1fcef77fa765dd7df304eb
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 17 07:25:15 2023 -0400

    Adding testing and training for abel

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
new file mode 100644
index 00000000..6dabcf6e
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_abel_ai.py
@@ -0,0 +1,1637 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Train the agent against a easy Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
+
+# Test against Kane-AI with this string:
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
+# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
+# craig		soft	nofile		8192
+# craig 	hard	nofile		1048576
+# Additionally, tensorboard needs to be run as root (due to higher FD limits):
+# ulimit -n 1048576
+# tensorboard --logdir=runs
+#
+# Validated with: ulimit -s -H && ulimit -n -S
+# required as after 1k episodes we run out of FD's:
+# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+
+import random
+from random import sample as random_sample
+import time
+import math
+import os
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+
+# Using TensorBoard for model performance tracking & visualizations
+from torch.utils.tensorboard import SummaryWriter
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...quite poor excellent.
+from torch.optim.lr_scheduler import CosineAnnealingLR
+from collections import Counter
+# Using itertools to slice deque's efficiently
+import itertools
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+from pysc2.lib import units
+# queue used for replay buffer
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-cnn-agent-model-17.pt_episode_14200_reward_-0.17.pt'
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+# CUSTOM
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+# END CUSTOM
+
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+# CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
+ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
+ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
+ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
+# END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
+    ACTION_MOVE_CAMERA_SELF_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
+]
+
+# DQN Non-Partial State size
+STATE_SIZE = 2
+
+################################## End of BoilerPlate Code #####################################################
+
+# Custom library of all terran buildings
+TERRAN_BUILDINGS = [
+    18,  # Command Center
+    19,  # Supply Depot
+    20,  # Refinery
+    21,  # Barracks
+    22,  # Orbital Command
+    23,  # Factory
+    24,  # Starport
+    25,  # Engineering Bay
+    26,  # Fusion Core
+    27,  # Tech Lab (Barracks)
+    28,  # Tech Lab (Factory)
+    29,  # Tech Lab (Starport)
+    30,  # Reactor (generic, as the building morphs)
+    37,  # Sensor Tower
+    38,  # Bunker
+    39,  # Missile Turret
+    40,  # Auto-turret (from Raven)
+    58,  # Planetary Fortress
+]
+
+TERRAN_UNITS = {
+    # Worker
+    45: "SCV",
+
+    # Basic
+    48: "MULE",
+    51: "Marine",
+    53: "Marauder",
+    54: "Reaper",
+    55: "Ghost",
+
+    # Factory
+    57: "Hellion",
+    58: "SiegeTank",
+    59: "Cyclone",
+    62: "Thor",
+    64: "Hellbat",
+
+    # Starport
+    35: "VikingFighter",
+    36: "VikingAssault",
+    67: "Medivac",
+    68: "Banshee",
+    69: "Raven",
+    70: "Battlecruiser",
+    132: "Liberator",
+    71: "AutoTurret",  # Spawned by Raven
+
+    # Other
+    102: "WidowMine",
+    488: "LiberatorAG"
+}
+
+# Define the top-left coordinates for each quadrant
+quadrants = [
+    (0, 0),       # Top left quadrant
+    (32, 0),      # Top right quadrant
+    (0, 32),      # Bottom left quadrant
+    (32, 32)      # Bottom right quadrant
+]
+
+# Calculate the offset points for each quadrant
+# This is used in the smart_attack functions (16 locations with offsets)
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
+
+
+def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
+    corner_offset = 3
+    mini_quadrant_size = 16  # Each quadrant is divided further into 4
+    
+    if quadrant == "top-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + corner_offset),                      # Top-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + corner_offset), # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "top-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + corner_offset),                   # Top-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-right of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size - corner_offset),                   # Bottom-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset), # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-right of bottom-right mini quadrant
+        ]
+
+    return points
+
+# For each quadrant, calculate the offset points and append the attack action
+quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
+for i, quad in enumerate(quadrants):
+    points = calculate_quadrant_points(*quad, quadrant_names[i])
+    for x, y in points:
+        smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
+
+# print("smart_actions is set to: ", smart_actions)
+print("--------------------")
+
+print("# Action Mapping")
+for index, action in enumerate(smart_actions):
+    print(f"# {index}: '{action}'")
+print("--------------------")
+
+# --------------------
+# # Action Mapping
+# # 0: 'donothing'
+# # 1: 'buildsupplydepot'
+# # 2: 'buildbarracks'
+# # 3: 'buildmarine'
+# # 4: 'attackunit'
+# # 5: 'buildscv'
+# # 6: 'resetcamera'
+# # 7: 'mcselfexpansion'
+# # 8: 'mcenemyexpansion'
+# # 9: 'mcenemyprimary'
+# # 10: 'attack_1_1'
+# # 11: 'attack_13_1'
+# # 12: 'attack_1_13'
+# # 13: 'attack_13_13'
+# # 14: 'attack_33_1'
+# # 15: 'attack_45_1'
+# # 16: 'attack_33_13'
+# # 17: 'attack_45_13'
+# # 18: 'attack_1_33'
+# # 19: 'attack_13_33'
+# # 20: 'attack_1_45'
+# # 21: 'attack_13_45'
+# # 22: 'attack_33_33'
+# # 23: 'attack_45_33'
+# # 24: 'attack_33_45'
+# # 25: 'attack_45_45'
+# --------------------
+
+
+# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+# Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
+# To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
+# 1. The 1.2M size deque where state/actions are appended/popped directly in O(1) time
+# 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
+# The drawback of course is that space complexity is 2N...
+class DQNModel(nn.Module):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0, gamma=0.9, e_greedy=0.001, buffer_capacity=5000, batch_size=1024):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = gamma
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.01
+        # We decay over 2.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (
+            self.epsilon - self.final_epsilon) / 2500000
+        self.action_counter = 0
+        #
+        self.state_size = non_spatial_state_size
+        self.disallowed_actions = {}
+        # Replay buffer deque and list
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque(maxlen=buffer_capacity)
+        self.training_buffer = []
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+        # Counter for tracking how frequently training is run
+        self.global_training_steps = 0
+        # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
+        self.writer_path = 'runs/dqn-cnn-agent-v15b-testing'
+        # Our replay buffer training theshold size
+        # self.training_buffer_requirement = 500000
+        self.training_buffer_requirement = 150000
+        # This is our custom reward/action mapping dictionary
+        # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        self.action_rewards = {
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
+            1: 1,  # 'buildsupplydepot'
+            2: 0.95,   # 'buildbarracks'
+            3: 0.8,  # 'buildmarine'
+            4: 0.8,  # 'attackunit' - High reward, attack visible units
+            5: 0.5,   # 'buildscv'
+            6: 0.6,   # 'resetcamera' - High reward as is required when available
+            7: 0.05,   # 'mcselfexpansion' - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
+            9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
+            10: 0,    # 'attack_4_4'
+            11: 0,    # 'attack_12_4'
+            12: 0,    # 'attack_4_12'
+            13: 0,    # 'attack_12_12'
+            14: 0,    # 'attack_36_4'
+            15: 0,    # 'attack_44_4'
+            16: 0,    # 'attack_36_12'
+            17: 0,    # 'attack_44_12'
+            18: 0,    # 'attack_4_36'
+            19: 0,    # 'attack_12_36'
+            20: 0,    # 'attack_4_44'
+            21: 0,    # 'attack_12_44'
+            22: 0,    # 'attack_36_36'
+            23: 0,    # 'attack_44_36'
+            24: 0,    # 'attack_36_44'
+            25: 0     # 'attack_44_44'
+        }
+        # Setting up quadrant mapping to avoid repetitive copies in the hot path
+        # Define which actions correspond to which quadrants
+        # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
+        # With the function transformLocation()
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        # CNN for RGB minimap with normalisation
+        self.conv = nn.Sequential(
+            nn.Conv2d(in_channels=3, out_channels=32,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(32),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=32, out_channels=64,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(64),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=64, out_channels=128,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(128),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+        ).to(self.device)  # Moving to GPU if available
+
+        # Calculate the size of our flattened output after convolutional layers
+        self.conv_out_size = self._get_conv_out(minimap_shape)
+
+        # Fully Connected Network (FCN) for non-spatial data with normalization
+        self.fc_non_spatial = nn.Sequential(
+            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
+            nn.ReLU(),
+            nn.Linear(128, 256),
+            nn.BatchNorm1d(300),
+            nn.ReLU()
+        ).to(self.device)  # Moving to GPU if available
+
+        # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
+        self.fc_decision = nn.Sequential(
+            nn.Linear(84992, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, 4096),
+            nn.ReLU(),
+
+            nn.Linear(4096, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, len(self.actions))
+        ).to(self.device)  # Moving to GPU if available
+
+        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+        # Our learning rate scheduler is enabled here (CosineAnnealing)
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=15000)
+
+    #
+    def _get_conv_out(self, shape):
+        o = self.conv(torch.zeros(1, *shape).to(self.device))
+        return int(np.prod(o.size()))
+
+    # This is an implicit PyTorch function that's called automatically
+    def forward(self, non_spatial_data, minimap):
+        non_spatial_data = non_spatial_data.to(self.device)
+        minimap = minimap.to(self.device)
+
+        # Separate treatments for different types of data
+        conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
+        fc_out = self.fc_non_spatial(non_spatial_data).reshape(
+            non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
+
+        # Combine both outputs
+        combined = torch.cat((conv_out, fc_out), dim=1)
+
+        return self.fc_decision(combined)
+
+    # Using a method to handle writes to ensure things are closed properly in the event of a crash
+
+    def get_writer(self):
+        return SummaryWriter(self.writer_path)
+
+    # This is where we store items for our replay buffer
+    # s: The current state of the environment.
+    # a: The action taken by the agent in state s.
+    # r: The reward received after taking action a in state s.
+    # s_next: The resulting state after taking
+    def store_transition(self, s, a, r, s_next):
+        # Transition is a tuple (s, a, r, s_next)
+        transition = (s, a, r, s_next)
+        # Append the transition to the replay buffer
+        self.buffer.append(transition)
+
+    def transfer_buffer(self):
+        self.training_buffer = list(self.buffer)
+
+        #####
+        # Debugging
+        # Extract the components of the last element in the buffer
+        # s, a, r, s_next = self.training_buffer[-1]
+
+        # # Print the components
+        # print("s:", s)
+        # print("a:", a)
+        # print("r:", r)
+        # print("s_next:", s_next)
+
+        # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
+        #####
+
+    # This function goes back and tweaks the rewards associated with a given game
+    # Based on the final tangible reward we get from PySC2
+    # Win/loss/draw are multipliers
+    # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
+    # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
+    def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
+        # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
+        steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
+
+        # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
+        for i in range(-steps_to_iterate, 0):
+            # Skip if the buffer index result is None
+            if self.buffer[i] is None:
+                continue
+            # Fetch the transition
+            s, a, r, s_next = self.buffer[i]
+
+            # Modify the reward to include the final reward
+            new_reward = r * final_reward
+            # print("Before backpropagation: ", r)
+
+            # Replace the transition in the replay buffer
+            self.buffer[i] = (s, a, new_reward, s_next)
+            # print("After backpropagation: ", self.buffer[i][2])
+
+        print("Replay buffer currently has: ",
+              len(self.training_buffer), "entries")
+
+    # # sample transitions from replay buffer queue for the last game
+    # def sample(self, batch_size):
+    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+    #     return list(self.buffer)[-batch_size:]
+
+    # Sample random transitions from replay buffer
+    # Hopefully in a stochastic manner...
+
+    def random_sample(self, batch_size):
+        return random_sample(self.training_buffer, batch_size)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Implementation that Steven Brown (PySC2 Dev) created here:
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
+    def choose_action(self, current_state, excluded_actions=[]):
+        # Extract non_spatial_data and rgb_minimap from current_state
+        non_spatial_data = current_state["non_spatial"]
+        rgb_minimap = current_state["rgb_minimap"]
+
+        # Debugs
+        # print("Non-spatial data shape before unsqueeze:", non_spatial_data.shape)
+
+        # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
+        self.eval()
+        # Convert data to tensors and move them to the GPU
+        non_spatial_data_tensor = torch.tensor(
+            non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
+        # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
+        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
+            2, 0, 1).unsqueeze(0).to(self.device)
+
+        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
+
+        # Epsilon-based exploration
+        # Disabled for testing
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+        #     # This is where we keep the logic for epsilon decay (linear)
+        #     self.action_counter += 1
+        #     self.epsilon -= self.epsilon_decay_rate
+        #     self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+            # Pass the minimap data through the CNN
+            conv_output = self.conv(rgb_minimap_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            conv_output = conv_output.view(conv_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
+
+            # print("Conv shape is: ", conv_output.shape)
+            # print("Non spatial shape is: ", non_spatial_output.shape)
+
+            # Concatenate the two outputs along the second dimension (feature axis)
+            combined_output = torch.cat(
+                (conv_output, non_spatial_output), dim=1)
+
+            # Squeeze out the dummy third dimension
+            combined_output = combined_output.squeeze(2)
+
+            # Finally, pass through the decision-making layers
+            q_values = self.fc_decision(combined_output)
+
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+            # print("The action we chose was: ", action)
+            # print("The excluded actions were: ", excluded_actions)
+
+        # Turn training mode back on
+        return action
+
+    # This is where we train the model
+    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
+    def learn(self):
+        # Check if the replay buffer has enough samples (using 500K as minimum)
+        if len(self.training_buffer) < self.training_buffer_requirement:
+            print("Replay buffer is currently too small to conduct training...")
+            return
+
+        # Increment our training counter
+        self.global_training_steps += 1
+
+        # Sample a mini-batch of transitions from the buffer
+        transitions = self.random_sample(self.batch_size)
+        # Unzip the transitions into separate variables
+        states, actions, rewards, next_states = zip(*transitions)
+
+        # Separate non_spatial and rgb_minimap components of states
+        non_spatial_states = [state["non_spatial"] for state in states]
+        rgb_minimap_states = [state["rgb_minimap"] for state in states]
+
+        non_spatial_next_states = [state["non_spatial"]
+                                   for state in next_states]
+        rgb_minimap_next_states = [state["rgb_minimap"]
+                                   for state in next_states]
+
+        # Convert the zipped values to numpy arrays
+        non_spatial_states_np = np.array(non_spatial_states, dtype=np.float32)
+        rgb_minimap_states_np = np.array(rgb_minimap_states, dtype=np.float32)
+        non_spatial_next_states_np = np.array(
+            non_spatial_next_states, dtype=np.float32)
+        rgb_minimap_next_states_np = np.array(
+            rgb_minimap_next_states, dtype=np.float32)
+        actions_np = np.array(actions, dtype=np.int64)
+        rewards_np = np.array(rewards, dtype=np.float32)
+
+        # Convert numpy arrays to tensors
+        non_spatial_states = torch.tensor(
+            non_spatial_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(
+            self.device).permute(0, 3, 1, 2)
+        non_spatial_next_states = torch.tensor(
+            non_spatial_next_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_next_states = torch.tensor(
+            rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2)
+
+        actions = torch.tensor(actions_np).to(self.device)
+        rewards = torch.tensor(rewards_np).to(self.device)
+
+        # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
+        with autocast():
+            # Compute the Q-values for the current states
+            q_values = self(non_spatial_states, rgb_minimap_states)
+            q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+
+            # Compute the target Q-values
+            next_q_values = self(non_spatial_next_states,
+                                 rgb_minimap_next_states)
+            max_next_q_values = next_q_values.max(1)[0]
+            q_target = rewards + self.gamma * max_next_q_values
+
+            # Compute the loss and update the model's weights
+            loss = self.loss_fn(q_predict, q_target)
+
+            # Debugs if required...
+            #     print("Q-target contains NaN or Infinity!")
+            # if torch.isnan(loss).any() or torch.isinf(loss).any():
+            #     print("Loss contains NaN or Infinity!")
+            # if torch.isnan(q_values).any() or torch.isinf(q_values).any():
+            #     print("Q-values contain NaN or Infinity!")
+            # if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
+            #     print("Non-spatial states contain NaN or Infinity!")
+            # if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
+            #     print("RGB Minimap states contain NaN or Infinity!")
+            # if torch.isnan(rewards).any() or torch.isinf(rewards).any():
+            #     print("Rewards contain NaN or Infinity!")
+            # if math.isnan(self.gamma) or math.isinf(self.gamma):
+            #     print("Gamma contains NaN or Infinity!")
+            # if torch.isnan(actions).any() or torch.isinf(actions).any():
+            #     print("Actions contain NaN or Infinity!")
+
+        # Clear accumulated gradients before back propagation
+        self.optimizer.zero_grad()
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Clip the gradient to avoid huge updates
+        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        # Update the learning rate based on CosineAnnealing scheduling
+        self.scheduler.step()
+
+        # These logs are generated less frequently (every 25 training runs)
+        # Try/Except blocks as they've routinely crashed the simulation :(
+        if self.global_training_steps % 25 == 0:
+            # Logging various metrics for visualization and debugging
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Loss/train', loss.item(),
+                                      self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Loss/train: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Epsilon/value', self.epsilon, self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Epsilon/value: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_histogram(
+                        'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Q-Values: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Learning Rate: {e}")
+
+            # Log the histograms of model weights
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    for name, param in self.named_parameters():
+                        writer.add_histogram(name, param.clone().cpu(
+                        ).data.numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging model weights: {e}")
+
+            # Create an action frequency histogram of the last 1000 actions in the replay buffer
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    last_1000_actions = list(itertools.islice(self.training_buffer, len(
+                        self.training_buffer) - 1000, len(self.training_buffer)))
+                    actions = [transition[1]
+                               for transition in last_1000_actions]
+                    action_frequencies = Counter(actions)
+
+                    # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...)
+                    for i in range(len(smart_actions)):
+                        action_frequencies[i] = action_frequencies.get(i, 0)
+
+                    action_list_for_histogram = [
+                        action for action, freq in action_frequencies.items() for _ in range(freq)]
+
+                    writer.add_histogram(
+                        'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Actions/Frequency: {e}")
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint weights
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.load_state_dict(torch.load(file_path))
+
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything about 12k score results in a full score being provided to the model
+        max_score = 12000
+        # Normalize the score to be between 0 and 1
+        normalized_reward = min(score / max_score, 1)
+
+        # --------------------
+        # Action Mapping Reference
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 1
+        attack_adjacent_quadrant = 0.5
+        penalty_home_quadrant = -1
+        penalty_home_expansion = -0.5
+
+        # Incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if previous_action in self.bottom_right_actions:
+            normalized_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            normalized_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            normalized_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            normalized_reward += penalty_home_expansion
+
+        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
+        action_reward = self.action_rewards.get(previous_action, 0)
+        # Add the action reward, ensuring the total reward stays between 0 and 1
+        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+
+        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
+
+        return normalized_reward
+
+    # Provide the x/y coordinates of our command center(s)
+    def get_command_center_coordinates(self, obs):
+        # Get Command Centers using the get_units_by_type function
+        command_centers = self.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+
+        # If there's a Command Center, return its coordinates
+        if command_centers:
+            # Grabbing the first for now (no expansion support for cameras)
+            command_center = command_centers[0]
+
+            # Checks to avoid out-of-bounds crashing
+            # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+            if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
+                return command_center.x, command_center.y
+            else:
+                print(
+                    f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                return None, None
+        else:
+            print("Command Center not found.")
+            return None, None
+
+    # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
+    def translate_coordinates(self, x, y, original_size=84, target_size=64):
+        scale_factor = target_size / original_size
+        return int(x * scale_factor), int(y * scale_factor)
+
+    # To ensure consistency across spawn locations, we invert the minimap locations based on base_top_left
+    def transform_minimap(self, minimap_data, base_top_left):
+        if not base_top_left:
+            # Flip only the spatial dimensions, leaving the color channels unchanged
+            # Copy required due to oddities in the return values..
+            # PyTorch can't deal with negative strides AFAIK
+            transformed_minimap = np.flip(minimap_data, axis=(0, 1)).copy()
+        else:
+            # Leave the minimap as it is
+            transformed_minimap = minimap_data
+
+        return transformed_minimap
+
+    # This creates a fixed-length vector to store visible units in
+    def transform_units_to_fixed_length(self, units, base_top_left):
+
+        # Safety checks
+        # Check if units are None or empty
+        if units is None or len(units) == 0:
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Check for NaN values
+        if np.isnan(np.array(units)).any():
+            print("Warning: NaN values detected in units!")
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Constant indices based on the field names
+        UNIT_TYPE_INDEX = 0
+        ALLIANCE_INDEX = 1
+        HEALTH_INDEX = 2
+        X_POS_INDEX = 12
+        Y_POS_INDEX = 13
+
+        # Number of units and features
+        num_units = len(units)
+        num_features = 5  # ['unit_type', 'health', 'x', 'y']
+
+        # Create an empty array of shape (300, num_features)
+        fixed_length_units = np.zeros((300, num_features))
+
+        # Fill the array with the actual values
+        for i in range(min(num_units, 300)):
+            unit = units[i]
+            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+
+            # Transform x, y coordinates as required for normalization
+            if not base_top_left:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+            else:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+
+        return fixed_length_units
+
+
+# Agent Implementation
+
+
+class DQNAgent(base_agent.BaseAgent):
+
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Available actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+
+        self.previous_action = None
+        self.previous_state = {
+            "non_spatial": None,
+            "rgb_minimap": None
+        }
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        self.minerals = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.training_time = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+        self.actual_root_level_steps_taken = 0
+        self.in_game_training_iterations = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.unit_attack_delay_timer = 0
+        self.supply_delay_timer = 0
+        self.barracks_delay_timer = 0
+        self.scv_delay_timer = 0
+        self.camera_move_timer = 0
+        self.last_camera_action = None
+        # Queue that tracks last actions for exclusionary purposes
+        self.last_three_actions = deque(maxlen=3)
+
+        self.command_center = []
+
+        # Action mapping
+        # Define which actions correspond to which quadrants
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.unit_attack_delay_timer += 1
+        self.supply_delay_timer += 1
+        self.barracks_delay_timer += 1
+        self.scv_delay_timer += 1
+        self.camera_move_timer += 1
+        # Minerals
+        self.minerals = obs.observation['player'][1]
+
+        # Visuals
+        rgb_minimap = obs.observation["rgb_minimap"]
+
+        # Check our current score, just for debugging
+        # print("Current score is: ", self.get_normalized_reward(obs))
+
+        # print("self.cc is: ", self.cc_y)
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                # print("Extra steps are: ", extra_steps)
+                # print("total sets were: ", episode_steps)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+                # Ensure combined_reward never goes below 1.1 for a win
+                combined_reward = max(combined_reward, 1.2)
+                final_reward_multiplier = combined_reward
+            elif base_reward == 0:  # 0 indicates a draw
+                combined_reward = base_reward
+                final_reward_multiplier = 0.7  # Slight decrease in score for a draw
+            else:  # -1 indicates a loss
+                combined_reward = base_reward
+                final_reward_multiplier = 0.25
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the rolling average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            print("------------------------------------------------------")
+            # print("Combined reward is set to: ", combined_reward)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            # Checkpoint the model every 200 games
+            # if self.episode_count % 200 == 0:
+            #     print("Checkpointing our model...")
+            #     self.dqn_model.save_model(
+            #         DATA_FILE, self.episode_count, avg_reward)
+
+            # print("Previous average reward was: ",
+            #       self.previous_avg_reward)
+            # print("Our rolling-average reward is: ", avg_reward)
+            # print("Latest game reward was: ", combined_reward)
+            # print("Number of steps were: ", episode_steps)
+            # # Backpropagate the final reward multiplier to previous actions
+            # print(
+            #     f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
+            # self.dqn_model.backpropagate_final_reward(
+            #     final_reward_multiplier, self.actual_root_level_steps_taken)
+
+            # # Copy over our deque replay buffer into our list if it's been 10 games
+            # if self.episode_count % 10 == 0:
+            #     self.dqn_model.transfer_buffer()
+
+            # Print statements if our buffer is large enough to train on...
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                print("Number of in-game model updates: ",
+                      self.in_game_training_iterations)
+                print("Training the model after game completion...")
+            # Learn after every game, not just the successful ones:
+
+            # Log our reward over time and training duration if we've started training
+            # Learning disabled for testing
+            # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     training_start_time = time.time()
+            #     self.dqn_model.learn()
+            #     training_end_time = time.time() - training_start_time
+            #     self.training_time.append(training_end_time)
+            #     avg_training_time = sum(self.training_time) / len(self.training_time)
+            #     print(
+            #         f"This training loop took {training_end_time:.4f} seconds.")
+            #     print("Training complete")
+            #     # Log our average reward to TensorBoard
+            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+            #         writer.add_scalar(
+            #             'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+            #     # Log training time
+            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+            #         writer.add_scalar('Average Training Time in Seconds/train_duration',
+            #                           avg_training_time, self.dqn_model.global_training_steps)
+
+            # Reset remaining, episode-specific counters
+            print("------------------------------------------------------")
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+            self.actual_root_level_steps_taken = 0
+            self.in_game_training_iterations = 0
+            # Clear our action queue
+            self.last_three_actions.clear()
+            for _ in range(3):
+                self.last_three_actions.append(None)
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            # Original logic, doesn't work properly...
+            # player_y, player_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            # # print("Player x and y are set to: ", self.player_x, self.player_y)
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+            # Using similar approach to Kane AI to Figure out where our home base is
+            # Original reference is here of course:
+            # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+
+            player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                  features.PlayerRelative.SELF).nonzero()
+            xmean = player_x.mean()
+            ymean = player_y.mean()
+
+            if xmean <= 31 and ymean <= 31:
+                self.base_top_left = True
+            else:
+                self.base_top_left = False
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # supply_depot_count = int(round(len(depot_y) / 69))
+        all_supply_depots = self.dqn_model.get_units_by_type(
+            obs, units.Terran.SupplyDepot)
+        supply_depot_count = len(
+            [unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        all_barracks = self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks)
+        barracks_count = len(
+            [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+
+        all_command_centers = self.dqn_model.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+        command_center_count = len(
+            [unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
+
+        # Find our command centers:
+        # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
+
+        # If we haven't set our global CC yet, add it here
+        if not self.command_center:
+            self.command_center = self.dqn_model.get_units_by_type(
+                obs, units.Terran.CommandCenter)
+
+        # if self.command_center:
+        #     print("Command centers are set to:", self.command_center)
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        # BEGIN CUSTOM CODE
+        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        # print("army_supply is set to: ", army_supply)
+        enemy_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        # To do - prioritize enemy units over structures...
+        # enemy_terran_structures = [
+        #     unit for unit in obs.observation.feature_units
+        #     if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
+        # ]
+        # if len(enemy_terran_structures) > 0:
+        #     print("Enemy structures are set to: ", enemy_terran_structures)
+
+        # self_units = [
+        #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
+        # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
+        # END CUSTOM CODE
+
+        visible_units = [unit for unit in obs.observation.feature_units]
+        # print("Visible Units are set to: ", visible_units)
+        # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = {
+                "non_spatial": np.zeros(300),
+                "rgb_minimap": None
+            }
+            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
+                visible_units, self.base_top_left)
+            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+                rgb_minimap, self.base_top_left)
+
+            # current_state["non_spatial"][0] = supply_depot_count
+            # current_state["non_spatial"][1] = barracks_count
+            # current_state["non_spatial"][2] = army_supply
+            # # Custom State Add-on
+            # current_state["non_spatial"][3] = command_center_count
+            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
+            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
+            # # A check to see where the camera is looking
+            # current_state["non_spatial"][6] = self.last_camera_action
+            # # spawn location boolean seems critical for learning
+            # current_state["non_spatial"][7] = self.base_top_left
+
+            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
+
+            # print("State at the end of the step is set to: ", current_state)
+
+            # Push s/a/r/s_next our replay buffer
+            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+            if self.previous_action is not None:
+                self.actual_root_level_steps_taken += 1
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+                self.dqn_model.store_transition(self.previous_state,
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+
+            # Do in-game training of the model for every 100 root actions the agent takes:
+            # Learning disabled for testing
+            # if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     # print("Beginning in-game training for the model.")
+            #     self.in_game_training_iterations += 1
+            #     self.dqn_model.learn()
+
+            # this is where we store arbitrary actions which the agent is not allowed to take this game step
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+
+            # --------------------
+            # Action Mapping Reference
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
+            # 6: 'resetcamera'
+            # 7: 'mcselfexpansion'
+            # 8: 'mcenemyexpansion'
+            # 9: 'mcenemyprimary'
+            # 10: 'attack_4_4'
+            # 11: 'attack_12_4'
+            # 12: 'attack_4_12'
+            # 13: 'attack_12_12'
+            # 14: 'attack_36_4'
+            # 15: 'attack_44_4'
+            # 16: 'attack_36_12'
+            # 17: 'attack_44_12'
+            # 18: 'attack_4_36'
+            # 19: 'attack_12_36'
+            # 20: 'attack_4_44'
+            # 21: 'attack_12_44'
+            # 22: 'attack_36_36'
+            # 23: 'attack_44_36'
+            # 24: 'attack_36_44'
+            # 25: 'attack_44_44'
+            # --------------------
+
+            # Modified, self-generated code to scale supply depot creation
+            # Includes sleep timer so bot doesn't build them in a loop
+            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+            # We also get the agent to build at least one barracks before building >1 supply depot
+
+            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
+                excluded_actions.append(1)
+
+            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            # Also check to see that we have enough minerals
+            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150:
+                excluded_actions.append(2)
+
+            # Exclude marinies from the build queue
+            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                excluded_actions.append(3)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (army_supply < 16):
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            # SCV Checks
+            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
+                excluded_actions.append(5)
+
+            # Camera reset handling
+            # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
+            # print("self.last_camera_action is set to: ", self.last_camera_action)
+            if self.last_camera_action == 6 or self.camera_move_timer < 5:
+                excluded_actions.append(6)
+
+            # Camera move handling
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
+                # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+
+            # modified original logic, waits for 8 marines before attacking
+            # Excludes all minimap attack actions if we just issued one (at least...for now)
+            # Post-bootstrap, it may be possible to relax these checks
+            # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
+                # Add actions from 10-25 to excluded_actions if they are not present
+                excluded_actions.extend(x for x in range(
+                    10, 26) if x not in excluded_actions)
+                
+            #  Excludes attacking home base
+            # These actions always target the home base after x/y transformation
+            # ---- Required for bootstrapping, but not after training due to reward structure ----
+            #
+            # for action in self.top_left_actions:
+            #     if action not in excluded_actions:
+            #         excluded_actions.append(action)
+
+            # Prevent the bot from doing nothing in perpetuity
+            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
+            # Safety check to make sure if no other options are available we don't crash...
+
+            # Old Logic - was specific to '0' but now we check for everything instead
+            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
+            #     # print("Excluding DO NOTHING")
+            #     excluded_actions.append(0)
+
+            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
+                action_to_exclude = self.last_three_actions[0]
+                if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
+                    excluded_actions.append(action_to_exclude)
+
+            # Updated for DQN - let the model  select the action
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            print("Agent action choice: ", rl_action)
+            print("Excluded actions were: ", excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # Set our last camera action:
+            if rl_action in [6, 7, 8, 9]:  # One of the camera actions
+                self.last_camera_action = rl_action
+                # print("self.last_camera_action is:", self.last_camera_action)
+
+            # Weird error condition where our camera gets locked - purge entries every 100 actions
+            if self.actual_root_level_steps_taken % 100 == 0:
+                # print("zeroing out camera action")
+                self.last_camera_action = 0
+
+            # Add the action to our tiny 3-element queue
+            self.last_three_actions.append(rl_action)
+
+            # DEBUG : Print all exclusions!
+            # print("Our excluded actions for this step are: ", excluded_actions)
+            # print("The model chose: ", rl_action)
+
+            # using reference code for smart action implementation
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # print("Smart action is set to: ", smart_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    else:
+                        print(f"SCV coordinates out of range: {target}")
+
+                else:
+                    print("Failed unit.y check")
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                    else:
+                        print(f"Barracks coordinates out of range: {target}")
+
+            elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("base is top left: ", self.base_top_left)
+                safe_cc_x, safe_cc_y = self.dqn_model.get_command_center_coordinates(
+                    obs)
+                # print("New target should be: ", safe_cc_x, safe_cc_y)
+
+                if safe_cc_x:
+                    target = safe_cc_x, safe_cc_y
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            # To Do:
+            # If get_command_center_coordinates returns None, None
+            # This means we've probably lost our command center and need to build a new one
+            # New action required
+            # Alternative -> Try and repair it if it's taking damage
+
+            # Custom camera reset code
+            # AI can only act on units on its screen (unless using hotkeys/mappings...)
+            # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            # Coordinate flipping isn't ideal for camera movement unfortunately...
+            elif smart_action == ACTION_RESET_CAMERA:
+                # print("Camera counter is at: ",
+                #       self.camera_reset_required, "resetting camera")
+                self.camera_move_timer = 0
+
+                if self.base_top_left:
+                    # print("Spawned top left - moving camera")
+                    return actions.FUNCTIONS.move_camera((22, 18))
+                else:
+                    # print("Spawned bottom right - moving camera")
+                    return actions.FUNCTIONS.move_camera((43, 51))
+
+            # Camera move logic - need to move the camera to 'see' enemy units to attack them directly
+            # Self expansion
+            # Enemy Primary
+            # Enemy expansion
+
+            # Move camera to our expansion
+            elif smart_action == ACTION_MOVE_CAMERA_SELF_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+
+                if self.base_top_left:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+                else:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+
+            # Move camera to enemy's expansion
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+                else:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+
+            # Move camera to enemy's primary base
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_PRIMARY:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 51))
+                else:
+                    # Top left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 18))
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                # Sleep to avoid duplicate actions
+                self.supply_delay_timer = 0
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                # sleep to avoid loops
+                self.barracks_delay_timer = 0
+
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("SCV Smart Action Set")
+
+                # Zero out our build timer
+                self.scv_delay_timer = 0
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    # print("Trying to train an SCV")
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+
+            # start of boiler plate code (small modifications like delay timers)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+                    # print("Attacking: ", self.transformLocation(int(x), int(y)))
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    # return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        # print(
+                        #     "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+
+                        # Reset our timers to 0
+                        self.unit_attack_delay_timer = 0
+                        self.attack_delay_timer = 0
+
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
new file mode 100644
index 00000000..7ed3bce9
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_abel_ai.py
@@ -0,0 +1,1727 @@
+'''
+Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
+
+This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
+The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
+Out of the 1600~ Lines-of-Code, approximately 1300~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+
+To begin training against a built-in Starcraft II bot (very_easy recommended to begin), use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush
+
+To begin Self-Training (Self-Play reinforcement Learning), use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
+
+To train against Kane-AI, use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
+
+---- add to readme:
+file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
+craig		soft	nofile		8192
+craig 	hard	nofile		1048576
+Additionally, tensorboard needs to be run as root (due to higher FD limits):
+ulimit -n 1048576
+tensorboard --logdir=runs
+
+Validated with: ulimit -s -H && ulimit -n -S
+required as after 1k episodes we run out of FD's:
+FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+'''
+
+
+# Import our other agent
+from Kane_AI import KaneAI
+from test_abel_ai import DQNAgent as test_DQN_Agent
+
+import random
+from random import sample as random_sample
+import time
+import math
+import argparse
+
+from absl import app
+import os
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+# queue used for replay buffer
+from collections import deque
+
+# Using TensorBoard for model performance tracking & visualizations
+from torch.utils.tensorboard import SummaryWriter
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...quite poor excellent.
+from torch.optim.lr_scheduler import CosineAnnealingLR
+from collections import Counter
+# Using itertools to slice deque's efficiently
+import itertools
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+from pysc2.lib import units
+from pysc2.env import sc2_env
+from pysc2.env import run_loop
+# Environment Setup
+# There is a predictable crash after every 986th game currently, this code is a workaround
+_MAX_GAMES_BEFORE_RESTART = 1
+
+# Command line parsing
+def parse_arguments():
+    parser = argparse.ArgumentParser(description="SC2 Agent Trainer")
+    parser.add_argument("training_type", choices=["self", "kane", "bot"], help="Choose training scenario: self, kane or bot")
+    parser.add_argument("--difficulty", choices=["very_easy", "easy", "medium", "hard"], default="very_easy", help="Bot difficulty level")
+    return parser.parse_args()
+
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+####### CUSTOM CODE ######
+#
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-18.pt_episode_600_reward_-0.56.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-19-a.pt'
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
+ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
+ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
+ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
+###### END OF GLOBAL CUSTOM CODE #####
+
+# Steven Brown also used smart_actions although for this bot I've expanded them considerably
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
+    ACTION_MOVE_CAMERA_SELF_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
+]
+
+# DQN Non-Spatial State size
+STATE_SIZE = 2
+
+################################## End of BoilerPlate Code #####################################################
+
+# Custom library of all terran buildings
+TERRAN_BUILDINGS = [
+    18,  # Command Center
+    19,  # Supply Depot
+    20,  # Refinery
+    21,  # Barracks
+    22,  # Orbital Command
+    23,  # Factory
+    24,  # Starport
+    25,  # Engineering Bay
+    26,  # Fusion Core
+    27,  # Tech Lab (Barracks)
+    28,  # Tech Lab (Factory)
+    29,  # Tech Lab (Starport)
+    30,  # Reactor (generic, as the building morphs)
+    37,  # Sensor Tower
+    38,  # Bunker
+    39,  # Missile Turret
+    40,  # Auto-turret (from Raven)
+    58,  # Planetary Fortress
+]
+
+TERRAN_UNITS = {
+    # Worker
+    45: "SCV",
+
+    # Basic
+    48: "MULE",
+    51: "Marine",
+    53: "Marauder",
+    54: "Reaper",
+    55: "Ghost",
+
+    # Factory
+    57: "Hellion",
+    58: "SiegeTank",
+    59: "Cyclone",
+    62: "Thor",
+    64: "Hellbat",
+
+    # Starport
+    35: "VikingFighter",
+    36: "VikingAssault",
+    67: "Medivac",
+    68: "Banshee",
+    69: "Raven",
+    70: "Battlecruiser",
+    132: "Liberator",
+    71: "AutoTurret",  # Spawned by Raven
+
+    # Other
+    102: "WidowMine",
+    488: "LiberatorAG"
+}
+
+# Define the top-left coordinates for each quadrant
+quadrants = [
+    (0, 0),       # Top left quadrant
+    (32, 0),      # Top right quadrant
+    (0, 32),      # Bottom left quadrant
+    (32, 32)      # Bottom right quadrant
+]
+
+# Calculate the offset points for each quadrant
+# This is used in the smart_attack functions (16 locations with offsets)
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
+
+
+def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
+    corner_offset = 3
+    mini_quadrant_size = 16  # Each quadrant is divided further into 4
+
+    if quadrant == "top-left":
+        points = [
+            # Top-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             corner_offset),  # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + \
+             corner_offset),  # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "top-right":
+        points = [
+            # Top-right of top-left mini quadrant
+            (top_left_x + mini_quadrant_size - \
+             corner_offset, top_left_y + corner_offset),
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset),  # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-right of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-left":
+        points = [
+            # Bottom-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - \
+             corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y +
+             mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset),  # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-right of bottom-right mini quadrant
+        ]
+
+    return points
+
+
+# For each quadrant, calculate the offset points and append the attack action
+quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
+for i, quad in enumerate(quadrants):
+    points = calculate_quadrant_points(*quad, quadrant_names[i])
+    for x, y in points:
+        smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
+
+
+# print("smart_actions is set to: ", smart_actions)
+print("--------------------")
+
+print("# Action Mapping")
+for index, action in enumerate(smart_actions):
+    print(f"# {index}: '{action}'")
+print("--------------------")
+
+# --------------------
+# Action Mapping
+# 0: 'donothing'
+# 1: 'buildsupplydepot'
+# 2: 'buildbarracks'
+# 3: 'buildmarine'
+# 4: 'attackunit'
+# 5: 'buildscv'
+# 6: 'resetcamera'
+# 7: 'mcselfexpansion'
+# 8: 'mcenemyexpansion'
+# 9: 'mcenemyprimary'
+# 10: 'attack_3_3'
+# 11: 'attack_19_3'
+# 12: 'attack_3_19'
+# 13: 'attack_19_19'
+# 14: 'attack_45_3'
+# 15: 'attack_61_3'
+# 16: 'attack_45_19'
+# 17: 'attack_61_19'
+# 18: 'attack_3_45'
+# 19: 'attack_19_45'
+# 20: 'attack_3_61'
+# 21: 'attack_19_61'
+# 22: 'attack_45_45'
+# 23: 'attack_61_45'
+# 24: 'attack_45_61'
+# 25: 'attack_61_61'
+# --------------------
+
+
+# Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 2048 (confirmed optimal via hyperparameter search)
+# Technically I believe this architecture could be called a "Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
+# To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
+# 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
+# Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
+# The drawback of course is that space complexity is 2N...
+class DQNModel(nn.Module):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=2048):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = gamma
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.001
+        # We decay over 3M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (
+            self.epsilon - self.final_epsilon) / 300000
+        self.action_counter = 0
+        #
+        self.state_size = non_spatial_state_size
+        self.disallowed_actions = {}
+        # Replay buffer deque and list
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque(maxlen=buffer_capacity)
+        self.training_buffer = []
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+        # Counter for tracking how frequently training is run
+        self.global_training_steps = 0
+        # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
+        self.writer_path = 'runs/dqn-cnn-agent-v18-kane-a'
+        # Our replay buffer training theshold size
+        self.training_buffer_requirement = 250000
+        # This is our custom reward/action mapping dictionary
+        # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        self.action_rewards = {
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as the AI sees it as a 'safe' move in perpetuity
+            1: 1,     # 'buildsupplydepot'
+            2: 0.95,  # 'buildbarracks'
+            3: 0.8,   # 'buildmarine'
+            4: 0.8,   # 'attackunit' - High reward, attack visible units
+            5: 0.8,   # 'buildscv'
+            6: 0.8,   # 'resetcamera' - High reward as is required when available
+            7: 0.05,  # 'mcselfexpansion' - moves camera - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - moves camera - checks enemy expansion
+            9: 0.15,  # 'mcenemyprimary' - moves camera - checks enemy primary base
+            10: 0,    # 'attack_3_3'
+            11: 0,    # 'attack_19_3'
+            12: 0,    # 'attack_3_19'
+            13: 0,    # 'attack_19_19'
+            14: 0,    # 'attack_45_3'
+            15: 0,    # 'attack_61_3'
+            16: 0,    # 'attack_45_19'
+            17: 0,    # 'attack_61_19'
+            18: 0,    # 'attack_3_45'
+            19: 0,    # 'attack_19_45'
+            20: 0,    # 'attack_3_61'
+            21: 0,    # 'attack_19_61'
+            22: 0,    # 'attack_45_45'
+            23: 0,    # 'attack_61_45'
+            24: 0,    # 'attack_45_61'
+            25: 0     # 'attack_61_61'
+        }
+
+        # Setting up quadrant mapping to avoid repetitive copies in the hot path
+        # Define which actions correspond to which quadrants
+        # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
+        # With the function transformLocation()
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        # CNN for RGB minimap with normalisation
+        self.conv = nn.Sequential(
+            nn.Conv2d(in_channels=3, out_channels=32,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(32),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=32, out_channels=64,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(64),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=64, out_channels=128,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(128),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+        ).to(self.device)  # Moving to GPU if available
+
+        # Calculate the size of our flattened output after convolutional layers
+        self.conv_out_size = self._get_conv_out(minimap_shape)
+
+        # Fully Connected Network (FCN) for non-spatial data with normalization
+        self.fc_non_spatial = nn.Sequential(
+            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
+            nn.ReLU(),
+            nn.Linear(128, 256),
+            nn.BatchNorm1d(300),
+            nn.ReLU()
+        ).to(self.device)  # Moving to GPU if available
+
+        # Decision-making layers (takes concatenated, processed outputs from CNN and FCN and gives us a decision)
+        self.fc_decision = nn.Sequential(
+            nn.Linear(84992, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, 4096),
+            nn.ReLU(),
+
+            nn.Linear(4096, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, len(self.actions))
+        ).to(self.device)  # Moving to GPU if available
+
+        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+        # Our learning rate scheduler is enabled here (CosineAnnealing)
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10000)
+
+    #
+    def _get_conv_out(self, shape):
+        o = self.conv(torch.zeros(1, *shape).to(self.device))
+        return int(np.prod(o.size()))
+
+    # This is an implicit PyTorch function that's called automatically
+    def forward(self, non_spatial_data, minimap):
+        non_spatial_data = non_spatial_data.to(self.device)
+        minimap = minimap.to(self.device)
+
+        # Separate treatments for different types of data
+        conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
+        fc_out = self.fc_non_spatial(non_spatial_data).reshape(
+            non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
+
+        # Combine both outputs
+        combined = torch.cat((conv_out, fc_out), dim=1)
+
+        return self.fc_decision(combined)
+
+    # Using a method to handle writes to ensure things are closed properly in the event of a crash
+    def get_writer(self):
+        return SummaryWriter(self.writer_path)
+
+    # This is where we store items for our replay buffer
+    # s: The current state of the environment.
+    # a: The action taken by the agent in state s.
+    # r: The reward received after taking action a in state s.
+    # s_next: The resulting state after taking
+    def store_transition(self, s, a, r, s_next):
+        # Transition is a tuple (s, a, r, s_next)
+        transition = (s, a, r, s_next)
+        # Append the transition to the replay buffer
+        self.buffer.append(transition)
+
+    def transfer_buffer(self):
+        self.training_buffer = list(self.buffer)
+
+        #####
+        # Debugging
+        # Extract the components of the last element in the buffer
+        # s, a, r, s_next = self.training_buffer[-1]
+
+        # # Print the components
+        # print("s:", s)
+        # print("a:", a)
+        # print("r:", r)
+        # print("s_next:", s_next)
+
+        # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
+        #####
+
+    # This function goes back and tweaks the rewards associated with a given game
+    # Based on the final tangible reward we get from PySC2
+    # Win/loss/draw are multipliers
+    # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
+    # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
+    def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
+        # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
+        steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
+
+        # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
+        for i in range(-steps_to_iterate, 0):
+            # Skip if the buffer index result is None
+            if self.buffer[i] is None:
+                continue
+            # Fetch the transition
+            s, a, r, s_next = self.buffer[i]
+
+            # Modify the reward to include the final reward
+            new_reward = r * final_reward
+            # print("Before backpropagation: ", r)
+
+            # Replace the transition in the replay buffer
+            self.buffer[i] = (s, a, new_reward, s_next)
+            # print("After backpropagation: ", self.buffer[i][2])
+
+        print("Replay buffer currently has: ",
+              len(self.training_buffer), "entries")
+
+    # # sample transitions from replay buffer queue for the last game
+    # def sample(self, batch_size):
+    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+    #     return list(self.buffer)[-batch_size:]
+
+    # Sample random transitions from replay buffer
+    def random_sample(self, batch_size):
+        return random_sample(self.training_buffer, batch_size)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Implementation that Steven Brown (PySC2 Dev) created here:
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
+    def choose_action(self, current_state, excluded_actions=[]):
+        # Extract non_spatial_data and rgb_minimap from current_state
+        non_spatial_data = current_state["non_spatial"]
+        rgb_minimap = current_state["rgb_minimap"]
+
+        # Debugs
+        # print("Non-spatial data shape before unsqueeze:", non_spatial_data.shape)
+
+        # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
+        self.eval()
+        # Convert data to tensors and move them to the GPU
+        non_spatial_data_tensor = torch.tensor(
+            non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
+        # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
+        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
+            2, 0, 1).unsqueeze(0).to(self.device)
+
+        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
+
+        # Epsilon-based exploration
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+        else:
+            # Pass the minimap data through the CNN
+            conv_output = self.conv(rgb_minimap_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            conv_output = conv_output.view(conv_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
+
+            # print("Conv shape is: ", conv_output.shape)
+            # print("Non spatial shape is: ", non_spatial_output.shape)
+
+            # Concatenate the two outputs along the second dimension (feature axis)
+            combined_output = torch.cat(
+                (conv_output, non_spatial_output), dim=1)
+
+            # Squeeze out the dummy third dimension
+            combined_output = combined_output.squeeze(2)
+
+            # Finally, pass through the decision-making layers
+            q_values = self.fc_decision(combined_output)
+
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+        # This is where we keep the logic for epsilon decay (linear)
+        self.action_counter += 1
+        self.epsilon -= self.epsilon_decay_rate
+        self.epsilon = max(self.final_epsilon, self.epsilon)
+
+        # print("The action we chose was: ", action)
+        # print("The excluded actions were: ", excluded_actions)
+
+        # Turn training mode back on
+        self.train()
+        return action
+
+    # This is where we train the model
+    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
+    def learn(self, target_network):
+        # Check if the replay buffer has enough samples (using 500K as minimum)
+        if len(self.training_buffer) < self.training_buffer_requirement:
+            print("Replay buffer is currently too small to conduct training...")
+            return
+
+        # Increment our training counter
+        self.global_training_steps += 1
+
+        # Sample a mini-batch of transitions from the buffer
+        transitions = self.random_sample(self.batch_size)
+        # Unzip the transitions into separate variables
+        states, actions, rewards, next_states = zip(*transitions)
+
+        # Separate non_spatial and rgb_minimap components of states
+        non_spatial_states = [state["non_spatial"] for state in states]
+        rgb_minimap_states = [state["rgb_minimap"] for state in states]
+
+        non_spatial_next_states = [state["non_spatial"]
+                                   for state in next_states]
+        rgb_minimap_next_states = [state["rgb_minimap"]
+                                   for state in next_states]
+
+        # Convert the zipped values to numpy arrays
+        non_spatial_states_np = np.array(non_spatial_states, dtype=np.float32)
+        rgb_minimap_states_np = np.array(rgb_minimap_states, dtype=np.float32)
+        non_spatial_next_states_np = np.array(
+            non_spatial_next_states, dtype=np.float32)
+        rgb_minimap_next_states_np = np.array(
+            rgb_minimap_next_states, dtype=np.float32)
+        actions_np = np.array(actions, dtype=np.int64)
+        rewards_np = np.array(rewards, dtype=np.float32)
+
+        # Convert numpy arrays to tensors
+        non_spatial_states = torch.tensor(
+            non_spatial_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(
+            self.device).permute(0, 3, 1, 2)
+        non_spatial_next_states = torch.tensor(
+            non_spatial_next_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_next_states = torch.tensor(
+            rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2)
+
+        actions = torch.tensor(actions_np).to(self.device)
+        rewards = torch.tensor(rewards_np).to(self.device)
+
+        # If this is the first training step, save the model's graph to TensorBoard for visualization purposes
+        if self.global_training_steps == 1:
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_graph(
+                        self, (non_spatial_states, rgb_minimap_states))
+            except Exception as e:
+                print(f"Error logging model graph: {e}")
+
+        # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
+        with autocast():
+            # Compute the Q-values for the current states
+            q_values = self(non_spatial_states, rgb_minimap_states)
+            q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+
+            # Compute the target Q-values
+            # next_q_values = self(non_spatial_next_states,
+            #                      rgb_minimap_next_states)
+            next_q_values = target_network(
+                non_spatial_next_states, rgb_minimap_next_states)
+            max_next_q_values = next_q_values.max(1)[0]
+            q_target = rewards + self.gamma * max_next_q_values
+
+            # Compute the loss and update the model's weights
+            loss = self.loss_fn(q_predict, q_target)
+
+            # Debugs if required...
+            #     print("Q-target contains NaN or Infinity!")
+            # if torch.isnan(loss).any() or torch.isinf(loss).any():
+            #     print("Loss contains NaN or Infinity!")
+            # if torch.isnan(q_values).any() or torch.isinf(q_values).any():
+            #     print("Q-values contain NaN or Infinity!")
+            # if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
+            #     print("Non-spatial states contain NaN or Infinity!")
+            # if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
+            #     print("RGB Minimap states contain NaN or Infinity!")
+            # if torch.isnan(rewards).any() or torch.isinf(rewards).any():
+            #     print("Rewards contain NaN or Infinity!")
+            # if math.isnan(self.gamma) or math.isinf(self.gamma):
+            #     print("Gamma contains NaN or Infinity!")
+            # if torch.isnan(actions).any() or torch.isinf(actions).any():
+            #     print("Actions contain NaN or Infinity!")
+
+        # Clear accumulated gradients before back propagation
+        self.optimizer.zero_grad()
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Clip the gradient to avoid huge updates
+        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        # Update the learning rate based on CosineAnnealing scheduling
+        self.scheduler.step()
+
+        # These logs are generated less frequently (every 25 training runs)
+        # Try/Except blocks as they've routinely crashed the simulation :(
+        if self.global_training_steps % 25 == 0:
+            # Logging various metrics for visualization and debugging
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Loss/train', loss.item(),
+                                      self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Loss/train: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Epsilon/value', self.epsilon, self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Epsilon/value: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_histogram(
+                        'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Q-Values: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Learning Rate: {e}")
+
+            # Log the histograms of model weights
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    for name, param in self.named_parameters():
+                        writer.add_histogram(name, param.clone().cpu(
+                        ).data.numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging model weights: {e}")
+
+            # Create an action frequency histogram of the last 1000 actions in the replay buffer
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    last_1000_actions = list(itertools.islice(self.training_buffer, len(
+                        self.training_buffer) - 1000, len(self.training_buffer)))
+                    actions = [transition[1]
+                               for transition in last_1000_actions]
+                    action_frequencies = Counter(actions)
+
+                    # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...)
+                    for i in range(len(smart_actions)):
+                        action_frequencies[i] = action_frequencies.get(i, 0)
+
+                    action_list_for_histogram = [
+                        action for action, freq in action_frequencies.items() for _ in range(freq)]
+
+                    writer.add_histogram(
+                        'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Actions/Frequency: {e}")
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint weights
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.load_state_dict(torch.load(file_path))
+
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything about 12k score results in a full score being provided to the model
+        max_score = 12000
+        # Normalize the score to be between 0 and 1
+        normalized_reward = min(score / max_score, 1)
+
+        # --------------------
+        # Action Mapping Reference
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 1
+        attack_adjacent_quadrant = 0.5
+        penalty_home_quadrant = -1
+        penalty_home_expansion = -0.5
+
+        # Incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if previous_action in self.bottom_right_actions:
+            normalized_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            normalized_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            normalized_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            normalized_reward += penalty_home_expansion
+
+        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
+        action_reward = self.action_rewards.get(previous_action, 0)
+        # Add the action reward, ensuring the total reward stays between 0 and 1
+        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+
+        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
+
+        return normalized_reward
+
+    # Provide the x/y coordinates of our command center(s)
+    def get_command_center_coordinates(self, obs):
+        # Get Command Centers using the get_units_by_type function
+        command_centers = self.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+
+        # If there's a Command Center, return its coordinates
+        if command_centers:
+            # Grabbing the first for now (no expansion support for cameras)
+            command_center = command_centers[0]
+
+            # Checks to avoid out-of-bounds crashing
+            # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+            if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
+                return command_center.x, command_center.y
+            else:
+                print(
+                    f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                return None, None
+        else:
+            print("Command Center not found.")
+            return None, None
+
+    # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
+    def translate_coordinates(self, x, y, original_size=84, target_size=64):
+        scale_factor = target_size / original_size
+        return int(x * scale_factor), int(y * scale_factor)
+
+    # To ensure consistency across spawn locations, we invert the minimap locations based on base_top_left
+    def transform_minimap(self, minimap_data, base_top_left):
+        if not base_top_left:
+            # Flip only the spatial dimensions, leaving the color channels unchanged
+            # Copy required due to oddities in the return values..
+            # PyTorch can't deal with negative strides AFAIK
+            transformed_minimap = np.flip(minimap_data, axis=(0, 1)).copy()
+        else:
+            # Leave the minimap as it is
+            transformed_minimap = minimap_data
+
+        return transformed_minimap
+
+    # This creates a fixed-length vector to store visible units in
+    def transform_units_to_fixed_length(self, units, base_top_left):
+
+        # Safety checks
+        # Check if units are None or empty
+        if units is None or len(units) == 0:
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Check for NaN values
+        if np.isnan(np.array(units)).any():
+            print("Warning: NaN values detected in units!")
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Constant indices based on the field names
+        UNIT_TYPE_INDEX = 0
+        ALLIANCE_INDEX = 1
+        HEALTH_INDEX = 2
+        X_POS_INDEX = 12
+        Y_POS_INDEX = 13
+
+        # Number of units and features
+        num_units = len(units)
+        num_features = 5  # ['unit_type', 'health', 'x', 'y']
+
+        # Create an empty array of shape (300, num_features)
+        fixed_length_units = np.zeros((300, num_features))
+
+        # Fill the array with the actual values
+        for i in range(min(num_units, 300)):
+            unit = units[i]
+            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+
+            # Transform x, y coordinates as required for normalization
+            if not base_top_left:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+            else:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+
+        return fixed_length_units
+
+
+# Agent Implementation
+
+
+class DQNAgent(base_agent.BaseAgent):
+
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Available actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        # Our primary DQN
+        self.dqn_model = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+        # Our target network used for Q-Value Calculations
+        self.target_network = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+
+        self.previous_action = None
+        self.previous_state = {
+            "non_spatial": None,
+            "rgb_minimap": None
+        }
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Minerals
+        self.minerals = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.training_time = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+        self.actual_root_level_steps_taken = 0
+        self.in_game_training_iterations = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.unit_attack_delay_timer = 0
+        self.scv_delay_timer = 0
+        self.camera_move_timer = 0
+        self.last_camera_action = None
+        # Queue that tracks last actions for exclusionary purposes
+        self.last_three_actions = deque(maxlen=3)
+
+        self.command_center = []
+
+        # Action mapping
+        # Define which actions correspond to which quadrants
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        if os.path.isfile(_INITIAL_WEIGHTS):
+            print("Loading previous model: ", _INITIAL_WEIGHTS)
+            self.dqn_model.load_model(_INITIAL_WEIGHTS)
+            self.target_network.load_model(_INITIAL_WEIGHTS)
+
+        # Offload to GPU
+        self.dqn_model = self.target_network.to(self.dqn_model.device)
+        self.target_network = self.target_network.to(self.dqn_model.device)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.unit_attack_delay_timer += 1
+        self.scv_delay_timer += 1
+        self.camera_move_timer += 1
+
+        # Visuals
+        rgb_minimap = obs.observation["rgb_minimap"]
+
+        # Check our current score, just for debugging
+        # print("Current score is: ", self.get_normalized_reward(obs))
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                # print("Extra steps are: ", extra_steps)
+                # print("total sets were: ", episode_steps)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+                # Ensure combined_reward never goes below 1.1 for a win
+                combined_reward = max(combined_reward, 1.2)
+                final_reward_multiplier = combined_reward
+            elif base_reward == 0:  # 0 indicates a draw
+                combined_reward = base_reward
+                final_reward_multiplier = 0.7  # Slight decrease in score for a draw
+            else:  # -1 indicates a loss
+                combined_reward = base_reward
+                final_reward_multiplier = 0.25
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the rolling average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            print("------------------------------------------------------")
+            # print("Combined reward is set to: ", combined_reward)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            # Checkpoint the model every 100 games once we've started training
+            if self.episode_count % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                print("Saving our model weights (Checkpoint)...")
+                self.dqn_model.save_model(
+                    _DATA_FILE, self.episode_count, avg_reward)
+
+            print("Previous average reward was: ",
+                  self.previous_avg_reward)
+            print("Our rolling-average reward is: ", avg_reward)
+            print("Latest game reward was: ", combined_reward)
+            print("Number of steps were: ", episode_steps)
+            # Backpropagate the final reward multiplier to previous actions
+            print(
+                f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
+            self.dqn_model.backpropagate_final_reward(
+                final_reward_multiplier, self.actual_root_level_steps_taken)
+
+            # Copy over our deque replay buffer into our list if it's been 10 games
+            if self.episode_count % 10 == 0:
+                self.dqn_model.transfer_buffer()
+
+            # Print statements if our buffer is large enough to train on...
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                print("Number of in-game model updates: ",
+                      self.in_game_training_iterations)
+                print("Training the model after game completion...")
+            # Learn after every game, not just the successful ones:
+
+            # Log our reward over time and training duration if we've started training
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                training_start_time = time.time()
+                self.dqn_model.learn(self.target_network)
+                training_end_time = time.time() - training_start_time
+                self.training_time.append(training_end_time)
+                avg_training_time = sum(
+                    self.training_time) / len(self.training_time)
+                # Update our target network every 5 games once training has begun
+                if self.episode_count % 5 == 0:
+                    self.target_network.load_state_dict(
+                        self.dqn_model.state_dict())
+                print(
+                    f"This training loop took {training_end_time:.4f} seconds.")
+                print("Training complete")
+                # Log our average reward to TensorBoard
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar(
+                        'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+                # Log training time
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar('Average Training Time in Seconds/train_duration',
+                                      avg_training_time, self.dqn_model.global_training_steps)
+
+            # Reset remaining, episode-specific counters
+            print("------------------------------------------------------")
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+            self.actual_root_level_steps_taken = 0
+            self.in_game_training_iterations = 0
+            # Clear our action queue
+            self.last_three_actions.clear()
+            self.last_camera_action = None
+            for _ in range(3):
+                self.last_three_actions.append(None)
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+
+            # Original logic, doesn't work properly...
+            # player_y, player_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            # # print("Player x and y are set to: ", self.player_x, self.player_y)
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+            # Using similar approach to Kane AI to Figure out where our home base is
+            # Original reference is here of course:
+            # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+            if obs.first():
+
+                player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                      features.PlayerRelative.SELF).nonzero()
+                xmean = player_x.mean()
+                ymean = player_y.mean()
+
+                if xmean <= 31 and ymean <= 31:
+                    self.base_top_left = True
+                else:
+                    self.base_top_left = False
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        all_supply_depots = self.dqn_model.get_units_by_type(
+            obs, units.Terran.SupplyDepot)
+        supply_depot_count = len(
+            [unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        all_barracks = self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks)
+        barracks_count = len(
+            [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+
+        all_command_centers = self.dqn_model.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+        command_center_count = len(
+            [unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
+
+        # Find our command centers:
+        # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
+
+        # If we haven't set our global CC yet, add it here
+        if not self.command_center:
+            self.command_center = self.dqn_model.get_units_by_type(
+                obs, units.Terran.CommandCenter)
+
+        # if self.command_center:
+        #     print("Command centers are set to:", self.command_center)
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        enemy_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        visible_units = [unit for unit in obs.observation.feature_units]
+        # print("Visible Units are set to: ", visible_units)
+        # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
+
+        supply_free = supply_limit - supply_used
+
+        # Minerals
+        self.minerals = obs.observation['player'][1]
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = {
+                "non_spatial": np.zeros(300),
+                "rgb_minimap": None
+            }
+            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
+                visible_units, self.base_top_left)
+            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+                rgb_minimap, self.base_top_left)
+
+            # current_state["non_spatial"][0] = supply_depot_count
+            # current_state["non_spatial"][1] = barracks_count
+            # current_state["non_spatial"][2] = army_supply
+            # # Custom State Add-on
+            # current_state["non_spatial"][3] = command_center_count
+            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
+            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
+            # # A check to see where the camera is looking
+            # current_state["non_spatial"][6] = self.last_camera_action
+            # # spawn location boolean seems critical for learning
+            # current_state["non_spatial"][7] = self.base_top_left
+
+            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
+
+            # print("State at the end of the step is set to: ", current_state)
+
+            # Push s/a/r/s_next our replay buffer
+            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+            if self.previous_action is not None:
+                self.actual_root_level_steps_taken += 1
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+                self.dqn_model.store_transition(self.previous_state,
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+
+            # Do in-game training of the model for every 100 root actions the agent takes
+            if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                # print("Beginning in-game training for the model.")
+                self.in_game_training_iterations += 1
+                self.dqn_model.learn(self.target_network)
+
+            # this is where we store arbitrary actions which the agent is not allowed to take this game step
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+
+            # --------------------
+            # Action Mapping Reference
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
+            # 6: 'resetcamera'
+            # 7: 'mcselfexpansion'
+            # 8: 'mcenemyexpansion'
+            # 9: 'mcenemyprimary'
+            # 10: 'attack_4_4'
+            # 11: 'attack_12_4'
+            # 12: 'attack_4_12'
+            # 13: 'attack_12_12'
+            # 14: 'attack_36_4'
+            # 15: 'attack_44_4'
+            # 16: 'attack_36_12'
+            # 17: 'attack_44_12'
+            # 18: 'attack_4_36'
+            # 19: 'attack_12_36'
+            # 20: 'attack_4_44'
+            # 21: 'attack_12_44'
+            # 22: 'attack_36_36'
+            # 23: 'attack_44_36'
+            # 24: 'attack_36_44'
+            # 25: 'attack_44_44'
+            # --------------------
+
+            # Modified, self-generated code to scale supply depot creation
+            # Includes sleep timer so bot doesn't build them in a loop
+            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+            # We also get the agent to build at least three barracks before building >1 supply depot
+
+            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
+                excluded_actions.append(1)
+
+            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            # Also check to see that we have enough minerals
+            if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150:
+                excluded_actions.append(2)
+
+            # Exclude marinies from the build queue
+            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                excluded_actions.append(3)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (army_supply < 16):
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            # SCV Checks
+            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
+                excluded_actions.append(5)
+
+            # Camera reset handling
+            # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
+            # print("self.last_camera_action is set to: ", self.last_camera_action)
+            if self.last_camera_action == 6 or self.camera_move_timer < 5:
+                excluded_actions.append(6)
+
+            # Camera move handling
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
+                # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+
+            # modified original logic, waits for 16 marines before attacking
+            # Excludes all minimap attack actions if we just issued one (at least...for now)
+            # Post-bootstrap, it may be possible to relax these checks
+            # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
+                # Add actions from 10-25 to excluded_actions if they are not present
+                excluded_actions.extend(x for x in range(
+                    10, 26) if x not in excluded_actions)
+
+            #  Excludes attacking home base
+            # These actions always target the home base after x/y transformation
+            # ---- Required for bootstrapping, but not after training due to reward structure ----
+            #
+            # for action in self.top_left_actions:
+            #     if action not in excluded_actions:
+            #         excluded_actions.append(action)
+
+            # Prevent the bot from doing nothing in perpetuity
+            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
+            # Safety check to make sure if no other options are available we don't crash...
+
+            # Old Logic - was specific to '0' but now we check for everything instead
+            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
+            #     # print("Excluding DO NOTHING")
+            #     excluded_actions.append(0)
+
+            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
+                action_to_exclude = self.last_three_actions[0]
+                if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
+                    excluded_actions.append(action_to_exclude)
+
+            # Updated for DQN - let the model  select the action
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # Set our last camera action:
+            if rl_action in [6, 7, 8, 9]:  # One of the camera actions
+                self.last_camera_action = rl_action
+
+            # Weird error condition where our camera gets locked - purge entries every 100 actions
+            if self.actual_root_level_steps_taken % 100 == 0:
+                # print("zeroing out camera action")
+                self.last_camera_action = 0
+
+            # Add the action to our tiny 3-element queue
+            self.last_three_actions.append(rl_action)
+
+            # DEBUG : Print all exclusions!
+            # print("Our excluded actions for this step are: ", excluded_actions)
+            # print("The model chose: ", rl_action)
+
+            # using reference code for smart action implementation
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    else:
+                        print(f"SCV coordinates out of range: {target}")
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                    else:
+                        print(f"Barracks coordinates out of range: {target}")
+
+            elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("base is top left: ", self.base_top_left)
+                safe_cc_x, safe_cc_y = self.dqn_model.get_command_center_coordinates(
+                    obs)
+                # print("New target should be: ", safe_cc_x, safe_cc_y)
+
+                if safe_cc_x:
+                    target = safe_cc_x, safe_cc_y
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            # To Do:
+            # If get_command_center_coordinates returns None, None
+            # This means we've probably lost our command center and need to build a new one
+            # New action required
+            # Alternative -> Try and repair it if it's taking damage
+
+            # Custom camera reset code
+            # AI can only act on units on its screen (unless using hotkeys/mappings...)
+            # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            # Coordinate flipping isn't ideal for camera movement unfortunately...
+            elif smart_action == ACTION_RESET_CAMERA:
+                # print("Camera counter is at: ",
+                #       self.camera_reset_required, "resetting camera")
+                self.camera_move_timer = 0
+
+                if self.base_top_left:
+                    # print("Spawned top left - moving camera")
+                    return actions.FUNCTIONS.move_camera((22, 18))
+                else:
+                    # print("Spawned bottom right - moving camera")
+                    return actions.FUNCTIONS.move_camera((43, 51))
+
+            # Camera move logic - need to move the camera to 'see' enemy units to attack them directly
+            # Self expansion
+            # Enemy Primary
+            # Enemy expansion
+
+            # Move camera to our expansion
+            elif smart_action == ACTION_MOVE_CAMERA_SELF_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+
+                if self.base_top_left:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+                else:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+
+            # Move camera to enemy's expansion
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+                else:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+
+            # Move camera to enemy's primary base
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_PRIMARY:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 51))
+                else:
+                    # Top left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 18))
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                # sleep to avoid loops
+
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("SCV Smart Action Set")
+                # Zero out our build timer
+                self.scv_delay_timer = 0
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    # print("Trying to train an SCV")
+
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+
+            # start of boiler plate code (small modifications like delay timers)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+                    # print("Attacking: ", self.transformLocation(int(x), int(y)))
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    # return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        # print(
+                        #     "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+
+                        # Reset our timers to 0
+                        self.unit_attack_delay_timer = 0
+                        self.attack_delay_timer = 0
+                        self.last_camera_action = 0
+
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code
+
+
+def main(args):
+    agent1 = DQNAgent()
+    # Depending on the training type, create agent2
+    if args.training_type == "self":
+        agent2 = test_DQN_Agent()
+    elif args.training_type == "kane":
+        agent2 = KaneAI()
+    else:  # "bot"
+        agent2 = sc2_env.Bot(sc2_env.Race.terran, sc2_env.Difficulty[args.difficulty.upper()])
+
+    USE_FEATURE_UNITS = True
+    RGB_SCREEN_SIZE = 84
+    RGB_MINIMAP_SIZE = 64
+
+    try:
+        while True:
+            with sc2_env.SC2Env(
+                map_name="Simple64",
+                players=[
+                    sc2_env.Agent(sc2_env.Race.terran),
+                    sc2_env.Agent(sc2_env.Race.terran), 
+                ],
+                agent_interface_format=features.AgentInterfaceFormat(
+                    action_space=actions.ActionSpace.RGB,
+                    use_feature_units=USE_FEATURE_UNITS,
+                    rgb_dimensions=features.Dimensions(screen=RGB_SCREEN_SIZE, minimap=RGB_MINIMAP_SIZE),
+                    feature_dimensions=features.Dimensions(screen=84, minimap=64)
+                ),
+                step_mul=16,
+                game_steps_per_episode=21000,
+                visualize=False
+            ) as env:
+                run_loop.run_loop([agent1, agent2], env)
+                # There is a predictable crash after every 986th game currently, this code is a workaround
+                if agent1.episode_count % _MAX_GAMES_BEFORE_RESTART == 0:
+                    print(f"Games played is: {agent1.episode_count}")
+                    print("Restarting underlying SC2 Environment!")
+                    # the full SC2 environment will be recreated at the start of the next iteration of the main loop
+
+
+    except KeyboardInterrupt:
+        pass
+    except Exception as e:
+        print(f"An error occurred: {e}")
+        print("Attempting to restart the SC2 environment.")
+        # The full SC2 environment will be recreated at the start of the next iteration of the main loop.
+
+if __name__ == "__main__":
+    args = parse_arguments()
+    app.run(main(args))
\ No newline at end of file

commit 520288e9d3642d616619ee8f1194b6697ef3169b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 16 16:44:16 2023 -0400

    Double DQN Implementation

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index acd7a623..57f79494 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -1,5 +1,5 @@
 '''
-Abel AI - A Dual-Input Mixed Precision DQN with CNN and Efficient Dual-Storage Replay Buffer.
+Abel AI - A Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer.
 
 This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
 The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
@@ -93,8 +93,8 @@ ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-17.pt_episode_14200_reward_-0.17.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-18.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-18.pt_episode_600_reward_-0.56.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-19-a.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -293,15 +293,15 @@ print("--------------------")
 # --------------------
 
 
-# Custom DQN Agent implementation with a replay buffer of 1.5M, gamma of 0.95 (hopefully prioritizing longer term play), and batch_size (confirmed optimal) of 1024
-# Technically I believe this design could be called a "Dual-Input Mixed Precision DQN with CNN and Efficient Dual-Storage Replay Buffer."
+# Custom Dual DQN Agent implementation with a replay buffer of 2M, gamma of 0.90 (hopefully prioritizing longer term play), and batch_size of 2048 (confirmed optimal via hyperparameter search)
+# Technically I believe this architecture could be called a "Double DQN with Dual Input using Heuristic-Guided Mixed Precision, with a CNN for Spatial Data, and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
 # 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
 # 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.10, buffer_capacity=1500000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.90, e_greedy=0.10, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -310,9 +310,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.001
-        # We decay over 5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 3M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 5000000
+            self.epsilon - self.final_epsilon) / 300000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -328,7 +328,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v17-kane-a'
+        self.writer_path = 'runs/dqn-cnn-agent-v18-kane-a'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
@@ -599,7 +599,7 @@ class DQNModel(nn.Module):
 
     # This is where we train the model
     # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
-    def learn(self):
+    def learn(self, target_network):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.training_buffer) < self.training_buffer_requirement:
             print("Replay buffer is currently too small to conduct training...")
@@ -662,8 +662,9 @@ class DQNModel(nn.Module):
             q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
 
             # Compute the target Q-values
-            next_q_values = self(non_spatial_next_states,
-                                 rgb_minimap_next_states)
+            # next_q_values = self(non_spatial_next_states,
+            #                      rgb_minimap_next_states)
+            next_q_values = target_network(non_spatial_next_states, rgb_minimap_next_states)
             max_next_q_values = next_q_values.max(1)[0]
             q_target = rewards + self.gamma * max_next_q_values
 
@@ -947,8 +948,12 @@ class DQNAgent(base_agent.BaseAgent):
         print("Available actions are set to:", initial_actions)
         print("State Size:", STATE_SIZE)
 
+        # Our primary DQN
         self.dqn_model = DQNModel(
             actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+        # Our target network used for Q-Value Calculations
+        self.target_network = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
 
         self.previous_action = None
         self.previous_state = {
@@ -994,6 +999,11 @@ class DQNAgent(base_agent.BaseAgent):
         if os.path.isfile(_INITIAL_WEIGHTS):
             print("Loading previous model: ", _INITIAL_WEIGHTS)
             self.dqn_model.load_model(_INITIAL_WEIGHTS)
+            self.target_network.load_model(_INITIAL_WEIGHTS)
+
+        # Offload to GPU
+        self.dqn_model = self.target_network.to(self.dqn_model.device)
+        self.target_network = self.target_network.to(self.dqn_model.device)
 
     # BOILER PLATE CODE AGAIN
     def transformLocation(self, x, y):
@@ -1078,8 +1088,8 @@ class DQNAgent(base_agent.BaseAgent):
             # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            # Checkpoint the model every 200 games once we've started training
-            if self.episode_count % 200 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            # Checkpoint the model every 100 games once we've started training
+            if self.episode_count % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 print("Saving our model weights (Checkpoint)...")
                 self.dqn_model.save_model(
                     _DATA_FILE, self.episode_count, avg_reward)
@@ -1109,11 +1119,14 @@ class DQNAgent(base_agent.BaseAgent):
             # Log our reward over time and training duration if we've started training
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
-                self.dqn_model.learn()
+                self.dqn_model.learn(self.target_network)
                 training_end_time = time.time() - training_start_time
                 self.training_time.append(training_end_time)
                 avg_training_time = sum(
                     self.training_time) / len(self.training_time)
+                # Update our target network every 5 games once training has begun
+                if self.episode_count % 5 == 0:
+                    self.target_network.load_state_dict(self.dqn_model.state_dict())
                 print(
                     f"This training loop took {training_end_time:.4f} seconds.")
                 print("Training complete")
@@ -1262,11 +1275,11 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.store_transition(self.previous_state,
                                                 self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
 
-            # Do in-game training of the model for every 100 root actions the agent takes:
+            # Do in-game training of the model for every 100 root actions the agent takes
             if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 # print("Beginning in-game training for the model.")
                 self.in_game_training_iterations += 1
-                self.dqn_model.learn()
+                self.dqn_model.learn(self.target_network)            
 
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []

commit 513639ad26052641c713995aa9456f14ea7ceff3
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 16 11:05:10 2023 -0400

    removed legacy SCV attack checks

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index ab419ad4..acd7a623 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -1574,12 +1574,6 @@ class DQNAgent(base_agent.BaseAgent):
             elif smart_action == ACTION_ATTACK:
                 do_it = True
 
-                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
-                    do_it = False
-
-                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
-                    do_it = False
-
                 if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
 
                     # Debugs

commit 77c3fd771f171b743e13b14c06708d3647081c29
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 16 11:04:42 2023 -0400

    adjusted exclusions

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
index 48b632de..6dabcf6e 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -56,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-16.pt_episode_2100_reward_-0.06.pt'
+DATA_FILE = 'dqn-cnn-agent-model-17.pt_episode_14200_reward_-0.17.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -1293,16 +1293,16 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 3) or self.minerals < 100:
+            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
             # Also check to see that we have enough minerals
-            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
+            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count == 0 or self.minerals < 50:
+            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1310,7 +1310,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (army_supply < 8):
+            if (len(enemy_units) == 0) or (army_supply < 16):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1325,7 +1325,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 12 or barracks_count < 3 or self.last_camera_action != 6:
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -1339,7 +1339,7 @@ class DQNAgent(base_agent.BaseAgent):
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
-
+                
             #  Excludes attacking home base
             # These actions always target the home base after x/y transformation
             # ---- Required for bootstrapping, but not after training due to reward structure ----
@@ -1366,8 +1366,8 @@ class DQNAgent(base_agent.BaseAgent):
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
 
-            # print("Agent action choice: ", rl_action)
-            # print("Excluded actions were: ", excluded_actions)
+            print("Agent action choice: ", rl_action)
+            print("Excluded actions were: ", excluded_actions)
 
             self.previous_state = current_state
             self.previous_action = rl_action
@@ -1574,12 +1574,6 @@ class DQNAgent(base_agent.BaseAgent):
             elif smart_action == ACTION_ATTACK:
                 do_it = True
 
-                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
-                    do_it = False
-
-                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
-                    do_it = False
-
                 if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
 
                     # Debugs

commit 2066d9a8657080479fd86b4ccfe083592b6ee944
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 16 11:04:23 2023 -0400

    added model graph and adjusted exclusions

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 7bc25c42..ab419ad4 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -12,7 +12,7 @@ To begin Self-Training (Self-Play reinforcement Learning), use this string:
 python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
 
 To train against Kane-AI, use this string:
-python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 
 ---- add to readme:
 file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
@@ -93,8 +93,8 @@ ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-16.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-17.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-17.pt_episode_14200_reward_-0.17.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-18.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -301,7 +301,7 @@ print("--------------------")
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.15, buffer_capacity=1500000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.10, buffer_capacity=1500000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -328,7 +328,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-d'
+        self.writer_path = 'runs/dqn-cnn-agent-v17-kane-a'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
@@ -647,6 +647,14 @@ class DQNModel(nn.Module):
         actions = torch.tensor(actions_np).to(self.device)
         rewards = torch.tensor(rewards_np).to(self.device)
 
+        # If this is the first training step, save the model's graph to TensorBoard for visualization purposes
+        if self.global_training_steps == 1:
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_graph(self, (non_spatial_states, rgb_minimap_states))
+            except Exception as e:
+                print(f"Error logging model graph: {e}")
+
         # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
         with autocast():
             # Compute the Q-values for the current states
@@ -1299,23 +1307,23 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least three barracks before building >1 supply depot
 
-            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 3) or self.minerals < 100:
+            if supply_free > 5 or self.last_camera_action != 6 or (supply_depot_count > 1 and barracks_count < 5) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
             # Also check to see that we have enough minerals
-            if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
+            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count == 0 or self.minerals < 50:
+            if supply_free == 0 or barracks_count < 5 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
-            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16))
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
             if (len(enemy_units) == 0) or (army_supply < 16):
                 # print("excluding attack units")
                 excluded_actions.append(4)
@@ -1326,12 +1334,12 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
+            # print("self.last_camera_action is set to: ", self.last_camera_action)
             if self.last_camera_action == 6 or self.camera_move_timer < 5:
-                # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 12 or barracks_count < 3 or self.last_camera_action != 6:
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)

commit bb7a090cf48418ba3adb41594e6b28e47c038921
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 11 10:16:20 2023 -0400

    last update for week-long training

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 848045cd..7bc25c42 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -93,8 +93,8 @@ ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-15-latest.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-16.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-16.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-17.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
 ACTION_BUILD_SCV = 'buildscv'
@@ -293,15 +293,15 @@ print("--------------------")
 # --------------------
 
 
-# Custom DQN Agent implementation with a replay buffer of 1.25M, gamma of 0.95 (hopefully prioritizing longer term play), and batch_size (confirmed optimal) of 1024
+# Custom DQN Agent implementation with a replay buffer of 1.5M, gamma of 0.95 (hopefully prioritizing longer term play), and batch_size (confirmed optimal) of 1024
 # Technically I believe this design could be called a "Dual-Input Mixed Precision DQN with CNN and Efficient Dual-Storage Replay Buffer."
 # To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
-# 1. The 1.25M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 1. The 1.5M size deque where state/actions are appended/popped directly in O(1) time per game step
 # 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
 # Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.10, buffer_capacity=1250000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.15, buffer_capacity=1500000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -310,9 +310,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.001
-        # We decay over 2M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 3000000
+            self.epsilon - self.final_epsilon) / 5000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -328,7 +328,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-c'
+        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-d'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
@@ -1070,8 +1070,8 @@ class DQNAgent(base_agent.BaseAgent):
             # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            # Checkpoint the model every 100 games once we've started training
-            if self.episode_count % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            # Checkpoint the model every 200 games once we've started training
+            if self.episode_count % 200 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 print("Saving our model weights (Checkpoint)...")
                 self.dqn_model.save_model(
                     _DATA_FILE, self.episode_count, avg_reward)

commit 6209a7fc3dc7cb67c793b3af540a4da0e01c9ebb
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 11 06:54:08 2023 -0400

    tweaking barracks and comments

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 198a569f..848045cd 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -408,7 +408,7 @@ class DQNModel(nn.Module):
             nn.ReLU()
         ).to(self.device)  # Moving to GPU if available
 
-        # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
+        # Decision-making layers (takes concatenated, processed outputs from CNN and FCN and gives us a decision)
         self.fc_decision = nn.Sequential(
             nn.Linear(84992, 2048),
             nn.ReLU(),
@@ -1297,9 +1297,9 @@ class DQNAgent(base_agent.BaseAgent):
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
-            # We also get the agent to build at least one barracks before building >1 supply depot
+            # We also get the agent to build at least three barracks before building >1 supply depot
 
-            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
+            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 3) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting

commit 57a4440fefbb7c6f1d805e856968354e27de0f8c
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 11 06:53:49 2023 -0400

    tweaking barracks

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
index 2c9a03b8..48b632de 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -1293,7 +1293,7 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
+            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 3) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting

commit 69427816dced7e2b364cb66b38fadab6362b1175
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 11 06:44:22 2023 -0400

    updating excluded action logic

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
index 61af0d0f..2c9a03b8 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -56,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-14.pt'
+DATA_FILE = 'dqn-cnn-agent-model-16.pt_episode_2100_reward_-0.06.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -1293,12 +1293,12 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
+            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
             # Also check to see that we have enough minerals
-            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
+            if barracks_count > 5  or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
@@ -1309,8 +1309,8 @@ class DQNAgent(base_agent.BaseAgent):
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
-            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16))
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16):
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (army_supply < 8):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1325,7 +1325,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
+            if self.camera_move_timer < 12 or barracks_count < 3 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -1335,7 +1335,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 50:
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)

commit 689164d250aaee53b947386bf05500d6dc93ddef
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 10 14:31:53 2023 -0400

    Comments and formatting

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 3ebb741d..198a569f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -1,29 +1,31 @@
-# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
-# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
-# The RL Algorithm can properly interact with StarCraft II / PySC2
-# All of the RL algorithms were implemented by me
-
-# Self Train with the following string (crashes at 986 games, limits to 900 per session unfortunately)
-# Needs to be a training agent vs a testing agent
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
-
-# Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush
-
-# Try with --bot_build rush (medium)
-
-# Train against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
-# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
-# craig		soft	nofile		8192
-# craig 	hard	nofile		1048576
-# Additionally, tensorboard needs to be run as root (due to higher FD limits):
-# ulimit -n 1048576
-# tensorboard --logdir=runs
-#
-# Validated with: ulimit -s -H && ulimit -n -S
-# required as after 1k episodes we run out of FD's:
-# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+'''
+Abel AI - A Dual-Input Mixed Precision DQN with CNN and Efficient Dual-Storage Replay Buffer.
+
+This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series: https://github.com/skjb/pysc2-tutorial/tree/master
+The PySC2 test harness that he built for his agents is leveraged here (somewhat...) so that the DQN can properly interact with the StarCraft II / PySC2 environment.
+Out of the 1600~ Lines-of-Code, approximately 1300~ are net-new code written by me, with the 300~ lines involving boilerplate code responsible for basic SC2/PySC2 multi-step functions like creating buildings/units
+
+To begin training against a built-in Starcraft II bot (very_easy recommended to begin), use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush
+
+To begin Self-Training (Self-Play reinforcement Learning), use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
+
+To train against Kane-AI, use this string:
+python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+
+---- add to readme:
+file-descriptor limits need to be artificially raised in /etc/security/limits.conf:
+craig		soft	nofile		8192
+craig 	hard	nofile		1048576
+Additionally, tensorboard needs to be run as root (due to higher FD limits):
+ulimit -n 1048576
+tensorboard --logdir=runs
+
+Validated with: ulimit -s -H && ulimit -n -S
+required as after 1k episodes we run out of FD's:
+FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+'''
 
 import random
 from random import sample as random_sample
@@ -35,6 +37,8 @@ import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as F
+# queue used for replay buffer
+from collections import deque
 
 # Using TensorBoard for model performance tracking & visualizations
 from torch.utils.tensorboard import SummaryWriter
@@ -51,8 +55,7 @@ from pysc2.agents import base_agent
 from pysc2.lib import actions
 from pysc2.lib import features
 from pysc2.lib import units
-# queue used for replay buffer
-from collections import deque
+
 
 # BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
 # https://github.com/skjb/pysc2-tutorial/tree/master
@@ -188,41 +191,61 @@ quadrants = [
 # This is used in the smart_attack functions (16 locations with offsets)
 # Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
 
+
 def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
     corner_offset = 3
     mini_quadrant_size = 16  # Each quadrant is divided further into 4
-    
+
     if quadrant == "top-left":
         points = [
-            (top_left_x + corner_offset, top_left_y + corner_offset),                      # Top-left of top-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + corner_offset), # Top-left of top-right mini quadrant
-            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-left of bottom-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-left of bottom-right mini quadrant
+            # Top-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             corner_offset),  # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + \
+             corner_offset),  # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-left of bottom-right mini quadrant
         ]
     elif quadrant == "top-right":
         points = [
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + corner_offset),                   # Top-right of top-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + corner_offset),               # Top-right of top-right mini quadrant
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-right of bottom-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-right of bottom-right mini quadrant
+            # Top-right of top-left mini quadrant
+            (top_left_x + mini_quadrant_size - \
+             corner_offset, top_left_y + corner_offset),
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset),  # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size + corner_offset)  # Top-right of bottom-right mini quadrant
         ]
     elif quadrant == "bottom-left":
         points = [
-            (top_left_x + corner_offset, top_left_y + mini_quadrant_size - corner_offset),                   # Bottom-left of top-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-left of top-right mini quadrant
-            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset),               # Bottom-left of bottom-left mini quadrant
-            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-left of bottom-right mini quadrant
+            # Bottom-left of top-left mini quadrant
+            (top_left_x + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - \
+             corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-left of bottom-right mini quadrant
         ]
     elif quadrant == "bottom-right":
         points = [
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-right of top-right mini quadrant
-            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset), # Bottom-right of bottom-left mini quadrant
-            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-right of bottom-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y +
+             mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + \
+             mini_quadrant_size - corner_offset),  # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset),  # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * \
+             mini_quadrant_size - corner_offset)  # Bottom-right of bottom-right mini quadrant
         ]
 
     return points
 
+
 # For each quadrant, calculate the offset points and append the attack action
 quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
 for i, quad in enumerate(quadrants):
@@ -239,7 +262,7 @@ for index, action in enumerate(smart_actions):
     print(f"# {index}: '{action}'")
 print("--------------------")
 
-#--------------------
+# --------------------
 # Action Mapping
 # 0: 'donothing'
 # 1: 'buildsupplydepot'
@@ -267,8 +290,7 @@ print("--------------------")
 # 23: 'attack_61_45'
 # 24: 'attack_45_61'
 # 25: 'attack_61_61'
-#--------------------
-
+# --------------------
 
 
 # Custom DQN Agent implementation with a replay buffer of 1.25M, gamma of 0.95 (hopefully prioritizing longer term play), and batch_size (confirmed optimal) of 1024
@@ -1147,7 +1169,6 @@ class DQNAgent(base_agent.BaseAgent):
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
 
-
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         all_supply_depots = self.dqn_model.get_units_by_type(
             obs, units.Terran.SupplyDepot)

commit f58e510553631e9af81a44e269f173b380f2329d
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 10 14:22:05 2023 -0400

    Updating comments

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index e9b49787..3ebb741d 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -90,7 +90,7 @@ ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-14.pt-latest.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-15-latest.pt'
 _DATA_FILE = 'dqn-cnn-agent-model-16.pt'
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 
@@ -271,14 +271,15 @@ print("--------------------")
 
 
 
-# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
-# Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
-# To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
-# 1. The 1.2M size deque where state/actions are appended/popped directly in O(1) time
-# 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
+# Custom DQN Agent implementation with a replay buffer of 1.25M, gamma of 0.95 (hopefully prioritizing longer term play), and batch_size (confirmed optimal) of 1024
+# Technically I believe this design could be called a "Dual-Input Mixed Precision DQN with CNN and Efficient Dual-Storage Replay Buffer."
+# To solve the replay buffer's O(n) random lookup issue in Python's `deque`, I have created two separate buffers:
+# 1. The 1.25M size deque where state/actions are appended/popped directly in O(1) time per game step
+# 2. A python list with O(1) time for random lookups. This is copied once from the deque every 10 games/episodes in O(N) time, resulting in large performance improvements
+# Particularly as training occurs multiple times per game (every 100 actions), and fetching 1k items in O(n) time dramatically slowed down gameplay
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.25, buffer_capacity=1250000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.0005, gamma=0.95, e_greedy=0.10, buffer_capacity=1250000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -404,7 +405,7 @@ class DQNModel(nn.Module):
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=15000)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10000)
 
     #
     def _get_conv_out(self, shape):

commit 5b99a6f9b8d6d61eea1a7616ff7541e508d75ae4
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 10 13:47:11 2023 -0400

    Fixed timers, formatting

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index effd52af..e9b49787 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -60,11 +60,6 @@ from collections import deque
 # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
 # His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
 
-####### CUSTOM CODE ######
-_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model13-latest.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-14.pt'
-###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
 _NO_OP = actions.FUNCTIONS.no_op.id
@@ -75,12 +70,8 @@ _TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
 _SELECT_ARMY = actions.FUNCTIONS.select_army.id
 _ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
 _HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
-# CUSTOM
-_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
-# END CUSTOM
 
 _UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
-# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
 
 _TERRAN_COMMANDCENTER = 18
 _TERRAN_SCV = 45
@@ -97,15 +88,21 @@ ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
 ACTION_BUILD_BARRACKS = 'buildbarracks'
 ACTION_BUILD_MARINE = 'buildmarine'
 ACTION_ATTACK = 'attack'
-# CUSTOM
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-14.pt-latest.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-16.pt'
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+
 ACTION_BUILD_SCV = 'buildscv'
 ACTION_ATTACK_UNIT = 'attackunit'
 ACTION_RESET_CAMERA = 'resetcamera'
 ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
 ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
 ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
-# END OF CUSTOM
+###### END OF GLOBAL CUSTOM CODE #####
 
+# Steven Brown also used smart_actions although for this bot I've expanded them considerably
 smart_actions = [
     ACTION_DO_NOTHING,
     ACTION_BUILD_SUPPLY_DEPOT,
@@ -119,7 +116,7 @@ smart_actions = [
     ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
 ]
 
-# DQN Non-Partial State size
+# DQN Non-Spatial State size
 STATE_SIZE = 2
 
 ################################## End of BoilerPlate Code #####################################################
@@ -308,7 +305,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-b'
+        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-c'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
@@ -430,7 +427,6 @@ class DQNModel(nn.Module):
         return self.fc_decision(combined)
 
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
-
     def get_writer(self):
         return SummaryWriter(self.writer_path)
 
@@ -496,8 +492,6 @@ class DQNModel(nn.Module):
     #     return list(self.buffer)[-batch_size:]
 
     # Sample random transitions from replay buffer
-    # Hopefully in a stochastic manner...
-
     def random_sample(self, batch_size):
         return random_sample(self.training_buffer, batch_size)
 
@@ -951,8 +945,6 @@ class DQNAgent(base_agent.BaseAgent):
         # Custom Delay Timers
         self.attack_delay_timer = 0
         self.unit_attack_delay_timer = 0
-        self.supply_delay_timer = 0
-        self.barracks_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
         self.last_camera_action = None
@@ -1013,8 +1005,6 @@ class DQNAgent(base_agent.BaseAgent):
         # Using delay timers to avoid duplicate commands being issued by the AI
         self.attack_delay_timer += 1
         self.unit_attack_delay_timer += 1
-        self.supply_delay_timer += 1
-        self.barracks_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_move_timer += 1
 
@@ -1156,10 +1146,8 @@ class DQNAgent(base_agent.BaseAgent):
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
 
-        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
 
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
-        # supply_depot_count = int(round(len(depot_y) / 69))
         all_supply_depots = self.dqn_model.get_units_by_type(
             obs, units.Terran.SupplyDepot)
         supply_depot_count = len(
@@ -1194,26 +1182,11 @@ class DQNAgent(base_agent.BaseAgent):
         supply_limit = obs.observation['player'][4]
         army_supply = obs.observation['player'][5]
         worker_supply = obs.observation['player'][6]
-        # BEGIN CUSTOM CODE
         # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
-        # print("army_supply is set to: ", army_supply)
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
-        # To do - prioritize enemy units over structures...
-        # enemy_terran_structures = [
-        #     unit for unit in obs.observation.feature_units
-        #     if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
-        # ]
-        # if len(enemy_terran_structures) > 0:
-        #     print("Enemy structures are set to: ", enemy_terran_structures)
-
-        # self_units = [
-        #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
-        # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
-        # END CUSTOM CODE
-
         visible_units = [unit for unit in obs.observation.feature_units]
         # print("Visible Units are set to: ", visible_units)
         # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
@@ -1304,12 +1277,12 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
+            if supply_free > 6 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
             # Also check to see that we have enough minerals
-            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
+            if barracks_count > 5 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
@@ -1321,7 +1294,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
             # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16))
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16):
+            if (len(enemy_units) == 0) or (army_supply < 16):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1346,7 +1319,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 50:
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 5:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
@@ -1513,8 +1486,6 @@ class DQNAgent(base_agent.BaseAgent):
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
             if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
-                # Sleep to avoid duplicate actions
-                self.supply_delay_timer = 0
                 if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
@@ -1535,7 +1506,6 @@ class DQNAgent(base_agent.BaseAgent):
 
             elif smart_action == ACTION_BUILD_BARRACKS:
                 # sleep to avoid loops
-                self.barracks_delay_timer = 0
 
                 if _BUILD_BARRACKS in obs.observation['available_actions']:
                     if self.cc_y.any():

commit a726e86aab356cfc6f59b2cbe9630ba36c42e7b3
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 9 13:26:39 2023 -0400

    updating attack location logic and added mineral checks

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
index 4a415a38..61af0d0f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -4,10 +4,10 @@
 # All of the RL algorithms were implemented by me
 
 # Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 
 # Test against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --game_steps_per_episode 21000
 # file-descriptor limits need to be artificially raised in /etc/security/limits.conf
 # craig		soft	nofile		8192
 # craig 	hard	nofile		1048576
@@ -56,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-12-latest.pt'
+DATA_FILE = 'dqn-cnn-agent-model-14.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -184,30 +184,49 @@ quadrants = [
 # This is used in the smart_attack functions (16 locations with offsets)
 # Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
 
-def calculate_quadrant_points(top_left_x, top_left_y):
-    offset_near = 1
-    offset_far = 3
-    midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
-
-    return [
-        (top_left_x + offset_near, top_left_y +
-         offset_near),  # Close to top-left corner
-        (top_left_x + midpoint_offset - offset_far,
-         top_left_y + offset_near),  # Close to top-right corner
-        (top_left_x + offset_near, top_left_y + midpoint_offset - \
-         offset_far),  # Close to bottom-left corner
-        (top_left_x + midpoint_offset - offset_far, top_left_y + \
-         midpoint_offset - offset_far)  # Close to bottom-right corner
-    ]
 
+def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
+    corner_offset = 3
+    mini_quadrant_size = 16  # Each quadrant is divided further into 4
+    
+    if quadrant == "top-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + corner_offset),                      # Top-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + corner_offset), # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "top-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + corner_offset),                   # Top-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-right of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size - corner_offset),                   # Bottom-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset), # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-right of bottom-right mini quadrant
+        ]
+
+    return points
 
 # For each quadrant, calculate the offset points and append the attack action
-for quad in quadrants:
-    points = calculate_quadrant_points(*quad)
+quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
+for i, quad in enumerate(quadrants):
+    points = calculate_quadrant_points(*quad, quadrant_names[i])
     for x, y in points:
         smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
 
-
 # print("smart_actions is set to: ", smart_actions)
 print("--------------------")
 
@@ -247,7 +266,6 @@ print("--------------------")
 # --------------------
 
 
-
 # Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 # Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
 # To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
@@ -255,7 +273,7 @@ print("--------------------")
 # 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0, gamma=0.9, e_greedy=0.02, buffer_capacity=5000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0, gamma=0.9, e_greedy=0.001, buffer_capacity=5000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -511,7 +529,7 @@ class DQNModel(nn.Module):
         #     self.epsilon -= self.epsilon_decay_rate
         #     self.epsilon = max(self.final_epsilon, self.epsilon)
         else:
-                # Pass the minimap data through the CNN
+            # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
             # Adjusting the shape to [batch_size, feature_size, 1]
             conv_output = conv_output.view(conv_output.size(0), -1, 1)
@@ -909,6 +927,8 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.move_number = 0
 
+        self.minerals = 0
+
         # Used for tracking rewards for use in model saving/checkpointing
         # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
         self.last_rewards = deque([0] * 100, maxlen=100)
@@ -925,8 +945,6 @@ class DQNAgent(base_agent.BaseAgent):
         self.barracks_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        # Reset camera once to help agent place buildings outside of the mineral patch
-        self.camera_reset_required = 1
         self.last_camera_action = None
         # Queue that tracks last actions for exclusionary purposes
         self.last_three_actions = deque(maxlen=3)
@@ -989,6 +1007,8 @@ class DQNAgent(base_agent.BaseAgent):
         self.barracks_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_move_timer += 1
+        # Minerals
+        self.minerals = obs.observation['player'][1]
 
         # Visuals
         rgb_minimap = obs.observation["rgb_minimap"]
@@ -1087,7 +1107,6 @@ class DQNAgent(base_agent.BaseAgent):
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
             self.in_game_training_iterations = 0
-            self.camera_reset_required = 1
             # Clear our action queue
             self.last_three_actions.clear()
             for _ in range(3):
@@ -1101,9 +1120,6 @@ class DQNAgent(base_agent.BaseAgent):
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
         if obs.first():
-
-            self.camera_reset_required = 1
-
             # Original logic, doesn't work properly...
             # player_y, player_x = (
             #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
@@ -1119,7 +1135,7 @@ class DQNAgent(base_agent.BaseAgent):
             # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
 
             player_y, player_x = (obs.observation.feature_minimap.player_relative ==
-                                    features.PlayerRelative.SELF).nonzero()
+                                  features.PlayerRelative.SELF).nonzero()
             xmean = player_x.mean()
             ymean = player_y.mean()
 
@@ -1275,38 +1291,37 @@ class DQNAgent(base_agent.BaseAgent):
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+            # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6:
+            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
-            # TO DO - SLEEP TIMER!
-            # print("Truthy test:", barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6)
-            # print("barracks timer: ", self.barracks_delay_timer)
-            if barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
+            # Also check to see that we have enough minerals
+            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count == 0 or self.last_camera_action != 6:
+            if supply_free == 0 or barracks_count == 0 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
-            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8):
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16))
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
             # SCV Checks
-            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6:
+            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
                 excluded_actions.append(5)
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
-            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 5:
-                # print("self.last_camera_action ==", self.last_camera_action)
+            # print("self.last_camera_action is set to: ", self.last_camera_action)
+            if self.last_camera_action == 6 or self.camera_move_timer < 5:
                 excluded_actions.append(6)
 
             # Camera move handling
@@ -1320,7 +1335,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 10 or self.attack_delay_timer < 60:
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 50:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
@@ -1350,7 +1365,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
-            
+
             # print("Agent action choice: ", rl_action)
             # print("Excluded actions were: ", excluded_actions)
 
@@ -1362,7 +1377,6 @@ class DQNAgent(base_agent.BaseAgent):
                 self.last_camera_action = rl_action
                 # print("self.last_camera_action is:", self.last_camera_action)
 
-
             # Weird error condition where our camera gets locked - purge entries every 100 actions
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("zeroing out camera action")
@@ -1394,6 +1408,9 @@ class DQNAgent(base_agent.BaseAgent):
                     else:
                         print(f"SCV coordinates out of range: {target}")
 
+                else:
+                    print("Failed unit.y check")
+
             elif smart_action == ACTION_BUILD_MARINE:
                 if barracks_y.any():
                     i = random.randint(0, len(barracks_y) - 1)
@@ -1435,7 +1452,6 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Camera counter is at: ",
                 #       self.camera_reset_required, "resetting camera")
                 self.camera_move_timer = 0
-                self.camera_reset_required = 0 
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1495,6 +1511,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
             if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                # Sleep to avoid duplicate actions
+                self.supply_delay_timer = 0
                 if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
@@ -1512,11 +1530,12 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # print("Trying to build a supply depot at:", target)
 
-                        # Sleep to avoid duplicate actions
-                        self.supply_delay_timer = 0
                         return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
             elif smart_action == ACTION_BUILD_BARRACKS:
+                # sleep to avoid loops
+                self.barracks_delay_timer = 0
+
                 if _BUILD_BARRACKS in obs.observation['available_actions']:
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
@@ -1535,16 +1554,16 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # print("Trying to build a barracks at:", target)
 
-                        self.barracks_delay_timer = 0
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
             elif smart_action == ACTION_BUILD_SCV:
                 # print("SCV Smart Action Set")
+
+                # Zero out our build timer
+                self.scv_delay_timer = 0
                 if _TRAIN_SCV in obs.observation['available_actions']:
                     # print("Trying to train an SCV")
-                    # Zero out our build timer
-                    self.scv_delay_timer = 0
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
             # start of boiler plate code (small modifications like delay timers)
@@ -1590,13 +1609,12 @@ class DQNAgent(base_agent.BaseAgent):
                         target_y = max(0, min(target_unit.y, 83))
 
                         # Move the camera to the clamped coordinates
-                        print(
-                            "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+                        # print(
+                        #     "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
 
                         # Reset our timers to 0
                         self.unit_attack_delay_timer = 0
                         self.attack_delay_timer = 0
-                        self.camera_reset_required += 1
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit f66e8f00f7134d73d5a97ea5467732eddf0d6d1f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 9 13:26:09 2023 -0400

    Added mineral checks and updated attack location logic

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 016f8131..effd52af 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -4,11 +4,11 @@
 # All of the RL algorithms were implemented by me
 
 # Self Train with the following string (crashes at 986 games, limits to 900 per session unfortunately)
-# Needs to be a training agent vs a testing agent 
+# Needs to be a training agent vs a testing agent
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
 
 # Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush 
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush
 
 # Try with --bot_build rush (medium)
 
@@ -189,28 +189,47 @@ quadrants = [
 
 # Calculate the offset points for each quadrant
 # This is used in the smart_attack functions (16 locations with offsets)
-# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
-
-def calculate_quadrant_points(top_left_x, top_left_y):
-    offset_near = 1
-    offset_far = 3
-    midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
-
-    return [
-        (top_left_x + offset_near, top_left_y +
-         offset_near),  # Close to top-left corner
-        (top_left_x + midpoint_offset - offset_far,
-         top_left_y + offset_near),  # Close to top-right corner
-        (top_left_x + offset_near, top_left_y + midpoint_offset - \
-         offset_far),  # Close to bottom-left corner
-        (top_left_x + midpoint_offset - offset_far, top_left_y + \
-         midpoint_offset - offset_far)  # Close to bottom-right corner
-    ]
-
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations in mini-quadrants vs 4, complex offsets, etc)
+
+def calculate_quadrant_points(top_left_x, top_left_y, quadrant):
+    corner_offset = 3
+    mini_quadrant_size = 16  # Each quadrant is divided further into 4
+    
+    if quadrant == "top-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + corner_offset),                      # Top-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + corner_offset), # Top-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "top-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + corner_offset),                   # Top-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + corner_offset),               # Top-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset), # Top-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size + corner_offset) # Top-right of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-left":
+        points = [
+            (top_left_x + corner_offset, top_left_y + mini_quadrant_size - corner_offset),                   # Bottom-left of top-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-left of top-right mini quadrant
+            (top_left_x + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset),               # Bottom-left of bottom-left mini quadrant
+            (top_left_x + mini_quadrant_size + corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-left of bottom-right mini quadrant
+        ]
+    elif quadrant == "bottom-right":
+        points = [
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset),     # Bottom-right of top-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + mini_quadrant_size - corner_offset), # Bottom-right of top-right mini quadrant
+            (top_left_x + mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset), # Bottom-right of bottom-left mini quadrant
+            (top_left_x + 2 * mini_quadrant_size - corner_offset, top_left_y + 2 * mini_quadrant_size - corner_offset) # Bottom-right of bottom-right mini quadrant
+        ]
+
+    return points
 
 # For each quadrant, calculate the offset points and append the attack action
-for quad in quadrants:
-    points = calculate_quadrant_points(*quad)
+quadrant_names = ["top-left", "top-right", "bottom-left", "bottom-right"]
+for i, quad in enumerate(quadrants):
+    points = calculate_quadrant_points(*quad, quadrant_names[i])
     for x, y in points:
         smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
 
@@ -223,35 +242,35 @@ for index, action in enumerate(smart_actions):
     print(f"# {index}: '{action}'")
 print("--------------------")
 
-# --------------------
-# # Action Mapping
-# # 0: 'donothing'
-# # 1: 'buildsupplydepot'
-# # 2: 'buildbarracks'
-# # 3: 'buildmarine'
-# # 4: 'attackunit'
-# # 5: 'buildscv'
-# # 6: 'resetcamera'
-# # 7: 'mcselfexpansion'
-# # 8: 'mcenemyexpansion'
-# # 9: 'mcenemyprimary'
-# # 10: 'attack_1_1'
-# # 11: 'attack_13_1'
-# # 12: 'attack_1_13'
-# # 13: 'attack_13_13'
-# # 14: 'attack_33_1'
-# # 15: 'attack_45_1'
-# # 16: 'attack_33_13'
-# # 17: 'attack_45_13'
-# # 18: 'attack_1_33'
-# # 19: 'attack_13_33'
-# # 20: 'attack_1_45'
-# # 21: 'attack_13_45'
-# # 22: 'attack_33_33'
-# # 23: 'attack_45_33'
-# # 24: 'attack_33_45'
-# # 25: 'attack_45_45'
-# --------------------
+#--------------------
+# Action Mapping
+# 0: 'donothing'
+# 1: 'buildsupplydepot'
+# 2: 'buildbarracks'
+# 3: 'buildmarine'
+# 4: 'attackunit'
+# 5: 'buildscv'
+# 6: 'resetcamera'
+# 7: 'mcselfexpansion'
+# 8: 'mcenemyexpansion'
+# 9: 'mcenemyprimary'
+# 10: 'attack_3_3'
+# 11: 'attack_19_3'
+# 12: 'attack_3_19'
+# 13: 'attack_19_19'
+# 14: 'attack_45_3'
+# 15: 'attack_61_3'
+# 16: 'attack_45_19'
+# 17: 'attack_61_19'
+# 18: 'attack_3_45'
+# 19: 'attack_19_45'
+# 20: 'attack_3_61'
+# 21: 'attack_19_61'
+# 22: 'attack_45_45'
+# 23: 'attack_61_45'
+# 24: 'attack_45_61'
+# 25: 'attack_61_61'
+#--------------------
 
 
 
@@ -289,40 +308,40 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-a'
+        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-b'
         # Our replay buffer training theshold size
-        # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
-            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
-            1: 1,  # 'buildsupplydepot'
-            2: 0.95,   # 'buildbarracks'
-            3: 0.8,  # 'buildmarine'
-            4: 0.8,  # 'attackunit' - High reward, attack visible units
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as the AI sees it as a 'safe' move in perpetuity
+            1: 1,     # 'buildsupplydepot'
+            2: 0.95,  # 'buildbarracks'
+            3: 0.8,   # 'buildmarine'
+            4: 0.8,   # 'attackunit' - High reward, attack visible units
             5: 0.8,   # 'buildscv'
             6: 0.8,   # 'resetcamera' - High reward as is required when available
-            7: 0.05,   # 'mcselfexpansion' - checks our expansion
-            8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
-            9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
-            10: 0,    # 'attack_4_4'
-            11: 0,    # 'attack_12_4'
-            12: 0,    # 'attack_4_12'
-            13: 0,    # 'attack_12_12'
-            14: 0,    # 'attack_36_4'
-            15: 0,    # 'attack_44_4'
-            16: 0,    # 'attack_36_12'
-            17: 0,    # 'attack_44_12'
-            18: 0,    # 'attack_4_36'
-            19: 0,    # 'attack_12_36'
-            20: 0,    # 'attack_4_44'
-            21: 0,    # 'attack_12_44'
-            22: 0,    # 'attack_36_36'
-            23: 0,    # 'attack_44_36'
-            24: 0,    # 'attack_36_44'
-            25: 0     # 'attack_44_44'
+            7: 0.05,  # 'mcselfexpansion' - moves camera - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - moves camera - checks enemy expansion
+            9: 0.15,  # 'mcenemyprimary' - moves camera - checks enemy primary base
+            10: 0,    # 'attack_3_3'
+            11: 0,    # 'attack_19_3'
+            12: 0,    # 'attack_3_19'
+            13: 0,    # 'attack_19_19'
+            14: 0,    # 'attack_45_3'
+            15: 0,    # 'attack_61_3'
+            16: 0,    # 'attack_45_19'
+            17: 0,    # 'attack_61_19'
+            18: 0,    # 'attack_3_45'
+            19: 0,    # 'attack_19_45'
+            20: 0,    # 'attack_3_61'
+            21: 0,    # 'attack_19_61'
+            22: 0,    # 'attack_45_45'
+            23: 0,    # 'attack_61_45'
+            24: 0,    # 'attack_45_61'
+            25: 0     # 'attack_61_61'
         }
+
         # Setting up quadrant mapping to avoid repetitive copies in the hot path
         # Define which actions correspond to which quadrants
         # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
@@ -512,7 +531,6 @@ class DQNModel(nn.Module):
                 a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
 
-
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
@@ -549,7 +567,7 @@ class DQNModel(nn.Module):
             for action in excluded_actions:
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
-              
+
         # This is where we keep the logic for epsilon decay (linear)
         self.action_counter += 1
         self.epsilon -= self.epsilon_decay_rate
@@ -918,6 +936,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.move_number = 0
 
+        # Minerals
+        self.minerals = 0
+
         # Used for tracking rewards for use in model saving/checkpointing
         # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
         self.last_rewards = deque([0] * 100, maxlen=100)
@@ -1070,7 +1091,8 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.learn()
                 training_end_time = time.time() - training_start_time
                 self.training_time.append(training_end_time)
-                avg_training_time = sum(self.training_time) / len(self.training_time)
+                avg_training_time = sum(
+                    self.training_time) / len(self.training_time)
                 print(
                     f"This training loop took {training_end_time:.4f} seconds.")
                 print("Training complete")
@@ -1198,6 +1220,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         supply_free = supply_limit - supply_used
 
+        # Minerals
+        self.minerals = obs.observation['player'][1]
+
         if self.move_number == 0:
             self.move_number += 1
 
@@ -1279,15 +1304,16 @@ class DQNAgent(base_agent.BaseAgent):
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
             # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1):
+            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1) or self.minerals < 100:
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
-            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
+            # Also check to see that we have enough minerals
+            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6 or self.minerals < 150 or supply_free == 0:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count == 0 or self.last_camera_action != 6:
+            if supply_free == 0 or barracks_count == 0 or self.minerals < 50:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1300,7 +1326,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(4)
 
             # SCV Checks
-            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6:
+            if worker_supply > 14 or self.scv_delay_timer < 7 or self.last_camera_action != 6 or self.minerals < 50 or supply_free < 3:
                 excluded_actions.append(5)
 
             # Camera reset handling
@@ -1336,7 +1362,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Prevent the bot from doing nothing in perpetuity
             # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
             # Safety check to make sure if no other options are available we don't crash...
-            
+
             # Old Logic - was specific to '0' but now we check for everything instead
             # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
             #     # print("Excluding DO NOTHING")
@@ -1487,6 +1513,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Used to ensure buildings are placed on the border, resulting in trapped units
             BORDER_PADDING = 6
             if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                # Sleep to avoid duplicate actions
+                self.supply_delay_timer = 0
                 if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
@@ -1503,12 +1531,12 @@ class DQNAgent(base_agent.BaseAgent):
                             target[1], 83 - BORDER_PADDING))
 
                         # print("Trying to build a supply depot at:", target)
-
-                        # Sleep to avoid duplicate actions
-                        self.supply_delay_timer = 0
                         return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
             elif smart_action == ACTION_BUILD_BARRACKS:
+                # sleep to avoid loops
+                self.barracks_delay_timer = 0
+
                 if _BUILD_BARRACKS in obs.observation['available_actions']:
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
@@ -1526,16 +1554,16 @@ class DQNAgent(base_agent.BaseAgent):
                             target[1], 83 - BORDER_PADDING))
 
                         # print("Trying to build a barracks at:", target)
-                        self.barracks_delay_timer = 0
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
             elif smart_action == ACTION_BUILD_SCV:
                 # print("SCV Smart Action Set")
+                # Zero out our build timer
+                self.scv_delay_timer = 0
                 if _TRAIN_SCV in obs.observation['available_actions']:
                     # print("Trying to train an SCV")
-                    # Zero out our build timer
-                    self.scv_delay_timer = 0
+
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
             # start of boiler plate code (small modifications like delay timers)

commit 8500881b60da5926bec9e64438aa6ee63d605b36
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Sep 8 16:57:01 2023 -0400

    Fixed epsilon decay and camera movement locks

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 3c324c24..016f8131 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -5,9 +5,10 @@
 
 # Self Train with the following string (crashes at 986 games, limits to 900 per session unfortunately)
 # Needs to be a training agent vs a testing agent 
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 26000
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 21000
+
 # Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --bot_build rush --game_steps_per_episode 22000 --bot_build rush 
 
 # Try with --bot_build rush (medium)
 
@@ -61,8 +62,8 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-12-latest.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-12.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model13-latest.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-14.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -261,7 +262,7 @@ print("--------------------")
 # 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.25, buffer_capacity=1000000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.25, buffer_capacity=1250000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -269,10 +270,10 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.02
+        self.final_epsilon = 0.001
         # We decay over 2M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 2000000
+            self.epsilon - self.final_epsilon) / 3000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -288,10 +289,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v16-self-reinforcement-c'
+        self.writer_path = 'runs/dqn-cnn-agent-v17-medium-rush-bot-a'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 150000
+        self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -511,10 +512,7 @@ class DQNModel(nn.Module):
                 a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
 
-            # This is where we keep the logic for epsilon decay (linear)
-            self.action_counter += 1
-            self.epsilon -= self.epsilon_decay_rate
-            self.epsilon = max(self.final_epsilon, self.epsilon)
+
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
@@ -551,6 +549,11 @@ class DQNModel(nn.Module):
             for action in excluded_actions:
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
+              
+        # This is where we keep the logic for epsilon decay (linear)
+        self.action_counter += 1
+        self.epsilon -= self.epsilon_decay_rate
+        self.epsilon = max(self.final_epsilon, self.epsilon)
 
         # print("The action we chose was: ", action)
         # print("The excluded actions were: ", excluded_actions)
@@ -931,8 +934,6 @@ class DQNAgent(base_agent.BaseAgent):
         self.barracks_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
-        # Reset camera once to help agent place buildings outside of the mineral patch
-        self.camera_reset_required = 1
         self.last_camera_action = None
         # Queue that tracks last actions for exclusionary purposes
         self.last_three_actions = deque(maxlen=3)
@@ -1090,9 +1091,9 @@ class DQNAgent(base_agent.BaseAgent):
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
             self.in_game_training_iterations = 0
-            self.camera_reset_required = 1
             # Clear our action queue
             self.last_three_actions.clear()
+            self.last_camera_action = None
             for _ in range(3):
                 self.last_three_actions.append(None)
 
@@ -1105,8 +1106,6 @@ class DQNAgent(base_agent.BaseAgent):
 
         if obs.first():
 
-            self.camera_reset_required = 1
-
             # Original logic, doesn't work properly...
             # player_y, player_x = (
             #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
@@ -1278,12 +1277,13 @@ class DQNAgent(base_agent.BaseAgent):
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+            # We also get the agent to build at least one barracks before building >1 supply depot
 
-            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6:
+            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6 or (supply_depot_count > 0 and barracks_count < 1):
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
-            if barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
+            if barracks_count > 5 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
@@ -1294,8 +1294,8 @@ class DQNAgent(base_agent.BaseAgent):
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
-            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8):
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16))
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 16):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1305,22 +1305,22 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
-            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 5:
+            if self.last_camera_action == 6 or self.camera_move_timer < 5:
                 # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
+            if self.camera_move_timer < 12 or barracks_count < 3 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
                 excluded_actions.append(9)
 
-            # modified original logic, waits for 8 marines before attacking
+            # modified original logic, waits for 16 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or army_supply < 10 or self.attack_delay_timer < 60:
+            if 4 not in excluded_actions or army_supply < 16 or self.attack_delay_timer < 50:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
@@ -1428,7 +1428,6 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Camera counter is at: ",
                 #       self.camera_reset_required, "resetting camera")
                 self.camera_move_timer = 0
-                self.camera_reset_required = 0
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1582,13 +1581,13 @@ class DQNAgent(base_agent.BaseAgent):
                         target_y = max(0, min(target_unit.y, 83))
 
                         # Move the camera to the clamped coordinates
-                        print(
-                            "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+                        # print(
+                        #     "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
 
                         # Reset our timers to 0
                         self.unit_attack_delay_timer = 0
                         self.attack_delay_timer = 0
-                        self.camera_reset_required += 1
+                        self.last_camera_action = 0
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit 5d44fdb5b4692040a60a8f5cb6edf15041953536
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 6 14:32:41 2023 -0400

    camera race condition fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 35ed8d0a..3c324c24 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -61,7 +61,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-11-latest.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-12-latest.pt'
 _DATA_FILE = 'dqn-cnn-agent-model-12.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -288,7 +288,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v16-self-reinforcement-b'
+        self.writer_path = 'runs/dqn-cnn-agent-v16-self-reinforcement-c'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 150000
@@ -1305,7 +1305,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
-            if self.camera_reset_required <= 0 or self.last_camera_action == 6 or self.camera_move_timer < 5:
+            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 5:
                 # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 

commit 342cd94e42c75ccb97a8f48a0ee82bef73d12522
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 6 14:32:23 2023 -0400

    camera race condition fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
index c4f38d8a..4a415a38 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -56,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-11-latest.pt'
+DATA_FILE = 'dqn-cnn-agent-model-12-latest.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -551,7 +551,6 @@ class DQNModel(nn.Module):
             # print("The excluded actions were: ", excluded_actions)
 
         # Turn training mode back on
-        # print("Agent is choosing: ", action)
         return action
 
     # This is where we train the model
@@ -1306,7 +1305,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
-            if self.camera_reset_required <= 0 or self.last_camera_action == 6 or self.camera_move_timer < 4:
+            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 5:
                 # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 

commit 69df22426deaba06dd13392cdbb7193be8ef9b65
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 6 10:47:47 2023 -0400

    This is where the 'test' agent is stored

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
new file mode 100644
index 00000000..c4f38d8a
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/test_dqn_agent_v14.py
@@ -0,0 +1,1626 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Train the agent against a easy Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+
+# Test against Kane-AI with this string:
+# python -m pysc2.bin.agent --map Simple64 --agent test_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
+# craig		soft	nofile		8192
+# craig 	hard	nofile		1048576
+# Additionally, tensorboard needs to be run as root (due to higher FD limits):
+# ulimit -n 1048576
+# tensorboard --logdir=runs
+#
+# Validated with: ulimit -s -H && ulimit -n -S
+# required as after 1k episodes we run out of FD's:
+# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+
+import random
+from random import sample as random_sample
+import time
+import math
+import os
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+
+# Using TensorBoard for model performance tracking & visualizations
+from torch.utils.tensorboard import SummaryWriter
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...quite poor excellent.
+from torch.optim.lr_scheduler import CosineAnnealingLR
+from collections import Counter
+# Using itertools to slice deque's efficiently
+import itertools
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+from pysc2.lib import units
+# queue used for replay buffer
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-cnn-agent-model-11-latest.pt'
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+# CUSTOM
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+# END CUSTOM
+
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+# CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
+ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
+ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
+ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
+# END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
+    ACTION_MOVE_CAMERA_SELF_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
+]
+
+# DQN Non-Partial State size
+STATE_SIZE = 2
+
+################################## End of BoilerPlate Code #####################################################
+
+# Custom library of all terran buildings
+TERRAN_BUILDINGS = [
+    18,  # Command Center
+    19,  # Supply Depot
+    20,  # Refinery
+    21,  # Barracks
+    22,  # Orbital Command
+    23,  # Factory
+    24,  # Starport
+    25,  # Engineering Bay
+    26,  # Fusion Core
+    27,  # Tech Lab (Barracks)
+    28,  # Tech Lab (Factory)
+    29,  # Tech Lab (Starport)
+    30,  # Reactor (generic, as the building morphs)
+    37,  # Sensor Tower
+    38,  # Bunker
+    39,  # Missile Turret
+    40,  # Auto-turret (from Raven)
+    58,  # Planetary Fortress
+]
+
+TERRAN_UNITS = {
+    # Worker
+    45: "SCV",
+
+    # Basic
+    48: "MULE",
+    51: "Marine",
+    53: "Marauder",
+    54: "Reaper",
+    55: "Ghost",
+
+    # Factory
+    57: "Hellion",
+    58: "SiegeTank",
+    59: "Cyclone",
+    62: "Thor",
+    64: "Hellbat",
+
+    # Starport
+    35: "VikingFighter",
+    36: "VikingAssault",
+    67: "Medivac",
+    68: "Banshee",
+    69: "Raven",
+    70: "Battlecruiser",
+    132: "Liberator",
+    71: "AutoTurret",  # Spawned by Raven
+
+    # Other
+    102: "WidowMine",
+    488: "LiberatorAG"
+}
+
+# Define the top-left coordinates for each quadrant
+quadrants = [
+    (0, 0),       # Top left quadrant
+    (32, 0),      # Top right quadrant
+    (0, 32),      # Bottom left quadrant
+    (32, 32)      # Bottom right quadrant
+]
+
+# Calculate the offset points for each quadrant
+# This is used in the smart_attack functions (16 locations with offsets)
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
+
+def calculate_quadrant_points(top_left_x, top_left_y):
+    offset_near = 1
+    offset_far = 3
+    midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
+
+    return [
+        (top_left_x + offset_near, top_left_y +
+         offset_near),  # Close to top-left corner
+        (top_left_x + midpoint_offset - offset_far,
+         top_left_y + offset_near),  # Close to top-right corner
+        (top_left_x + offset_near, top_left_y + midpoint_offset - \
+         offset_far),  # Close to bottom-left corner
+        (top_left_x + midpoint_offset - offset_far, top_left_y + \
+         midpoint_offset - offset_far)  # Close to bottom-right corner
+    ]
+
+
+# For each quadrant, calculate the offset points and append the attack action
+for quad in quadrants:
+    points = calculate_quadrant_points(*quad)
+    for x, y in points:
+        smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
+
+
+# print("smart_actions is set to: ", smart_actions)
+print("--------------------")
+
+print("# Action Mapping")
+for index, action in enumerate(smart_actions):
+    print(f"# {index}: '{action}'")
+print("--------------------")
+
+# --------------------
+# # Action Mapping
+# # 0: 'donothing'
+# # 1: 'buildsupplydepot'
+# # 2: 'buildbarracks'
+# # 3: 'buildmarine'
+# # 4: 'attackunit'
+# # 5: 'buildscv'
+# # 6: 'resetcamera'
+# # 7: 'mcselfexpansion'
+# # 8: 'mcenemyexpansion'
+# # 9: 'mcenemyprimary'
+# # 10: 'attack_1_1'
+# # 11: 'attack_13_1'
+# # 12: 'attack_1_13'
+# # 13: 'attack_13_13'
+# # 14: 'attack_33_1'
+# # 15: 'attack_45_1'
+# # 16: 'attack_33_13'
+# # 17: 'attack_45_13'
+# # 18: 'attack_1_33'
+# # 19: 'attack_13_33'
+# # 20: 'attack_1_45'
+# # 21: 'attack_13_45'
+# # 22: 'attack_33_33'
+# # 23: 'attack_45_33'
+# # 24: 'attack_33_45'
+# # 25: 'attack_45_45'
+# --------------------
+
+
+
+# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+# Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
+# To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
+# 1. The 1.2M size deque where state/actions are appended/popped directly in O(1) time
+# 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
+# The drawback of course is that space complexity is 2N...
+class DQNModel(nn.Module):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0, gamma=0.9, e_greedy=0.02, buffer_capacity=5000, batch_size=1024):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = gamma
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.01
+        # We decay over 2.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (
+            self.epsilon - self.final_epsilon) / 2500000
+        self.action_counter = 0
+        #
+        self.state_size = non_spatial_state_size
+        self.disallowed_actions = {}
+        # Replay buffer deque and list
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque(maxlen=buffer_capacity)
+        self.training_buffer = []
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+        # Counter for tracking how frequently training is run
+        self.global_training_steps = 0
+        # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
+        self.writer_path = 'runs/dqn-cnn-agent-v15b-testing'
+        # Our replay buffer training theshold size
+        # self.training_buffer_requirement = 500000
+        self.training_buffer_requirement = 150000
+        # This is our custom reward/action mapping dictionary
+        # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        self.action_rewards = {
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
+            1: 1,  # 'buildsupplydepot'
+            2: 0.95,   # 'buildbarracks'
+            3: 0.8,  # 'buildmarine'
+            4: 0.8,  # 'attackunit' - High reward, attack visible units
+            5: 0.5,   # 'buildscv'
+            6: 0.6,   # 'resetcamera' - High reward as is required when available
+            7: 0.05,   # 'mcselfexpansion' - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
+            9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
+            10: 0,    # 'attack_4_4'
+            11: 0,    # 'attack_12_4'
+            12: 0,    # 'attack_4_12'
+            13: 0,    # 'attack_12_12'
+            14: 0,    # 'attack_36_4'
+            15: 0,    # 'attack_44_4'
+            16: 0,    # 'attack_36_12'
+            17: 0,    # 'attack_44_12'
+            18: 0,    # 'attack_4_36'
+            19: 0,    # 'attack_12_36'
+            20: 0,    # 'attack_4_44'
+            21: 0,    # 'attack_12_44'
+            22: 0,    # 'attack_36_36'
+            23: 0,    # 'attack_44_36'
+            24: 0,    # 'attack_36_44'
+            25: 0     # 'attack_44_44'
+        }
+        # Setting up quadrant mapping to avoid repetitive copies in the hot path
+        # Define which actions correspond to which quadrants
+        # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
+        # With the function transformLocation()
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        # CNN for RGB minimap with normalisation
+        self.conv = nn.Sequential(
+            nn.Conv2d(in_channels=3, out_channels=32,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(32),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=32, out_channels=64,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(64),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=64, out_channels=128,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(128),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+        ).to(self.device)  # Moving to GPU if available
+
+        # Calculate the size of our flattened output after convolutional layers
+        self.conv_out_size = self._get_conv_out(minimap_shape)
+
+        # Fully Connected Network (FCN) for non-spatial data with normalization
+        self.fc_non_spatial = nn.Sequential(
+            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
+            nn.ReLU(),
+            nn.Linear(128, 256),
+            nn.BatchNorm1d(300),
+            nn.ReLU()
+        ).to(self.device)  # Moving to GPU if available
+
+        # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
+        self.fc_decision = nn.Sequential(
+            nn.Linear(84992, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, 4096),
+            nn.ReLU(),
+
+            nn.Linear(4096, 2048),
+            nn.ReLU(),
+
+            nn.Linear(2048, len(self.actions))
+        ).to(self.device)  # Moving to GPU if available
+
+        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+        # Our learning rate scheduler is enabled here (CosineAnnealing)
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=15000)
+
+    #
+    def _get_conv_out(self, shape):
+        o = self.conv(torch.zeros(1, *shape).to(self.device))
+        return int(np.prod(o.size()))
+
+    # This is an implicit PyTorch function that's called automatically
+    def forward(self, non_spatial_data, minimap):
+        non_spatial_data = non_spatial_data.to(self.device)
+        minimap = minimap.to(self.device)
+
+        # Separate treatments for different types of data
+        conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
+        fc_out = self.fc_non_spatial(non_spatial_data).reshape(
+            non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
+
+        # Combine both outputs
+        combined = torch.cat((conv_out, fc_out), dim=1)
+
+        return self.fc_decision(combined)
+
+    # Using a method to handle writes to ensure things are closed properly in the event of a crash
+
+    def get_writer(self):
+        return SummaryWriter(self.writer_path)
+
+    # This is where we store items for our replay buffer
+    # s: The current state of the environment.
+    # a: The action taken by the agent in state s.
+    # r: The reward received after taking action a in state s.
+    # s_next: The resulting state after taking
+    def store_transition(self, s, a, r, s_next):
+        # Transition is a tuple (s, a, r, s_next)
+        transition = (s, a, r, s_next)
+        # Append the transition to the replay buffer
+        self.buffer.append(transition)
+
+    def transfer_buffer(self):
+        self.training_buffer = list(self.buffer)
+
+        #####
+        # Debugging
+        # Extract the components of the last element in the buffer
+        # s, a, r, s_next = self.training_buffer[-1]
+
+        # # Print the components
+        # print("s:", s)
+        # print("a:", a)
+        # print("r:", r)
+        # print("s_next:", s_next)
+
+        # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
+        #####
+
+    # This function goes back and tweaks the rewards associated with a given game
+    # Based on the final tangible reward we get from PySC2
+    # Win/loss/draw are multipliers
+    # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
+    # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
+    def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
+        # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
+        steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
+
+        # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
+        for i in range(-steps_to_iterate, 0):
+            # Skip if the buffer index result is None
+            if self.buffer[i] is None:
+                continue
+            # Fetch the transition
+            s, a, r, s_next = self.buffer[i]
+
+            # Modify the reward to include the final reward
+            new_reward = r * final_reward
+            # print("Before backpropagation: ", r)
+
+            # Replace the transition in the replay buffer
+            self.buffer[i] = (s, a, new_reward, s_next)
+            # print("After backpropagation: ", self.buffer[i][2])
+
+        print("Replay buffer currently has: ",
+              len(self.training_buffer), "entries")
+
+    # # sample transitions from replay buffer queue for the last game
+    # def sample(self, batch_size):
+    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+    #     return list(self.buffer)[-batch_size:]
+
+    # Sample random transitions from replay buffer
+    # Hopefully in a stochastic manner...
+
+    def random_sample(self, batch_size):
+        return random_sample(self.training_buffer, batch_size)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Implementation that Steven Brown (PySC2 Dev) created here:
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
+    def choose_action(self, current_state, excluded_actions=[]):
+        # Extract non_spatial_data and rgb_minimap from current_state
+        non_spatial_data = current_state["non_spatial"]
+        rgb_minimap = current_state["rgb_minimap"]
+
+        # Debugs
+        # print("Non-spatial data shape before unsqueeze:", non_spatial_data.shape)
+
+        # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
+        self.eval()
+        # Convert data to tensors and move them to the GPU
+        non_spatial_data_tensor = torch.tensor(
+            non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
+        # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
+        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
+            2, 0, 1).unsqueeze(0).to(self.device)
+
+        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
+
+        # Epsilon-based exploration
+        # Disabled for testing
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+        #     # This is where we keep the logic for epsilon decay (linear)
+        #     self.action_counter += 1
+        #     self.epsilon -= self.epsilon_decay_rate
+        #     self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+                # Pass the minimap data through the CNN
+            conv_output = self.conv(rgb_minimap_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            conv_output = conv_output.view(conv_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
+            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
+
+            # print("Conv shape is: ", conv_output.shape)
+            # print("Non spatial shape is: ", non_spatial_output.shape)
+
+            # Concatenate the two outputs along the second dimension (feature axis)
+            combined_output = torch.cat(
+                (conv_output, non_spatial_output), dim=1)
+
+            # Squeeze out the dummy third dimension
+            combined_output = combined_output.squeeze(2)
+
+            # Finally, pass through the decision-making layers
+            q_values = self.fc_decision(combined_output)
+
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+            # print("The action we chose was: ", action)
+            # print("The excluded actions were: ", excluded_actions)
+
+        # Turn training mode back on
+        # print("Agent is choosing: ", action)
+        return action
+
+    # This is where we train the model
+    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
+    def learn(self):
+        # Check if the replay buffer has enough samples (using 500K as minimum)
+        if len(self.training_buffer) < self.training_buffer_requirement:
+            print("Replay buffer is currently too small to conduct training...")
+            return
+
+        # Increment our training counter
+        self.global_training_steps += 1
+
+        # Sample a mini-batch of transitions from the buffer
+        transitions = self.random_sample(self.batch_size)
+        # Unzip the transitions into separate variables
+        states, actions, rewards, next_states = zip(*transitions)
+
+        # Separate non_spatial and rgb_minimap components of states
+        non_spatial_states = [state["non_spatial"] for state in states]
+        rgb_minimap_states = [state["rgb_minimap"] for state in states]
+
+        non_spatial_next_states = [state["non_spatial"]
+                                   for state in next_states]
+        rgb_minimap_next_states = [state["rgb_minimap"]
+                                   for state in next_states]
+
+        # Convert the zipped values to numpy arrays
+        non_spatial_states_np = np.array(non_spatial_states, dtype=np.float32)
+        rgb_minimap_states_np = np.array(rgb_minimap_states, dtype=np.float32)
+        non_spatial_next_states_np = np.array(
+            non_spatial_next_states, dtype=np.float32)
+        rgb_minimap_next_states_np = np.array(
+            rgb_minimap_next_states, dtype=np.float32)
+        actions_np = np.array(actions, dtype=np.int64)
+        rewards_np = np.array(rewards, dtype=np.float32)
+
+        # Convert numpy arrays to tensors
+        non_spatial_states = torch.tensor(
+            non_spatial_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(
+            self.device).permute(0, 3, 1, 2)
+        non_spatial_next_states = torch.tensor(
+            non_spatial_next_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_next_states = torch.tensor(
+            rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2)
+
+        actions = torch.tensor(actions_np).to(self.device)
+        rewards = torch.tensor(rewards_np).to(self.device)
+
+        # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
+        with autocast():
+            # Compute the Q-values for the current states
+            q_values = self(non_spatial_states, rgb_minimap_states)
+            q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+
+            # Compute the target Q-values
+            next_q_values = self(non_spatial_next_states,
+                                 rgb_minimap_next_states)
+            max_next_q_values = next_q_values.max(1)[0]
+            q_target = rewards + self.gamma * max_next_q_values
+
+            # Compute the loss and update the model's weights
+            loss = self.loss_fn(q_predict, q_target)
+
+            # Debugs if required...
+            #     print("Q-target contains NaN or Infinity!")
+            # if torch.isnan(loss).any() or torch.isinf(loss).any():
+            #     print("Loss contains NaN or Infinity!")
+            # if torch.isnan(q_values).any() or torch.isinf(q_values).any():
+            #     print("Q-values contain NaN or Infinity!")
+            # if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
+            #     print("Non-spatial states contain NaN or Infinity!")
+            # if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
+            #     print("RGB Minimap states contain NaN or Infinity!")
+            # if torch.isnan(rewards).any() or torch.isinf(rewards).any():
+            #     print("Rewards contain NaN or Infinity!")
+            # if math.isnan(self.gamma) or math.isinf(self.gamma):
+            #     print("Gamma contains NaN or Infinity!")
+            # if torch.isnan(actions).any() or torch.isinf(actions).any():
+            #     print("Actions contain NaN or Infinity!")
+
+        # Clear accumulated gradients before back propagation
+        self.optimizer.zero_grad()
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Clip the gradient to avoid huge updates
+        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        # Update the learning rate based on CosineAnnealing scheduling
+        self.scheduler.step()
+
+        # These logs are generated less frequently (every 25 training runs)
+        # Try/Except blocks as they've routinely crashed the simulation :(
+        if self.global_training_steps % 25 == 0:
+            # Logging various metrics for visualization and debugging
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Loss/train', loss.item(),
+                                      self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Loss/train: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Epsilon/value', self.epsilon, self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Epsilon/value: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_histogram(
+                        'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Q-Values: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar(
+                        'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Learning Rate: {e}")
+
+            # Log the histograms of model weights
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    for name, param in self.named_parameters():
+                        writer.add_histogram(name, param.clone().cpu(
+                        ).data.numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging model weights: {e}")
+
+            # Create an action frequency histogram of the last 1000 actions in the replay buffer
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    last_1000_actions = list(itertools.islice(self.training_buffer, len(
+                        self.training_buffer) - 1000, len(self.training_buffer)))
+                    actions = [transition[1]
+                               for transition in last_1000_actions]
+                    action_frequencies = Counter(actions)
+
+                    # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...)
+                    for i in range(len(smart_actions)):
+                        action_frequencies[i] = action_frequencies.get(i, 0)
+
+                    action_list_for_histogram = [
+                        action for action, freq in action_frequencies.items() for _ in range(freq)]
+
+                    writer.add_histogram(
+                        'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Actions/Frequency: {e}")
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint weights
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.load_state_dict(torch.load(file_path))
+
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything about 12k score results in a full score being provided to the model
+        max_score = 12000
+        # Normalize the score to be between 0 and 1
+        normalized_reward = min(score / max_score, 1)
+
+        # --------------------
+        # Action Mapping Reference
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 1
+        attack_adjacent_quadrant = 0.5
+        penalty_home_quadrant = -1
+        penalty_home_expansion = -0.5
+
+        # Incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if previous_action in self.bottom_right_actions:
+            normalized_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            normalized_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            normalized_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            normalized_reward += penalty_home_expansion
+
+        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
+        action_reward = self.action_rewards.get(previous_action, 0)
+        # Add the action reward, ensuring the total reward stays between 0 and 1
+        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+
+        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
+
+        return normalized_reward
+
+    # Provide the x/y coordinates of our command center(s)
+    def get_command_center_coordinates(self, obs):
+        # Get Command Centers using the get_units_by_type function
+        command_centers = self.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+
+        # If there's a Command Center, return its coordinates
+        if command_centers:
+            # Grabbing the first for now (no expansion support for cameras)
+            command_center = command_centers[0]
+
+            # Checks to avoid out-of-bounds crashing
+            # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+            if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
+                return command_center.x, command_center.y
+            else:
+                print(
+                    f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                return None, None
+        else:
+            print("Command Center not found.")
+            return None, None
+
+    # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
+    def translate_coordinates(self, x, y, original_size=84, target_size=64):
+        scale_factor = target_size / original_size
+        return int(x * scale_factor), int(y * scale_factor)
+
+    # To ensure consistency across spawn locations, we invert the minimap locations based on base_top_left
+    def transform_minimap(self, minimap_data, base_top_left):
+        if not base_top_left:
+            # Flip only the spatial dimensions, leaving the color channels unchanged
+            # Copy required due to oddities in the return values..
+            # PyTorch can't deal with negative strides AFAIK
+            transformed_minimap = np.flip(minimap_data, axis=(0, 1)).copy()
+        else:
+            # Leave the minimap as it is
+            transformed_minimap = minimap_data
+
+        return transformed_minimap
+
+    # This creates a fixed-length vector to store visible units in
+    def transform_units_to_fixed_length(self, units, base_top_left):
+
+        # Safety checks
+        # Check if units are None or empty
+        if units is None or len(units) == 0:
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Check for NaN values
+        if np.isnan(np.array(units)).any():
+            print("Warning: NaN values detected in units!")
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Constant indices based on the field names
+        UNIT_TYPE_INDEX = 0
+        ALLIANCE_INDEX = 1
+        HEALTH_INDEX = 2
+        X_POS_INDEX = 12
+        Y_POS_INDEX = 13
+
+        # Number of units and features
+        num_units = len(units)
+        num_features = 5  # ['unit_type', 'health', 'x', 'y']
+
+        # Create an empty array of shape (300, num_features)
+        fixed_length_units = np.zeros((300, num_features))
+
+        # Fill the array with the actual values
+        for i in range(min(num_units, 300)):
+            unit = units[i]
+            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+
+            # Transform x, y coordinates as required for normalization
+            if not base_top_left:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+            else:
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+
+        return fixed_length_units
+
+
+# Agent Implementation
+
+
+class DQNAgent(base_agent.BaseAgent):
+
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Available actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
+
+        self.previous_action = None
+        self.previous_state = {
+            "non_spatial": None,
+            "rgb_minimap": None
+        }
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.training_time = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+        self.actual_root_level_steps_taken = 0
+        self.in_game_training_iterations = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.unit_attack_delay_timer = 0
+        self.supply_delay_timer = 0
+        self.barracks_delay_timer = 0
+        self.scv_delay_timer = 0
+        self.camera_move_timer = 0
+        # Reset camera once to help agent place buildings outside of the mineral patch
+        self.camera_reset_required = 1
+        self.last_camera_action = None
+        # Queue that tracks last actions for exclusionary purposes
+        self.last_three_actions = deque(maxlen=3)
+
+        self.command_center = []
+
+        # Action mapping
+        # Define which actions correspond to which quadrants
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.unit_attack_delay_timer += 1
+        self.supply_delay_timer += 1
+        self.barracks_delay_timer += 1
+        self.scv_delay_timer += 1
+        self.camera_move_timer += 1
+
+        # Visuals
+        rgb_minimap = obs.observation["rgb_minimap"]
+
+        # Check our current score, just for debugging
+        # print("Current score is: ", self.get_normalized_reward(obs))
+
+        # print("self.cc is: ", self.cc_y)
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                # print("Extra steps are: ", extra_steps)
+                # print("total sets were: ", episode_steps)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+                # Ensure combined_reward never goes below 1.1 for a win
+                combined_reward = max(combined_reward, 1.2)
+                final_reward_multiplier = combined_reward
+            elif base_reward == 0:  # 0 indicates a draw
+                combined_reward = base_reward
+                final_reward_multiplier = 0.7  # Slight decrease in score for a draw
+            else:  # -1 indicates a loss
+                combined_reward = base_reward
+                final_reward_multiplier = 0.25
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the rolling average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            print("------------------------------------------------------")
+            # print("Combined reward is set to: ", combined_reward)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            # Checkpoint the model every 200 games
+            # if self.episode_count % 200 == 0:
+            #     print("Checkpointing our model...")
+            #     self.dqn_model.save_model(
+            #         DATA_FILE, self.episode_count, avg_reward)
+
+            # print("Previous average reward was: ",
+            #       self.previous_avg_reward)
+            # print("Our rolling-average reward is: ", avg_reward)
+            # print("Latest game reward was: ", combined_reward)
+            # print("Number of steps were: ", episode_steps)
+            # # Backpropagate the final reward multiplier to previous actions
+            # print(
+            #     f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
+            # self.dqn_model.backpropagate_final_reward(
+            #     final_reward_multiplier, self.actual_root_level_steps_taken)
+
+            # # Copy over our deque replay buffer into our list if it's been 10 games
+            # if self.episode_count % 10 == 0:
+            #     self.dqn_model.transfer_buffer()
+
+            # Print statements if our buffer is large enough to train on...
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+                print("Number of in-game model updates: ",
+                      self.in_game_training_iterations)
+                print("Training the model after game completion...")
+            # Learn after every game, not just the successful ones:
+
+            # Log our reward over time and training duration if we've started training
+            # Learning disabled for testing
+            # if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     training_start_time = time.time()
+            #     self.dqn_model.learn()
+            #     training_end_time = time.time() - training_start_time
+            #     self.training_time.append(training_end_time)
+            #     avg_training_time = sum(self.training_time) / len(self.training_time)
+            #     print(
+            #         f"This training loop took {training_end_time:.4f} seconds.")
+            #     print("Training complete")
+            #     # Log our average reward to TensorBoard
+            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+            #         writer.add_scalar(
+            #             'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+            #     # Log training time
+            #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+            #         writer.add_scalar('Average Training Time in Seconds/train_duration',
+            #                           avg_training_time, self.dqn_model.global_training_steps)
+
+            # Reset remaining, episode-specific counters
+            print("------------------------------------------------------")
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+            self.actual_root_level_steps_taken = 0
+            self.in_game_training_iterations = 0
+            self.camera_reset_required = 1
+            # Clear our action queue
+            self.last_three_actions.clear()
+            for _ in range(3):
+                self.last_three_actions.append(None)
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+
+            self.camera_reset_required = 1
+
+            # Original logic, doesn't work properly...
+            # player_y, player_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            # # print("Player x and y are set to: ", self.player_x, self.player_y)
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+            # Using similar approach to Kane AI to Figure out where our home base is
+            # Original reference is here of course:
+            # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+
+            player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                    features.PlayerRelative.SELF).nonzero()
+            xmean = player_x.mean()
+            ymean = player_y.mean()
+
+            if xmean <= 31 and ymean <= 31:
+                self.base_top_left = True
+            else:
+                self.base_top_left = False
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # supply_depot_count = int(round(len(depot_y) / 69))
+        all_supply_depots = self.dqn_model.get_units_by_type(
+            obs, units.Terran.SupplyDepot)
+        supply_depot_count = len(
+            [unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        all_barracks = self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks)
+        barracks_count = len(
+            [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+
+        all_command_centers = self.dqn_model.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+        command_center_count = len(
+            [unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
+
+        # Find our command centers:
+        # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
+
+        # If we haven't set our global CC yet, add it here
+        if not self.command_center:
+            self.command_center = self.dqn_model.get_units_by_type(
+                obs, units.Terran.CommandCenter)
+
+        # if self.command_center:
+        #     print("Command centers are set to:", self.command_center)
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        # BEGIN CUSTOM CODE
+        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        # print("army_supply is set to: ", army_supply)
+        enemy_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
+        # To do - prioritize enemy units over structures...
+        # enemy_terran_structures = [
+        #     unit for unit in obs.observation.feature_units
+        #     if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
+        # ]
+        # if len(enemy_terran_structures) > 0:
+        #     print("Enemy structures are set to: ", enemy_terran_structures)
+
+        # self_units = [
+        #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
+        # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
+        # END CUSTOM CODE
+
+        visible_units = [unit for unit in obs.observation.feature_units]
+        # print("Visible Units are set to: ", visible_units)
+        # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = {
+                "non_spatial": np.zeros(300),
+                "rgb_minimap": None
+            }
+            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
+                visible_units, self.base_top_left)
+            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+                rgb_minimap, self.base_top_left)
+
+            # current_state["non_spatial"][0] = supply_depot_count
+            # current_state["non_spatial"][1] = barracks_count
+            # current_state["non_spatial"][2] = army_supply
+            # # Custom State Add-on
+            # current_state["non_spatial"][3] = command_center_count
+            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
+            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
+            # # A check to see where the camera is looking
+            # current_state["non_spatial"][6] = self.last_camera_action
+            # # spawn location boolean seems critical for learning
+            # current_state["non_spatial"][7] = self.base_top_left
+
+            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
+
+            # print("State at the end of the step is set to: ", current_state)
+
+            # Push s/a/r/s_next our replay buffer
+            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+            if self.previous_action is not None:
+                self.actual_root_level_steps_taken += 1
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+                self.dqn_model.store_transition(self.previous_state,
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+
+            # Do in-game training of the model for every 100 root actions the agent takes:
+            # Learning disabled for testing
+            # if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
+            #     # print("Beginning in-game training for the model.")
+            #     self.in_game_training_iterations += 1
+            #     self.dqn_model.learn()
+
+            # this is where we store arbitrary actions which the agent is not allowed to take this game step
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+
+            # --------------------
+            # Action Mapping Reference
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
+            # 6: 'resetcamera'
+            # 7: 'mcselfexpansion'
+            # 8: 'mcenemyexpansion'
+            # 9: 'mcenemyprimary'
+            # 10: 'attack_4_4'
+            # 11: 'attack_12_4'
+            # 12: 'attack_4_12'
+            # 13: 'attack_12_12'
+            # 14: 'attack_36_4'
+            # 15: 'attack_44_4'
+            # 16: 'attack_36_12'
+            # 17: 'attack_44_12'
+            # 18: 'attack_4_36'
+            # 19: 'attack_12_36'
+            # 20: 'attack_4_44'
+            # 21: 'attack_12_44'
+            # 22: 'attack_36_36'
+            # 23: 'attack_44_36'
+            # 24: 'attack_36_44'
+            # 25: 'attack_44_44'
+            # --------------------
+
+            # Modified, self-generated code to scale supply depot creation
+            # Includes sleep timer so bot doesn't build them in a loop
+            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+
+            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6:
+                excluded_actions.append(1)
+
+            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            # TO DO - SLEEP TIMER!
+            # print("Truthy test:", barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6)
+            # print("barracks timer: ", self.barracks_delay_timer)
+            if barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
+                excluded_actions.append(2)
+
+            # Exclude marinies from the build queue
+            if supply_free == 0 or barracks_count == 0 or self.last_camera_action != 6:
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                excluded_actions.append(3)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8):
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            # SCV Checks
+            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6:
+                excluded_actions.append(5)
+
+            # Camera reset handling
+            # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
+            if self.camera_reset_required <= 0 or self.last_camera_action == 6 or self.camera_move_timer < 4:
+                # print("self.last_camera_action ==", self.last_camera_action)
+                excluded_actions.append(6)
+
+            # Camera move handling
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
+                # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+
+            # modified original logic, waits for 8 marines before attacking
+            # Excludes all minimap attack actions if we just issued one (at least...for now)
+            # Post-bootstrap, it may be possible to relax these checks
+            # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
+            if 4 not in excluded_actions or army_supply < 10 or self.attack_delay_timer < 60:
+                # Add actions from 10-25 to excluded_actions if they are not present
+                excluded_actions.extend(x for x in range(
+                    10, 26) if x not in excluded_actions)
+
+            #  Excludes attacking home base
+            # These actions always target the home base after x/y transformation
+            # ---- Required for bootstrapping, but not after training due to reward structure ----
+            #
+            # for action in self.top_left_actions:
+            #     if action not in excluded_actions:
+            #         excluded_actions.append(action)
+
+            # Prevent the bot from doing nothing in perpetuity
+            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
+            # Safety check to make sure if no other options are available we don't crash...
+
+            # Old Logic - was specific to '0' but now we check for everything instead
+            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
+            #     # print("Excluding DO NOTHING")
+            #     excluded_actions.append(0)
+
+            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
+                action_to_exclude = self.last_three_actions[0]
+                if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
+                    excluded_actions.append(action_to_exclude)
+
+            # Updated for DQN - let the model  select the action
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+            
+            # print("Agent action choice: ", rl_action)
+            # print("Excluded actions were: ", excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # Set our last camera action:
+            if rl_action in [6, 7, 8, 9]:  # One of the camera actions
+                self.last_camera_action = rl_action
+                # print("self.last_camera_action is:", self.last_camera_action)
+
+
+            # Weird error condition where our camera gets locked - purge entries every 100 actions
+            if self.actual_root_level_steps_taken % 100 == 0:
+                # print("zeroing out camera action")
+                self.last_camera_action = 0
+
+            # Add the action to our tiny 3-element queue
+            self.last_three_actions.append(rl_action)
+
+            # DEBUG : Print all exclusions!
+            # print("Our excluded actions for this step are: ", excluded_actions)
+            # print("The model chose: ", rl_action)
+
+            # using reference code for smart action implementation
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # print("Smart action is set to: ", smart_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    else:
+                        print(f"SCV coordinates out of range: {target}")
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                    else:
+                        print(f"Barracks coordinates out of range: {target}")
+
+            elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("base is top left: ", self.base_top_left)
+                safe_cc_x, safe_cc_y = self.dqn_model.get_command_center_coordinates(
+                    obs)
+                # print("New target should be: ", safe_cc_x, safe_cc_y)
+
+                if safe_cc_x:
+                    target = safe_cc_x, safe_cc_y
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            # To Do:
+            # If get_command_center_coordinates returns None, None
+            # This means we've probably lost our command center and need to build a new one
+            # New action required
+            # Alternative -> Try and repair it if it's taking damage
+
+            # Custom camera reset code
+            # AI can only act on units on its screen (unless using hotkeys/mappings...)
+            # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            # Coordinate flipping isn't ideal for camera movement unfortunately...
+            elif smart_action == ACTION_RESET_CAMERA:
+                # print("Camera counter is at: ",
+                #       self.camera_reset_required, "resetting camera")
+                self.camera_move_timer = 0
+                self.camera_reset_required = 0 
+
+                if self.base_top_left:
+                    # print("Spawned top left - moving camera")
+                    return actions.FUNCTIONS.move_camera((22, 18))
+                else:
+                    # print("Spawned bottom right - moving camera")
+                    return actions.FUNCTIONS.move_camera((43, 51))
+
+            # Camera move logic - need to move the camera to 'see' enemy units to attack them directly
+            # Self expansion
+            # Enemy Primary
+            # Enemy expansion
+
+            # Move camera to our expansion
+            elif smart_action == ACTION_MOVE_CAMERA_SELF_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+
+                if self.base_top_left:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+                else:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+
+            # Move camera to enemy's expansion
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+                else:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+
+            # Move camera to enemy's primary base
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_PRIMARY:
+                # reset our counter so we go back to home base eventually
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 51))
+                else:
+                    # Top left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 18))
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        self.barracks_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("SCV Smart Action Set")
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    # print("Trying to train an SCV")
+                    # Zero out our build timer
+                    self.scv_delay_timer = 0
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+
+            # start of boiler plate code (small modifications like delay timers)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+                    # print("Attacking: ", self.transformLocation(int(x), int(y)))
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    # return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        print(
+                            "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+
+                        # Reset our timers to 0
+                        self.unit_attack_delay_timer = 0
+                        self.attack_delay_timer = 0
+                        self.camera_reset_required += 1
+
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit 1d1852a4770eb5fdfbfd30645358cc4fd7c2415f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Sep 6 10:47:14 2023 -0400

    camera-reset fix and self-reinforcement strings

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 0658796b..35ed8d0a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -3,13 +3,14 @@
 # The RL Algorithm can properly interact with StarCraft II / PySC2
 # All of the RL algorithms were implemented by me
 
-# Self Train with the following string 
-# Needs to be a training agent vs a testing agent
-# String TBD
-
+# Self Train with the following string (crashes at 986 games, limits to 900 per session unfortunately)
+# Needs to be a training agent vs a testing agent 
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 test_dqn_agent_v14.DQNAgent --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84 --max_episodes 905 --game_steps_per_episode 26000
 # Train the agent against a easy Terran bot with the string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
+# Try with --bot_build rush (medium)
+
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 # file-descriptor limits need to be artificially raised in /etc/security/limits.conf
@@ -60,8 +61,8 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-safe.pt'
-_DATA_FILE = 'dqn-cnn-agent-model-11.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-11-latest.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-12.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -260,7 +261,7 @@ print("--------------------")
 # 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.75, buffer_capacity=1000000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.25, buffer_capacity=1000000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -287,7 +288,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v15-vs-kane'
+        self.writer_path = 'runs/dqn-cnn-agent-v16-self-reinforcement-b'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 150000
@@ -1034,8 +1035,8 @@ class DQNAgent(base_agent.BaseAgent):
             # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            # Checkpoint the model every 200 games
-            if self.episode_count % 200 == 0:
+            # Checkpoint the model every 100 games once we've started training
+            if self.episode_count % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 print("Saving our model weights (Checkpoint)...")
                 self.dqn_model.save_model(
                     _DATA_FILE, self.episode_count, avg_reward)
@@ -1304,7 +1305,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Camera reset handling
             # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
-            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 3:
+            if self.camera_reset_required <= 0 or self.last_camera_action == 6 or self.camera_move_timer < 5:
                 # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 
@@ -1427,6 +1428,7 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Camera counter is at: ",
                 #       self.camera_reset_required, "resetting camera")
                 self.camera_move_timer = 0
+                self.camera_reset_required = 0
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1586,6 +1588,7 @@ class DQNAgent(base_agent.BaseAgent):
                         # Reset our timers to 0
                         self.unit_attack_delay_timer = 0
                         self.attack_delay_timer = 0
+                        self.camera_reset_required += 1
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit 00fa5bb983c41f16974aaad2f23239b5efb1bcfd
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Sep 4 19:59:33 2023 -0400

    Action Loop Prevention, separate weight loading and checkpointing

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index a6bc2146..0658796b 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -3,6 +3,10 @@
 # The RL Algorithm can properly interact with StarCraft II / PySC2
 # All of the RL algorithms were implemented by me
 
+# Self Train with the following string 
+# Needs to be a training agent vs a testing agent
+# String TBD
+
 # Train the agent against a easy Terran bot with the string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
@@ -56,7 +60,8 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-10.pt'
+_INITIAL_WEIGHTS = 'dqn-cnn-agent-model-safe.pt'
+_DATA_FILE = 'dqn-cnn-agent-model-11.pt'
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -255,7 +260,7 @@ print("--------------------")
 # 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.80, buffer_capacity=1000000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.75, buffer_capacity=1000000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -263,10 +268,10 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.05
-        # We decay over 2.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.final_epsilon = 0.02
+        # We decay over 2M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 2500000
+            self.epsilon - self.final_epsilon) / 2000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -282,7 +287,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v15b'
+        self.writer_path = 'runs/dqn-cnn-agent-v15-vs-kane'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 150000
@@ -294,8 +299,8 @@ class DQNModel(nn.Module):
             2: 0.95,   # 'buildbarracks'
             3: 0.8,  # 'buildmarine'
             4: 0.8,  # 'attackunit' - High reward, attack visible units
-            5: 0.5,   # 'buildscv'
-            6: 0.6,   # 'resetcamera' - High reward as is required when available
+            5: 0.8,   # 'buildscv'
+            6: 0.8,   # 'resetcamera' - High reward as is required when available
             7: 0.05,   # 'mcselfexpansion' - checks our expansion
             8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
             9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
@@ -922,6 +927,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.attack_delay_timer = 0
         self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
+        self.barracks_delay_timer = 0
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
         # Reset camera once to help agent place buildings outside of the mineral patch
@@ -939,9 +945,9 @@ class DQNAgent(base_agent.BaseAgent):
         self.bottom_left_actions = [18, 19, 20, 21]
         self.bottom_right_actions = [22, 23, 24, 25]
 
-        if os.path.isfile(DATA_FILE):
-            print("Loading previous model: ", DATA_FILE)
-            self.dqn_model.load_model(DATA_FILE)
+        if os.path.isfile(_INITIAL_WEIGHTS):
+            print("Loading previous model: ", _INITIAL_WEIGHTS)
+            self.dqn_model.load_model(_INITIAL_WEIGHTS)
 
     # BOILER PLATE CODE AGAIN
     def transformLocation(self, x, y):
@@ -985,6 +991,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.attack_delay_timer += 1
         self.unit_attack_delay_timer += 1
         self.supply_delay_timer += 1
+        self.barracks_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_move_timer += 1
 
@@ -1029,9 +1036,9 @@ class DQNAgent(base_agent.BaseAgent):
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
             # Checkpoint the model every 200 games
             if self.episode_count % 200 == 0:
-                print("Checkpointing our model...")
+                print("Saving our model weights (Checkpoint)...")
                 self.dqn_model.save_model(
-                    DATA_FILE, self.episode_count, avg_reward)
+                    _DATA_FILE, self.episode_count, avg_reward)
 
             print("Previous average reward was: ",
                   self.previous_avg_reward)
@@ -1275,7 +1282,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(1)
 
             # We guard barracks builds by camera location - need to make sure we're at home base before starting
-            if barracks_count > 4 or self.last_camera_action != 6:
+            if barracks_count > 4 or self.barracks_delay_timer < 10 or self.last_camera_action != 6:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
@@ -1328,9 +1335,16 @@ class DQNAgent(base_agent.BaseAgent):
             # Prevent the bot from doing nothing in perpetuity
             # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
             # Safety check to make sure if no other options are available we don't crash...
-            if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
-                # print("Excluding DO NOTHING")
-                excluded_actions.append(0)
+            
+            # Old Logic - was specific to '0' but now we check for everything instead
+            # if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
+            #     # print("Excluding DO NOTHING")
+            #     excluded_actions.append(0)
+
+            if len(set(self.last_three_actions)) == 1:  # this means all 3 actions were the same
+                action_to_exclude = self.last_three_actions[0]
+                if action_to_exclude not in excluded_actions and len(excluded_actions) < 25:
+                    excluded_actions.append(action_to_exclude)
 
             # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(
@@ -1511,7 +1525,7 @@ class DQNAgent(base_agent.BaseAgent):
                             target[1], 83 - BORDER_PADDING))
 
                         # print("Trying to build a barracks at:", target)
-
+                        self.barracks_delay_timer = 0
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic

commit 814af97114c3969296027bda6ae6b49e3cda3865
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 3 07:23:30 2023 -0400

    Comment cleanup

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index d3344563..a6bc2146 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -57,7 +57,6 @@ from collections import deque
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
 DATA_FILE = 'dqn-cnn-agent-model-10.pt'
-
 ###### END OF GLOBAL CUSTOM CODE #####
 
 ################################## Start of BoilerPlate Code #####################################################
@@ -73,17 +72,11 @@ _HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
 _ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
 # END CUSTOM
 
-_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
 _UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
 # _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
 
-_PLAYER_SELF = 1
-_PLAYER_HOSTILE = 4
-_ARMY_SUPPLY = 5
-
 _TERRAN_COMMANDCENTER = 18
 _TERRAN_SCV = 45
-_TERRAN_SUPPLY_DEPOT = 19
 _TERRAN_BARRACKS = 21
 _NEUTRAL_MINERAL_FIELD = 341
 
@@ -91,6 +84,39 @@ _NOT_QUEUED = [0]
 _QUEUED = [1]
 _SELECT_ALL = [2]
 
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+# CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
+ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
+ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
+ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
+# END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
+    ACTION_MOVE_CAMERA_SELF_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
+]
+
+# DQN Non-Partial State size
+STATE_SIZE = 2
+
+################################## End of BoilerPlate Code #####################################################
+
 # Custom library of all terran buildings
 TERRAN_BUILDINGS = [
     18,  # Command Center
@@ -146,37 +172,6 @@ TERRAN_UNITS = {
     488: "LiberatorAG"
 }
 
-
-ACTION_DO_NOTHING = 'donothing'
-ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
-ACTION_BUILD_BARRACKS = 'buildbarracks'
-ACTION_BUILD_MARINE = 'buildmarine'
-ACTION_ATTACK = 'attack'
-# CUSTOM
-ACTION_BUILD_SCV = 'buildscv'
-ACTION_ATTACK_UNIT = 'attackunit'
-ACTION_RESET_CAMERA = 'resetcamera'
-ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
-ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
-ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
-# END OF CUSTOM
-
-smart_actions = [
-    ACTION_DO_NOTHING,
-    ACTION_BUILD_SUPPLY_DEPOT,
-    ACTION_BUILD_BARRACKS,
-    ACTION_BUILD_MARINE,
-    ACTION_ATTACK_UNIT,
-    ACTION_BUILD_SCV,
-    ACTION_RESET_CAMERA,
-    ACTION_MOVE_CAMERA_SELF_EXPANSION,
-    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
-    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
-]
-
-# DQN State size
-STATE_SIZE = 2
-
 # Define the top-left coordinates for each quadrant
 quadrants = [
     (0, 0),       # Top left quadrant
@@ -184,18 +179,10 @@ quadrants = [
     (0, 32),      # Bottom left quadrant
     (32, 32)      # Bottom right quadrant
 ]
-# 3
-# Steven Brown's implementation for spawn-location-agnostic quadrant attacks
-# for mm_x in range(0, 64):
-#     for mm_y in range(0, 64):
-#         if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
-#             smart_actions.append(ACTION_ATTACK + '_' +
-#                                  str(mm_x - 16) + '_' + str(mm_y - 16))
-################
 
 # Calculate the offset points for each quadrant
 # This is used in the smart_attack functions (16 locations with offsets)
-
+# Inspiration was Steven Brown's logic, however this is...highly modified (16 locations vs 4, complex offsets, etc)
 
 def calculate_quadrant_points(top_left_x, top_left_y):
     offset_near = 1
@@ -230,38 +217,36 @@ for index, action in enumerate(smart_actions):
 print("--------------------")
 
 # --------------------
-# Action Mapping Reference
-# 0: 'donothing'
-# 1: 'buildsupplydepot'
-# 2: 'buildbarracks'
-# 3: 'buildmarine'
-# 4: 'attackunit'
-# 5: 'buildscv'
-# 6: 'resetcamera'
-# 7: 'mcselfexpansion'
-# 8: 'mcenemyexpansion'
-# 9: 'mcenemyprimary'
-# 10: 'attack_4_4'
-# 11: 'attack_12_4'
-# 12: 'attack_4_12'
-# 13: 'attack_12_12'
-# 14: 'attack_36_4'
-# 15: 'attack_44_4'
-# 16: 'attack_36_12'
-# 17: 'attack_44_12'
-# 18: 'attack_4_36'
-# 19: 'attack_12_36'
-# 20: 'attack_4_44'
-# 21: 'attack_12_44'
-# 22: 'attack_36_36'
-# 23: 'attack_44_36'
-# 24: 'attack_36_44'
-# 25: 'attack_44_44'
+# # Action Mapping
+# # 0: 'donothing'
+# # 1: 'buildsupplydepot'
+# # 2: 'buildbarracks'
+# # 3: 'buildmarine'
+# # 4: 'attackunit'
+# # 5: 'buildscv'
+# # 6: 'resetcamera'
+# # 7: 'mcselfexpansion'
+# # 8: 'mcenemyexpansion'
+# # 9: 'mcenemyprimary'
+# # 10: 'attack_1_1'
+# # 11: 'attack_13_1'
+# # 12: 'attack_1_13'
+# # 13: 'attack_13_13'
+# # 14: 'attack_33_1'
+# # 15: 'attack_45_1'
+# # 16: 'attack_33_13'
+# # 17: 'attack_45_13'
+# # 18: 'attack_1_33'
+# # 19: 'attack_13_33'
+# # 20: 'attack_1_45'
+# # 21: 'attack_13_45'
+# # 22: 'attack_33_33'
+# # 23: 'attack_45_33'
+# # 24: 'attack_33_45'
+# # 25: 'attack_45_45'
 # --------------------
 
 
-################################## End of BoilerPlate Code #####################################################
-
 
 # Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 # Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
@@ -1070,23 +1055,21 @@ class DQNAgent(base_agent.BaseAgent):
                 print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
 
-            # Log our reward over time if we've started training
+            # Log our reward over time and training duration if we've started training
             if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
                 self.dqn_model.learn()
                 training_end_time = time.time() - training_start_time
                 self.training_time.append(training_end_time)
                 avg_training_time = sum(self.training_time) / len(self.training_time)
-                # Debug
                 print(
                     f"This training loop took {training_end_time:.4f} seconds.")
                 print("Training complete")
-                # Log our reward to TensorBoard
+                # Log our average reward to TensorBoard
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
-                    # Log average reward
                     writer.add_scalar(
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
-                    # Log training time
+                # Log training time
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar('Average Training Time in Seconds/train_duration',
                                       avg_training_time, self.dqn_model.global_training_steps)

commit 50840aee4f5dc179b0d502fc42c461d0a17ab5aa
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Sep 3 07:18:09 2023 -0400

    Lower learning rate, average training time, and formatting

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 54e0da3c..d3344563 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -270,7 +270,7 @@ print("--------------------")
 # 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
 # The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.01, gamma=0.95, e_greedy=0.90, buffer_capacity=1000000, batch_size=1024):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.001, gamma=0.95, e_greedy=0.80, buffer_capacity=1000000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -297,7 +297,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v15'
+        self.writer_path = 'runs/dqn-cnn-agent-v15b'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 150000
@@ -396,7 +396,7 @@ class DQNModel(nn.Module):
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=20000)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=15000)
 
     #
     def _get_conv_out(self, shape):
@@ -410,15 +410,16 @@ class DQNModel(nn.Module):
 
         # Separate treatments for different types of data
         conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
-        fc_out = self.fc_non_spatial(non_spatial_data).reshape(non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
+        fc_out = self.fc_non_spatial(non_spatial_data).reshape(
+            non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
         combined = torch.cat((conv_out, fc_out), dim=1)
 
         return self.fc_decision(combined)
 
-
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
+
     def get_writer(self):
         return SummaryWriter(self.writer_path)
 
@@ -437,7 +438,7 @@ class DQNModel(nn.Module):
         self.training_buffer = list(self.buffer)
 
         #####
-        ## Debugging
+        # Debugging
         # Extract the components of the last element in the buffer
         # s, a, r, s_next = self.training_buffer[-1]
 
@@ -446,7 +447,7 @@ class DQNModel(nn.Module):
         # print("a:", a)
         # print("r:", r)
         # print("s_next:", s_next)
-        
+
         # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
         #####
 
@@ -475,7 +476,8 @@ class DQNModel(nn.Module):
             self.buffer[i] = (s, a, new_reward, s_next)
             # print("After backpropagation: ", self.buffer[i][2])
 
-        print("Replay buffer currently has: ", len(self.training_buffer), "entries")
+        print("Replay buffer currently has: ",
+              len(self.training_buffer), "entries")
 
     # # sample transitions from replay buffer queue for the last game
     # def sample(self, batch_size):
@@ -509,9 +511,8 @@ class DQNModel(nn.Module):
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
-        
-        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
 
+        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
@@ -532,20 +533,22 @@ class DQNModel(nn.Module):
             # Pass the non-spatial data through its layers
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
             # Adjusting the shape to [batch_size, feature_size, 1]
-            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1, 1)
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
 
-            
             # Pass the non-spatial data through its layers
             # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
-            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1, 1)
+            non_spatial_output = non_spatial_output.view(
+                non_spatial_output.size(0), -1, 1)
             # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
 
             # print("Conv shape is: ", conv_output.shape)
             # print("Non spatial shape is: ", non_spatial_output.shape)
 
             # Concatenate the two outputs along the second dimension (feature axis)
-            combined_output = torch.cat((conv_output, non_spatial_output), dim=1)
+            combined_output = torch.cat(
+                (conv_output, non_spatial_output), dim=1)
 
             # Squeeze out the dummy third dimension
             combined_output = combined_output.squeeze(2)
@@ -629,6 +632,8 @@ class DQNModel(nn.Module):
 
             # Compute the loss and update the model's weights
             loss = self.loss_fn(q_predict, q_target)
+
+            # Debugs if required...
             #     print("Q-target contains NaN or Infinity!")
             # if torch.isnan(loss).any() or torch.isinf(loss).any():
             #     print("Loss contains NaN or Infinity!")
@@ -645,8 +650,6 @@ class DQNModel(nn.Module):
             # if torch.isnan(actions).any() or torch.isinf(actions).any():
             #     print("Actions contain NaN or Infinity!")
 
-
-
         # Clear accumulated gradients before back propagation
         self.optimizer.zero_grad()
 
@@ -665,34 +668,35 @@ class DQNModel(nn.Module):
         # Update the learning rate based on CosineAnnealing scheduling
         self.scheduler.step()
 
-
-
-
         # These logs are generated less frequently (every 25 training runs)
-        # Try/Except blocks as they've routinely crashed the simulation :( 
+        # Try/Except blocks as they've routinely crashed the simulation :(
         if self.global_training_steps % 25 == 0:
             # Logging various metrics for visualization and debugging
             try:
                 with SummaryWriter(self.writer_path) as writer:
-                    writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
+                    writer.add_scalar('Loss/train', loss.item(),
+                                      self.global_training_steps)
             except Exception as e:
                 print(f"Error logging Loss/train: {e}")
 
             try:
                 with SummaryWriter(self.writer_path) as writer:
-                    writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
+                    writer.add_scalar(
+                        'Epsilon/value', self.epsilon, self.global_training_steps)
             except Exception as e:
                 print(f"Error logging Epsilon/value: {e}")
 
             try:
                 with SummaryWriter(self.writer_path) as writer:
-                    writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+                    writer.add_histogram(
+                        'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
             except Exception as e:
                 print(f"Error logging Q-Values: {e}")
 
             try:
                 with SummaryWriter(self.writer_path) as writer:
-                    writer.add_scalar('Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+                    writer.add_scalar(
+                        'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
             except Exception as e:
                 print(f"Error logging Learning Rate: {e}")
 
@@ -700,24 +704,29 @@ class DQNModel(nn.Module):
             try:
                 with SummaryWriter(self.writer_path) as writer:
                     for name, param in self.named_parameters():
-                        writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
+                        writer.add_histogram(name, param.clone().cpu(
+                        ).data.numpy(), self.global_training_steps)
             except Exception as e:
                 print(f"Error logging model weights: {e}")
 
             # Create an action frequency histogram of the last 1000 actions in the replay buffer
             try:
                 with SummaryWriter(self.writer_path) as writer:
-                    last_1000_actions = list(itertools.islice(self.training_buffer, len(self.training_buffer) - 1000, len(self.training_buffer)))
-                    actions = [transition[1] for transition in last_1000_actions]
+                    last_1000_actions = list(itertools.islice(self.training_buffer, len(
+                        self.training_buffer) - 1000, len(self.training_buffer)))
+                    actions = [transition[1]
+                               for transition in last_1000_actions]
                     action_frequencies = Counter(actions)
 
                     # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...)
                     for i in range(len(smart_actions)):
                         action_frequencies[i] = action_frequencies.get(i, 0)
 
-                    action_list_for_histogram = [action for action, freq in action_frequencies.items() for _ in range(freq)]
+                    action_list_for_histogram = [
+                        action for action, freq in action_frequencies.items() for _ in range(freq)]
 
-                    writer.add_histogram('Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+                    writer.add_histogram(
+                        'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
             except Exception as e:
                 print(f"Error logging Actions/Frequency: {e}")
 
@@ -842,10 +851,10 @@ class DQNModel(nn.Module):
             transformed_minimap = minimap_data
 
         return transformed_minimap
-    
+
     # This creates a fixed-length vector to store visible units in
     def transform_units_to_fixed_length(self, units, base_top_left):
-        
+
         # Safety checks
         # Check if units are None or empty
         if units is None or len(units) == 0:
@@ -860,9 +869,9 @@ class DQNModel(nn.Module):
         UNIT_TYPE_INDEX = 0
         ALLIANCE_INDEX = 1
         HEALTH_INDEX = 2
-        X_POS_INDEX = 12 
-        Y_POS_INDEX = 13 
-        
+        X_POS_INDEX = 12
+        Y_POS_INDEX = 13
+
         # Number of units and features
         num_units = len(units)
         num_features = 5  # ['unit_type', 'health', 'x', 'y']
@@ -876,12 +885,14 @@ class DQNModel(nn.Module):
             fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
             fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
             fixed_length_units[i, 2] = unit[HEALTH_INDEX]
-            
+
             # Transform x, y coordinates as required for normalization
             if not base_top_left:
-                fixed_length_units[i, 3], fixed_length_units[i, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
             else:
-                fixed_length_units[i, 3], fixed_length_units[i, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+                fixed_length_units[i, 3], fixed_length_units[i,
+                                                             4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
 
         return fixed_length_units
 
@@ -916,6 +927,7 @@ class DQNAgent(base_agent.BaseAgent):
         # Used for tracking rewards for use in model saving/checkpointing
         # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
         self.last_rewards = deque([0] * 100, maxlen=100)
+        self.training_time = deque([0] * 100, maxlen=100)
         self.episode_count = 0
         self.previous_avg_reward = 0
         self.actual_root_level_steps_taken = 0
@@ -1046,7 +1058,7 @@ class DQNAgent(base_agent.BaseAgent):
                 f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
             self.dqn_model.backpropagate_final_reward(
                 final_reward_multiplier, self.actual_root_level_steps_taken)
-            
+
             # Copy over our deque replay buffer into our list if it's been 10 games
             if self.episode_count % 10 == 0:
                 self.dqn_model.transfer_buffer()
@@ -1063,6 +1075,8 @@ class DQNAgent(base_agent.BaseAgent):
                 training_start_time = time.time()
                 self.dqn_model.learn()
                 training_end_time = time.time() - training_start_time
+                self.training_time.append(training_end_time)
+                avg_training_time = sum(self.training_time) / len(self.training_time)
                 # Debug
                 print(
                     f"This training loop took {training_end_time:.4f} seconds.")
@@ -1074,8 +1088,8 @@ class DQNAgent(base_agent.BaseAgent):
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
                     # Log training time
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
-                    writer.add_scalar('Training Time in Seconds/train_duration',
-                                      training_end_time, self.dqn_model.global_training_steps)
+                    writer.add_scalar('Average Training Time in Seconds/train_duration',
+                                      avg_training_time, self.dqn_model.global_training_steps)
 
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -1183,7 +1197,7 @@ class DQNAgent(base_agent.BaseAgent):
         # if len(enemy_terran_structures) > 0:
         #     print("Enemy structures are set to: ", enemy_terran_structures)
 
-        #self_units = [
+        # self_units = [
         #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
         # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
         # END CUSTOM CODE
@@ -1201,10 +1215,10 @@ class DQNAgent(base_agent.BaseAgent):
                 "non_spatial": np.zeros(300),
                 "rgb_minimap": None
             }
-            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(visible_units, self.base_top_left)
+            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(
+                visible_units, self.base_top_left)
             current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
                 rgb_minimap, self.base_top_left)
-            
 
             # current_state["non_spatial"][0] = supply_depot_count
             # current_state["non_spatial"][1] = barracks_count
@@ -1218,7 +1232,6 @@ class DQNAgent(base_agent.BaseAgent):
             # # spawn location boolean seems critical for learning
             # current_state["non_spatial"][7] = self.base_top_left
 
-
             # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
 
             # print("State at the end of the step is set to: ", current_state)

commit a3a2096c2d865df5da47d3c34090aafdc2c64f58
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 2 20:58:09 2023 -0400

    try/except catching for SummaryWriter crashes

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 5079bc5b..54e0da3c 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -669,44 +669,57 @@ class DQNModel(nn.Module):
 
 
         # These logs are generated less frequently (every 25 training runs)
+        # Try/Except blocks as they've routinely crashed the simulation :( 
         if self.global_training_steps % 25 == 0:
             # Logging various metrics for visualization and debugging
-            with SummaryWriter(self.writer_path) as writer:
-                writer.add_scalar('Loss/train', loss.item(),
-                                  self.global_training_steps)
-            with SummaryWriter(self.writer_path) as writer:
-                writer.add_scalar('Epsilon/value', self.epsilon,
-                                  self.global_training_steps)
-            with SummaryWriter(self.writer_path) as writer:
-                writer.add_histogram(
-                    'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
-            with SummaryWriter(self.writer_path) as writer:
-                writer.add_scalar(
-                    'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Loss/train: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Epsilon/value: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Q-Values: {e}")
+
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    writer.add_scalar('Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Learning Rate: {e}")
 
             # Log the histograms of model weights
-            with SummaryWriter(self.writer_path) as writer:
-                # Log the histograms of model weights
-                for name, param in self.named_parameters():
-                    writer.add_histogram(name, param.clone().cpu(
-                    ).data.numpy(), self.global_training_steps)
-
-            with SummaryWriter(self.writer_path) as writer:
-                # Create an action frequency histogram of the last 1000 actions in the replay buffer
-                last_1000_actions = list(itertools.islice(
-                    self.training_buffer, len(self.training_buffer) - 1000, len(self.training_buffer)))
-                actions = [transition[1] for transition in last_1000_actions]
-                action_frequencies = Counter(actions)
-
-                # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...).
-                for i in range(len(smart_actions)):
-                    action_frequencies[i] = action_frequencies.get(i, 0)
-
-                action_list_for_histogram = [
-                    action for action, freq in action_frequencies.items() for _ in range(freq)]
-
-                writer.add_histogram(
-                    'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    for name, param in self.named_parameters():
+                        writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging model weights: {e}")
+
+            # Create an action frequency histogram of the last 1000 actions in the replay buffer
+            try:
+                with SummaryWriter(self.writer_path) as writer:
+                    last_1000_actions = list(itertools.islice(self.training_buffer, len(self.training_buffer) - 1000, len(self.training_buffer)))
+                    actions = [transition[1] for transition in last_1000_actions]
+                    action_frequencies = Counter(actions)
+
+                    # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...)
+                    for i in range(len(smart_actions)):
+                        action_frequencies[i] = action_frequencies.get(i, 0)
+
+                    action_list_for_histogram = [action for action, freq in action_frequencies.items() for _ in range(freq)]
+
+                    writer.add_histogram('Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
+            except Exception as e:
+                print(f"Error logging Actions/Frequency: {e}")
 
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint weights

commit 1c6448c717ee16ead0147844fcb028bbedab1e6e
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 2 16:14:26 2023 -0400

    Huge update to network, much larger current_state using all visible units

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 50cf8da0..5079bc5b 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -28,6 +28,8 @@ import numpy as np
 import torch
 import torch.nn as nn
 import torch.optim as optim
+import torch.nn.functional as F
+
 # Using TensorBoard for model performance tracking & visualizations
 from torch.utils.tensorboard import SummaryWriter
 # Using amp for mixed-precision (FP16) to improve RTX 4090 performance
@@ -54,7 +56,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-9.pt'
+DATA_FILE = 'dqn-cnn-agent-model-10.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -173,7 +175,7 @@ smart_actions = [
 ]
 
 # DQN State size
-STATE_SIZE = 7
+STATE_SIZE = 2
 
 # Define the top-left coordinates for each quadrant
 quadrants = [
@@ -295,10 +297,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v14-test'
+        self.writer_path = 'runs/dqn-cnn-agent-v15'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 200000
+        self.training_buffer_requirement = 150000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -366,19 +368,18 @@ class DQNModel(nn.Module):
         # Calculate the size of our flattened output after convolutional layers
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
-        # Fully Connected Network (FCN) for non-spatial data with normalisation
+        # Fully Connected Network (FCN) for non-spatial data with normalization
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(non_spatial_state_size, 256),
-            nn.BatchNorm1d(256),
-            nn.ReLU(),
-            nn.Linear(256, 512),
-            nn.BatchNorm1d(512),
+            nn.Linear(5, 128),  # Mapping 5 datapoints to 128
             nn.ReLU(),
+            nn.Linear(128, 256),
+            nn.BatchNorm1d(300),
+            nn.ReLU()
         ).to(self.device)  # Moving to GPU if available
 
         # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
         self.fc_decision = nn.Sequential(
-            nn.Linear(self.conv_out_size + 512, 2048),
+            nn.Linear(84992, 2048),
             nn.ReLU(),
 
             nn.Linear(2048, 4096),
@@ -402,19 +403,21 @@ class DQNModel(nn.Module):
         o = self.conv(torch.zeros(1, *shape).to(self.device))
         return int(np.prod(o.size()))
 
+    # This is an implicit PyTorch function that's called automatically
     def forward(self, non_spatial_data, minimap):
         non_spatial_data = non_spatial_data.to(self.device)
         minimap = minimap.to(self.device)
 
         # Separate treatments for different types of data
         conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
-        fc_out = self.fc_non_spatial(non_spatial_data)
+        fc_out = self.fc_non_spatial(non_spatial_data).reshape(non_spatial_data.size()[0], -1)  # Flatten the tensor to 2D
 
         # Combine both outputs
         combined = torch.cat((conv_out, fc_out), dim=1)
 
         return self.fc_decision(combined)
 
+
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
     def get_writer(self):
         return SummaryWriter(self.writer_path)
@@ -433,10 +436,25 @@ class DQNModel(nn.Module):
     def transfer_buffer(self):
         self.training_buffer = list(self.buffer)
 
+        #####
+        ## Debugging
+        # Extract the components of the last element in the buffer
+        # s, a, r, s_next = self.training_buffer[-1]
+
+        # # Print the components
+        # print("s:", s)
+        # print("a:", a)
+        # print("r:", r)
+        # print("s_next:", s_next)
+        
+        # print("\nlast transfer buffer element is set to:", self.training_buffer[-1])
+        #####
 
     # This function goes back and tweaks the rewards associated with a given game
     # Based on the final tangible reward we get from PySC2
     # Win/loss/draw are multipliers
+    # This backpropagation is slow O(n) but needs to be done in the deque prior to copying to the list in the current code
+    # Otherwise, we'd have to store in multi-game chunks and append it later (will perform better but...complexity/time issues)
     def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
         # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
         steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
@@ -479,6 +497,9 @@ class DQNModel(nn.Module):
         non_spatial_data = current_state["non_spatial"]
         rgb_minimap = current_state["rgb_minimap"]
 
+        # Debugs
+        # print("Non-spatial data shape before unsqueeze:", non_spatial_data.shape)
+
         # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
         self.eval()
         # Convert data to tensors and move them to the GPU
@@ -488,6 +509,9 @@ class DQNModel(nn.Module):
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
+        
+        # print("Non-spatial data shape after unsqueeze:", non_spatial_data.shape)
+
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
@@ -502,15 +526,29 @@ class DQNModel(nn.Module):
         else:
             # Pass the minimap data through the CNN
             conv_output = self.conv(rgb_minimap_tensor)
-            # Flatten the output for fully connected layers
-            conv_output = conv_output.view(conv_output.size(0), -1)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            conv_output = conv_output.view(conv_output.size(0), -1, 1)
+
+            # Pass the non-spatial data through its layers
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            # Adjusting the shape to [batch_size, feature_size, 1]
+            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1, 1)
 
+            
             # Pass the non-spatial data through its layers
+            # print("Non-spatial data shape before fc_non_spatial:", non_spatial_data.shape)
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+            non_spatial_output = non_spatial_output.view(non_spatial_output.size(0), -1, 1)
+            # print("Non-spatial data shape after fc_non_spatial:", non_spatial_data.shape)
+
+            # print("Conv shape is: ", conv_output.shape)
+            # print("Non spatial shape is: ", non_spatial_output.shape)
 
-            # Concatenate the two outputs
-            combined_output = torch.cat(
-                (conv_output, non_spatial_output), dim=1)
+            # Concatenate the two outputs along the second dimension (feature axis)
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1)
+
+            # Squeeze out the dummy third dimension
+            combined_output = combined_output.squeeze(2)
 
             # Finally, pass through the decision-making layers
             q_values = self.fc_decision(combined_output)
@@ -528,7 +566,7 @@ class DQNModel(nn.Module):
         return action
 
     # This is where we train the model
-    # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
+    # It samples randomly from the replay buffer - relatively simplistic compared to PER but seemingly effective!
     def learn(self):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.training_buffer) < self.training_buffer_requirement:
@@ -591,20 +629,21 @@ class DQNModel(nn.Module):
 
             # Compute the loss and update the model's weights
             loss = self.loss_fn(q_predict, q_target)
-
-            ### Debugs / off by default
-            if torch.isnan(q_values).any() or torch.isinf(q_values).any():
-                print("Q-values contain NaN or Infinity!")
-            if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
-                print("Non-spatial states contain NaN or Infinity!")
-            if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
-                print("RGB Minimap states contain NaN or Infinity!")
-            if torch.isnan(rewards).any() or torch.isinf(rewards).any():
-                print("Rewards contain NaN or Infinity!")
-            if math.isnan(self.gamma) or math.isinf(self.gamma):
-                print("Gamma contains NaN or Infinity!")
-            if torch.isnan(actions).any() or torch.isinf(actions).any():
-                print("Actions contain NaN or Infinity!")
+            #     print("Q-target contains NaN or Infinity!")
+            # if torch.isnan(loss).any() or torch.isinf(loss).any():
+            #     print("Loss contains NaN or Infinity!")
+            # if torch.isnan(q_values).any() or torch.isinf(q_values).any():
+            #     print("Q-values contain NaN or Infinity!")
+            # if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
+            #     print("Non-spatial states contain NaN or Infinity!")
+            # if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
+            #     print("RGB Minimap states contain NaN or Infinity!")
+            # if torch.isnan(rewards).any() or torch.isinf(rewards).any():
+            #     print("Rewards contain NaN or Infinity!")
+            # if math.isnan(self.gamma) or math.isinf(self.gamma):
+            #     print("Gamma contains NaN or Infinity!")
+            # if torch.isnan(actions).any() or torch.isinf(actions).any():
+            #     print("Actions contain NaN or Infinity!")
 
 
 
@@ -614,6 +653,9 @@ class DQNModel(nn.Module):
         # Backward pass with scaling
         self.scaler.scale(loss).backward()
 
+        # Clip the gradient to avoid huge updates
+        torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)
+
         # Update the model's weights with scaling
         self.scaler.step(self.optimizer)
 
@@ -632,10 +674,13 @@ class DQNModel(nn.Module):
             with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Loss/train', loss.item(),
                                   self.global_training_steps)
+            with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Epsilon/value', self.epsilon,
                                   self.global_training_steps)
+            with SummaryWriter(self.writer_path) as writer:
                 writer.add_histogram(
                     'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar(
                     'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
 
@@ -646,6 +691,7 @@ class DQNModel(nn.Module):
                     writer.add_histogram(name, param.clone().cpu(
                     ).data.numpy(), self.global_training_steps)
 
+            with SummaryWriter(self.writer_path) as writer:
                 # Create an action frequency histogram of the last 1000 actions in the replay buffer
                 last_1000_actions = list(itertools.islice(
                     self.training_buffer, len(self.training_buffer) - 1000, len(self.training_buffer)))
@@ -783,6 +829,48 @@ class DQNModel(nn.Module):
             transformed_minimap = minimap_data
 
         return transformed_minimap
+    
+    # This creates a fixed-length vector to store visible units in
+    def transform_units_to_fixed_length(self, units, base_top_left):
+        
+        # Safety checks
+        # Check if units are None or empty
+        if units is None or len(units) == 0:
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Check for NaN values
+        if np.isnan(np.array(units)).any():
+            print("Warning: NaN values detected in units!")
+            return np.zeros((300, 5))  # return a zero-filled array
+
+        # Constant indices based on the field names
+        UNIT_TYPE_INDEX = 0
+        ALLIANCE_INDEX = 1
+        HEALTH_INDEX = 2
+        X_POS_INDEX = 12 
+        Y_POS_INDEX = 13 
+        
+        # Number of units and features
+        num_units = len(units)
+        num_features = 5  # ['unit_type', 'health', 'x', 'y']
+
+        # Create an empty array of shape (300, num_features)
+        fixed_length_units = np.zeros((300, num_features))
+
+        # Fill the array with the actual values
+        for i in range(min(num_units, 300)):
+            unit = units[i]
+            fixed_length_units[i, 0] = unit[UNIT_TYPE_INDEX]
+            fixed_length_units[i, 1] = unit[ALLIANCE_INDEX]
+            fixed_length_units[i, 2] = unit[HEALTH_INDEX]
+            
+            # Transform x, y coordinates as required for normalization
+            if not base_top_left:
+                fixed_length_units[i, 3], fixed_length_units[i, 4] = 64 - unit[X_POS_INDEX], 64 - unit[Y_POS_INDEX]
+            else:
+                fixed_length_units[i, 3], fixed_length_units[i, 4] = unit[X_POS_INDEX], unit[Y_POS_INDEX]
+
+        return fixed_length_units
 
 
 # Agent Implementation
@@ -972,6 +1060,7 @@ class DQNAgent(base_agent.BaseAgent):
                     writer.add_scalar(
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
                     # Log training time
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar('Training Time in Seconds/train_duration',
                                       training_end_time, self.dqn_model.global_training_steps)
 
@@ -1074,39 +1163,48 @@ class DQNAgent(base_agent.BaseAgent):
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
         # To do - prioritize enemy units over structures...
-        enemy_terran_structures = [
-            unit for unit in obs.observation.feature_units
-            if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
-        ]
+        # enemy_terran_structures = [
+        #     unit for unit in obs.observation.feature_units
+        #     if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
+        # ]
         # if len(enemy_terran_structures) > 0:
         #     print("Enemy structures are set to: ", enemy_terran_structures)
 
-        self_units = [
-            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
+        #self_units = [
+        #    unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
+        # print("Self units are set to: ", self.dqn_model.transform_units_to_fixed_length(self_units))
         # END CUSTOM CODE
 
+        visible_units = [unit for unit in obs.observation.feature_units]
+        # print("Visible Units are set to: ", visible_units)
+        # print("But transformed, they are: ", self.dqn_model.transform_units_to_fixed_length(visible_units))
+
         supply_free = supply_limit - supply_used
 
         if self.move_number == 0:
             self.move_number += 1
 
             current_state = {
-                "non_spatial": np.zeros(7),
+                "non_spatial": np.zeros(300),
                 "rgb_minimap": None
             }
-            current_state["non_spatial"][0] = cc_count
-            current_state["non_spatial"][1] = supply_depot_count
-            current_state["non_spatial"][2] = barracks_count
-            current_state["non_spatial"][3] = army_supply
-            current_state["non_spatial"][4] = command_center_count
-            # Custom State Add-on
-            current_state["non_spatial"][4] = len(self_units)
-            current_state["non_spatial"][5] = len(enemy_units)
-            # spawn location boolean seems critical for learning
-            current_state["non_spatial"][6] = self.base_top_left
+            current_state["non_spatial"] = self.dqn_model.transform_units_to_fixed_length(visible_units, self.base_top_left)
             current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
                 rgb_minimap, self.base_top_left)
-            # current_state["rgb_minimap"] = rgb_minimap
+            
+
+            # current_state["non_spatial"][0] = supply_depot_count
+            # current_state["non_spatial"][1] = barracks_count
+            # current_state["non_spatial"][2] = army_supply
+            # # Custom State Add-on
+            # current_state["non_spatial"][3] = command_center_count
+            # current_state["non_spatial"][4] = self.dqn_model.transform_units_to_fixed_length(self_units).flatten()
+            # current_state["non_spatial"][5] = self.dqn_model.transform_units_to_fixed_length(enemy_units).flatten()
+            # # A check to see where the camera is looking
+            # current_state["non_spatial"][6] = self.last_camera_action
+            # # spawn location boolean seems critical for learning
+            # current_state["non_spatial"][7] = self.base_top_left
+
 
             # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
 
@@ -1121,7 +1219,7 @@ class DQNAgent(base_agent.BaseAgent):
                                                 self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
 
             # Do in-game training of the model for every 100 root actions the agent takes:
-            if self.actual_root_level_steps_taken % 100 == 0:
+            if self.actual_root_level_steps_taken % 100 == 0 and len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 # print("Beginning in-game training for the model.")
                 self.in_game_training_iterations += 1
                 self.dqn_model.learn()

commit 97fef25ce1c36613b50ba4a91744ca1fc6d97a0a
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 2 11:14:17 2023 -0400

    Implemented deque->list copy to improve random search performance

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index d729bf7f..50cf8da0 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -261,10 +261,14 @@ print("--------------------")
 ################################## End of BoilerPlate Code #####################################################
 
 
-# Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 # Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
+# To solve the O(n) random lookup issue in Python's `deque`, I have two separate bufers
+# 1. The 1.2M size deque where state/actions are appended/popped directly in O(1) time
+# 2. A python list with O(1) time for random lookups. This is populated once from the deque every 10 games/episodes in O(N) time, resulting in large performance savings
+# The drawback of course is that space complexity is 2N...
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.01, gamma=0.95, e_greedy=0.90, buffer_capacity=2000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.01, gamma=0.95, e_greedy=0.90, buffer_capacity=1000000, batch_size=1024):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -280,9 +284,10 @@ class DQNModel(nn.Module):
         #
         self.state_size = non_spatial_state_size
         self.disallowed_actions = {}
-        # Replay buffer
+        # Replay buffer deque and list
         self.buffer_capacity = buffer_capacity
         self.buffer = deque(maxlen=buffer_capacity)
+        self.training_buffer = []
         # self.position = 0
         self.batch_size = batch_size
         # Mixed Precision
@@ -293,7 +298,7 @@ class DQNModel(nn.Module):
         self.writer_path = 'runs/dqn-cnn-agent-v14-test'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 2500
+        self.training_buffer_requirement = 200000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -425,6 +430,10 @@ class DQNModel(nn.Module):
         # Append the transition to the replay buffer
         self.buffer.append(transition)
 
+    def transfer_buffer(self):
+        self.training_buffer = list(self.buffer)
+
+
     # This function goes back and tweaks the rewards associated with a given game
     # Based on the final tangible reward we get from PySC2
     # Win/loss/draw are multipliers
@@ -448,7 +457,7 @@ class DQNModel(nn.Module):
             self.buffer[i] = (s, a, new_reward, s_next)
             # print("After backpropagation: ", self.buffer[i][2])
 
-        print("Replay buffer currently has: ", len(self.buffer), "entries")
+        print("Replay buffer currently has: ", len(self.training_buffer), "entries")
 
     # # sample transitions from replay buffer queue for the last game
     # def sample(self, batch_size):
@@ -459,7 +468,7 @@ class DQNModel(nn.Module):
     # Hopefully in a stochastic manner...
 
     def random_sample(self, batch_size):
-        return random_sample(self.buffer, batch_size)
+        return random_sample(self.training_buffer, batch_size)
 
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
     # Implementation that Steven Brown (PySC2 Dev) created here:
@@ -522,7 +531,7 @@ class DQNModel(nn.Module):
     # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
     def learn(self):
         # Check if the replay buffer has enough samples (using 500K as minimum)
-        if len(self.buffer) < self.training_buffer_requirement:
+        if len(self.training_buffer) < self.training_buffer_requirement:
             print("Replay buffer is currently too small to conduct training...")
             return
 
@@ -639,7 +648,7 @@ class DQNModel(nn.Module):
 
                 # Create an action frequency histogram of the last 1000 actions in the replay buffer
                 last_1000_actions = list(itertools.islice(
-                    self.buffer, len(self.buffer) - 1000, len(self.buffer)))
+                    self.training_buffer, len(self.training_buffer) - 1000, len(self.training_buffer)))
                 actions = [transition[1] for transition in last_1000_actions]
                 action_frequencies = Counter(actions)
 
@@ -936,15 +945,20 @@ class DQNAgent(base_agent.BaseAgent):
                 f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
             self.dqn_model.backpropagate_final_reward(
                 final_reward_multiplier, self.actual_root_level_steps_taken)
+            
+            # Copy over our deque replay buffer into our list if it's been 10 games
+            if self.episode_count % 10 == 0:
+                self.dqn_model.transfer_buffer()
+
             # Print statements if our buffer is large enough to train on...
-            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 print("Number of in-game model updates: ",
                       self.in_game_training_iterations)
                 print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
 
             # Log our reward over time if we've started training
-            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+            if len(self.dqn_model.training_buffer) > self.dqn_model.training_buffer_requirement:
                 training_start_time = time.time()
                 self.dqn_model.learn()
                 training_end_time = time.time() - training_start_time

commit c10a8e7dddecb6d1c0cc7b3a3685546da7ab38c5
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Sep 2 08:04:17 2023 -0400

    Fixed location inversion, loss/q-values, added timing
    
    Still have to do queue-to-array conversion for O(1) random lookups instead of O(n)

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 7f004948..d729bf7f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -11,12 +11,17 @@
 # file-descriptor limits need to be artificially raised in /etc/security/limits.conf
 # craig		soft	nofile		8192
 # craig 	hard	nofile		1048576
+# Additionally, tensorboard needs to be run as root (due to higher FD limits):
+# ulimit -n 1048576
+# tensorboard --logdir=runs
+#
 # Validated with: ulimit -s -H && ulimit -n -S
 # required as after 1k episodes we run out of FD's:
 # FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
 
 import random
 from random import sample as random_sample
+import time
 import math
 import os
 import numpy as np
@@ -49,7 +54,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-8.pt'
+DATA_FILE = 'dqn-cnn-agent-model-9.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -285,17 +290,17 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v12'
+        self.writer_path = 'runs/dqn-cnn-agent-v14-test'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 500000
+        self.training_buffer_requirement = 2500
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
             0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
             1: 1,  # 'buildsupplydepot'
-            2: 1,   # 'buildbarracks'
-            3: 1,  # 'buildmarine'
+            2: 0.95,   # 'buildbarracks'
+            3: 0.8,  # 'buildmarine'
             4: 0.8,  # 'attackunit' - High reward, attack visible units
             5: 0.5,   # 'buildscv'
             6: 0.6,   # 'resetcamera' - High reward as is required when available
@@ -319,6 +324,14 @@ class DQNModel(nn.Module):
             24: 0,    # 'attack_36_44'
             25: 0     # 'attack_44_44'
         }
+        # Setting up quadrant mapping to avoid repetitive copies in the hot path
+        # Define which actions correspond to which quadrants
+        # When issuing an attack-minimap action, locations are normalized for the agent based on spawn location
+        # With the function transformLocation()
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
 
         # Attempt GPU acceleration
         self.device = torch.device(
@@ -570,6 +583,22 @@ class DQNModel(nn.Module):
             # Compute the loss and update the model's weights
             loss = self.loss_fn(q_predict, q_target)
 
+            ### Debugs / off by default
+            if torch.isnan(q_values).any() or torch.isinf(q_values).any():
+                print("Q-values contain NaN or Infinity!")
+            if torch.isnan(non_spatial_states).any() or torch.isinf(non_spatial_states).any():
+                print("Non-spatial states contain NaN or Infinity!")
+            if torch.isnan(rgb_minimap_states).any() or torch.isinf(rgb_minimap_states).any():
+                print("RGB Minimap states contain NaN or Infinity!")
+            if torch.isnan(rewards).any() or torch.isinf(rewards).any():
+                print("Rewards contain NaN or Infinity!")
+            if math.isnan(self.gamma) or math.isinf(self.gamma):
+                print("Gamma contains NaN or Infinity!")
+            if torch.isnan(actions).any() or torch.isinf(actions).any():
+                print("Actions contain NaN or Infinity!")
+
+
+
         # Clear accumulated gradients before back propagation
         self.optimizer.zero_grad()
 
@@ -585,41 +614,42 @@ class DQNModel(nn.Module):
         # Update the learning rate based on CosineAnnealing scheduling
         self.scheduler.step()
 
+
+
+
         # These logs are generated less frequently (every 25 training runs)
         if self.global_training_steps % 25 == 0:
             # Logging various metrics for visualization and debugging
             with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Loss/train', loss.item(),
                                   self.global_training_steps)
-
-            with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Epsilon/value', self.epsilon,
                                   self.global_training_steps)
-
-            with SummaryWriter(self.writer_path) as writer:
                 writer.add_histogram(
                     'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
-
-            with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar(
                     'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
 
             # Log the histograms of model weights
             with SummaryWriter(self.writer_path) as writer:
+                # Log the histograms of model weights
                 for name, param in self.named_parameters():
                     writer.add_histogram(name, param.clone().cpu(
                     ).data.numpy(), self.global_training_steps)
-            # Also create an action frequency histogram of the last 1000 actions in the replay buffer
-            last_1000_actions = list(itertools.islice(
-                self.buffer, len(self.buffer) - 1000, len(self.buffer)))
-            actions = [transition[1] for transition in last_1000_actions]
-            action_frequencies = Counter(actions)
-            # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...).
-            for i in range(len(smart_actions)):
-                action_frequencies[i] = action_frequencies.get(i, 0)
-            action_list_for_histogram = [
-                action for action, freq in action_frequencies.items() for _ in range(freq)]
-            with SummaryWriter(self.writer_path) as writer:
+
+                # Create an action frequency histogram of the last 1000 actions in the replay buffer
+                last_1000_actions = list(itertools.islice(
+                    self.buffer, len(self.buffer) - 1000, len(self.buffer)))
+                actions = [transition[1] for transition in last_1000_actions]
+                action_frequencies = Counter(actions)
+
+                # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...).
+                for i in range(len(smart_actions)):
+                    action_frequencies[i] = action_frequencies.get(i, 0)
+
+                action_list_for_histogram = [
+                    action for action, freq in action_frequencies.items() for _ in range(freq)]
+
                 writer.add_histogram(
                     'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
 
@@ -640,7 +670,7 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs, previous_action, base_top_left):
+    def get_normalized_reward(self, obs, previous_action):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
         # Anything about 12k score results in a full score being provided to the model
@@ -681,42 +711,19 @@ class DQNModel(nn.Module):
         # Incentive multipliers for attack logic
         attack_opposite_quadrant = 1
         attack_adjacent_quadrant = 0.5
-        penalty_home_quadrant = -0.75
+        penalty_home_quadrant = -1
         penalty_home_expansion = -0.5
 
-        # Define which actions correspond to which quadrants
-        top_left_actions = [10, 11, 12, 13]
-        top_right_actions = [14, 15, 16, 17]
-        bottom_left_actions = [18, 19, 20, 21]
-        bottom_right_actions = [22, 23, 24, 25]
-
-        # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
+        # Incentivize attacks to opposite quadrant & their expansion
         # Also de-incentivize attacking home base & expansion
-        if base_top_left:
-            # Reward attacking the bottom right the most
-            if previous_action in bottom_right_actions:
-                normalized_reward += attack_opposite_quadrant
-            # Reward attacking the bottom left less
-            elif previous_action in bottom_left_actions:
-                normalized_reward += attack_adjacent_quadrant
-            # Penalize attacking the top left (home base) the most
-            elif previous_action in top_left_actions:
-                normalized_reward += penalty_home_quadrant
-            # Penalize attacking the top right (home expansion) less
-            elif previous_action in top_right_actions:
-                normalized_reward += penalty_home_expansion
-        # Otherwise, we spawned in the bottom right, reward attacks in the top left
-        # Also de-incentivize attacking home base
-        else:
-            # Same logic, but inverted since base is in the bottom right
-            if previous_action in top_left_actions:
-                normalized_reward += attack_opposite_quadrant
-            elif previous_action in top_right_actions:
-                normalized_reward += attack_adjacent_quadrant
-            elif previous_action in bottom_right_actions:
-                normalized_reward += penalty_home_quadrant
-            elif previous_action in bottom_left_actions:
-                normalized_reward += penalty_home_expansion
+        if previous_action in self.bottom_right_actions:
+            normalized_reward += attack_opposite_quadrant
+        elif previous_action in self.bottom_left_actions:
+            normalized_reward += attack_adjacent_quadrant
+        elif previous_action in self.top_left_actions:
+            normalized_reward += penalty_home_quadrant
+        elif previous_action in self.top_right_actions:
+            normalized_reward += penalty_home_expansion
 
         # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
         action_reward = self.action_rewards.get(previous_action, 0)
@@ -818,8 +825,12 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.command_center = []
 
-        # Visuals
-        rgb_minimap = []
+        # Action mapping
+        # Define which actions correspond to which quadrants
+        self.top_left_actions = [10, 11, 12, 13]
+        self.top_right_actions = [14, 15, 16, 17]
+        self.bottom_left_actions = [18, 19, 20, 21]
+        self.bottom_right_actions = [22, 23, 24, 25]
 
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
@@ -931,14 +942,24 @@ class DQNAgent(base_agent.BaseAgent):
                       self.in_game_training_iterations)
                 print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
-            # Internal logic checks size of replay buffer, we don't need to check here also
-            self.dqn_model.learn()
+
             # Log our reward over time if we've started training
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                training_start_time = time.time()
+                self.dqn_model.learn()
+                training_end_time = time.time() - training_start_time
+                # Debug
+                print(
+                    f"This training loop took {training_end_time:.4f} seconds.")
                 print("Training complete")
+                # Log our reward to TensorBoard
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    # Log average reward
                     writer.add_scalar(
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+                    # Log training time
+                    writer.add_scalar('Training Time in Seconds/train_duration',
+                                      training_end_time, self.dqn_model.global_training_steps)
 
             # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -1083,7 +1104,7 @@ class DQNAgent(base_agent.BaseAgent):
                 self.actual_root_level_steps_taken += 1
                 # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
                 self.dqn_model.store_transition(self.previous_state,
-                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, self.base_top_left), current_state)
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
 
             # Do in-game training of the model for every 100 root actions the agent takes:
             if self.actual_root_level_steps_taken % 100 == 0:
@@ -1168,13 +1189,21 @@ class DQNAgent(base_agent.BaseAgent):
 
             # modified original logic, waits for 8 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
-            # Post-bootstrap, it may be possible to relax this
+            # Post-bootstrap, it may be possible to relax these checks
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
             if 4 not in excluded_actions or army_supply < 10 or self.attack_delay_timer < 60:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
 
+            #  Excludes attacking home base
+            # These actions always target the home base after x/y transformation
+            # ---- Required for bootstrapping, but not after training due to reward structure ----
+            #
+            # for action in self.top_left_actions:
+            #     if action not in excluded_actions:
+            #         excluded_actions.append(action)
+
             # Prevent the bot from doing nothing in perpetuity
             # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
             # Safety check to make sure if no other options are available we don't crash...
@@ -1388,8 +1417,6 @@ class DQNAgent(base_agent.BaseAgent):
                     do_it = False
 
                 if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
-                    # x_offset = random.randint(-1, 1)
-                    # y_offset = random.randint(-1, 1)
 
                     # Debugs
                     # print("Our base is top left: ", self.base_top_left)

commit b8c7791a3111756da0f892731e635b8fed3a6fc4
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Aug 29 17:27:52 2023 -0400

    grammar

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 1c3e6b34..7f004948 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -920,7 +920,7 @@ class DQNAgent(base_agent.BaseAgent):
             print("Our rolling-average reward is: ", avg_reward)
             print("Latest game reward was: ", combined_reward)
             print("Number of steps were: ", episode_steps)
-            # Backpropagate the final reward to previous actions
+            # Backpropagate the final reward multiplier to previous actions
             print(
                 f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
             self.dqn_model.backpropagate_final_reward(

commit 4d77458c4ea9aaac7247e5ce22ab93fb482c26a6
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Aug 29 15:11:54 2023 -0400

    Fixed Learning Rate Scheduler, Reward Structure, Logging, Grid Locations

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index c636eec5..1c3e6b34 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -191,14 +191,19 @@ quadrants = [
 
 
 def calculate_quadrant_points(top_left_x, top_left_y):
-    offset = 4
+    offset_near = 1
+    offset_far = 3
     midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
+
     return [
-        (top_left_x + offset, top_left_y + offset),
-        (top_left_x + midpoint_offset - offset, top_left_y + offset),
-        (top_left_x + offset, top_left_y + midpoint_offset - offset),
-        (top_left_x + midpoint_offset - offset,
-         top_left_y + midpoint_offset - offset)
+        (top_left_x + offset_near, top_left_y +
+         offset_near),  # Close to top-left corner
+        (top_left_x + midpoint_offset - offset_far,
+         top_left_y + offset_near),  # Close to top-right corner
+        (top_left_x + offset_near, top_left_y + midpoint_offset - \
+         offset_far),  # Close to bottom-left corner
+        (top_left_x + midpoint_offset - offset_far, top_left_y + \
+         midpoint_offset - offset_far)  # Close to bottom-right corner
     ]
 
 
@@ -263,9 +268,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.05
-        # We decay over 1M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 2.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 1000000
+            self.epsilon - self.final_epsilon) / 2500000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -280,10 +285,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v11'
+        self.writer_path = 'runs/dqn-cnn-agent-v12'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 250000
+        self.training_buffer_requirement = 500000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -372,7 +377,7 @@ class DQNModel(nn.Module):
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
-        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=20000)
 
     #
     def _get_conv_out(self, shape):
@@ -582,14 +587,14 @@ class DQNModel(nn.Module):
 
         # These logs are generated less frequently (every 25 training runs)
         if self.global_training_steps % 25 == 0:
-                    # Logging various metrics for visualization and debugging
+            # Logging various metrics for visualization and debugging
             with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Loss/train', loss.item(),
-                                self.global_training_steps)
+                                  self.global_training_steps)
 
             with SummaryWriter(self.writer_path) as writer:
                 writer.add_scalar('Epsilon/value', self.epsilon,
-                                self.global_training_steps)
+                                  self.global_training_steps)
 
             with SummaryWriter(self.writer_path) as writer:
                 writer.add_histogram(
@@ -674,10 +679,10 @@ class DQNModel(nn.Module):
         # --------------------
 
         # Incentive multipliers for attack logic
-        attack_opposite_quadrant = 0.5
-        attack_adjacent_quadrant = 0.25
-        penalty_home_quadrant = -0.5
-        penalty_home_expansion = -0.25
+        attack_opposite_quadrant = 1
+        attack_adjacent_quadrant = 0.5
+        penalty_home_quadrant = -0.75
+        penalty_home_expansion = -0.5
 
         # Define which actions correspond to which quadrants
         top_left_actions = [10, 11, 12, 13]
@@ -811,8 +816,6 @@ class DQNAgent(base_agent.BaseAgent):
         # Queue that tracks last actions for exclusionary purposes
         self.last_three_actions = deque(maxlen=3)
 
-
-
         self.command_center = []
 
         # Visuals
@@ -889,20 +892,21 @@ class DQNAgent(base_agent.BaseAgent):
                 step_reward = 1.0 - step_penalty
                 combined_reward = base_reward + step_reward
                 # Ensure combined_reward never goes below 1.1 for a win
-                combined_reward = max(combined_reward, 1.1)
-                final_reward_multiplier = 1
+                combined_reward = max(combined_reward, 1.2)
+                final_reward_multiplier = combined_reward
             elif base_reward == 0:  # 0 indicates a draw
                 combined_reward = base_reward
-                final_reward_multiplier = 0.95  # Slight decrease in score for a draw
+                final_reward_multiplier = 0.7  # Slight decrease in score for a draw
             else:  # -1 indicates a loss
                 combined_reward = base_reward
-                final_reward_multiplier = 0.5
+                final_reward_multiplier = 0.25
 
             self.last_rewards.append(combined_reward)  # Add the latest reward
             # Calculate the rolling average reward
             avg_reward = sum(self.last_rewards) / len(self.last_rewards)
 
             print("------------------------------------------------------")
+            # print("Combined reward is set to: ", combined_reward)
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
             # Checkpoint the model every 200 games
@@ -917,8 +921,8 @@ class DQNAgent(base_agent.BaseAgent):
             print("Latest game reward was: ", combined_reward)
             print("Number of steps were: ", episode_steps)
             # Backpropagate the final reward to previous actions
-            print("Backpropagating reward updates across",
-                  self.actual_root_level_steps_taken, "root level actions.")
+            print(
+                f"Backpropagating a {final_reward_multiplier}x reward multiplier across {self.actual_root_level_steps_taken} root level actions...")
             self.dqn_model.backpropagate_final_reward(
                 final_reward_multiplier, self.actual_root_level_steps_taken)
             # Print statements if our buffer is large enough to train on...
@@ -936,7 +940,7 @@ class DQNAgent(base_agent.BaseAgent):
                     writer.add_scalar(
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
 
-            # Zero out the remaining, episode-specific counters
+            # Reset remaining, episode-specific counters
             print("------------------------------------------------------")
             self.previous_avg_reward = avg_reward
             self.previous_action = None
@@ -944,6 +948,11 @@ class DQNAgent(base_agent.BaseAgent):
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
             self.in_game_training_iterations = 0
+            self.camera_reset_required = 1
+            # Clear our action queue
+            self.last_three_actions.clear()
+            for _ in range(3):
+                self.last_three_actions.append(None)
 
             return actions.FunctionCall(_NO_OP, [])
 
@@ -954,6 +963,8 @@ class DQNAgent(base_agent.BaseAgent):
 
         if obs.first():
 
+            self.camera_reset_required = 1
+
             # Original logic, doesn't work properly...
             # player_y, player_x = (
             #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
@@ -1113,7 +1124,7 @@ class DQNAgent(base_agent.BaseAgent):
             # 24: 'attack_36_44'
             # 25: 'attack_44_44'
             # --------------------
-            
+
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
@@ -1159,7 +1170,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax this
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 60):
+            if 4 not in excluded_actions or army_supply < 10 or self.attack_delay_timer < 60:
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
@@ -1182,6 +1193,11 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
+            # Weird error condition where our camera gets locked - purge entries every 100 actions
+            if self.actual_root_level_steps_taken % 100 == 0:
+                # print("zeroing out camera action")
+                self.last_camera_action = 0
+
             # Add the action to our tiny 3-element queue
             self.last_three_actions.append(rl_action)
 
@@ -1346,7 +1362,6 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # print("Trying to build a barracks at:", target)
 
-
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
@@ -1406,8 +1421,9 @@ class DQNAgent(base_agent.BaseAgent):
                         print(
                             "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
 
-                        # Reset our timer to 0
+                        # Reset our timers to 0
                         self.unit_attack_delay_timer = 0
+                        self.attack_delay_timer = 0
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit d2ccb6fdd1d11aa9a5cb013a5cfa81e55a6ca920
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 28 11:56:39 2023 -0400

    Excluded '0' as a queue and tweaked camera logic

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index f6182189..c636eec5 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -49,7 +49,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-7.pt'
+DATA_FILE = 'dqn-cnn-agent-model-8.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -280,10 +280,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v10'
+        self.writer_path = 'runs/dqn-cnn-agent-v11'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 200000
+        self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -808,6 +808,9 @@ class DQNAgent(base_agent.BaseAgent):
         # Reset camera once to help agent place buildings outside of the mineral patch
         self.camera_reset_required = 1
         self.last_camera_action = None
+        # Queue that tracks last actions for exclusionary purposes
+        self.last_three_actions = deque(maxlen=3)
+
 
 
         self.command_center = []
@@ -1110,7 +1113,7 @@ class DQNAgent(base_agent.BaseAgent):
             # 24: 'attack_36_44'
             # 25: 'attack_44_44'
             # --------------------
-
+            
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             # We guard supply depot builds by camera location - need to make sure we're at home base before starting
@@ -1140,12 +1143,13 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(5)
 
             # Camera reset handling
-            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 3:
-                print("self.last_camera_action == 6")
+            # print("Truthyness check: ", (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 4)
+            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer > 3:
+                # print("self.last_camera_action ==", self.last_camera_action)
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10 or barracks_count < 5 or self.last_camera_action != 6:
+            if self.camera_move_timer < 12 or barracks_count < 4 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -1155,14 +1159,17 @@ class DQNAgent(base_agent.BaseAgent):
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax this
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
-            if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 50):
+            if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 60):
                 # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
                     10, 26) if x not in excluded_actions)
-                # if 4 not in excluded_actions:
-                #     print("Bypassing attack actions as we can attack a unit directly")
 
-            # print("Our exclusions are set to: ", excluded_actions)
+            # Prevent the bot from doing nothing in perpetuity
+            # Action 0 - 2 out of 3 in the queue... don't 'do nothing' again this step
+            # Safety check to make sure if no other options are available we don't crash...
+            if self.last_three_actions.count(0) >= 2 and len(excluded_actions) < 24:
+                # print("Excluding DO NOTHING")
+                excluded_actions.append(0)
 
             # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(
@@ -1175,8 +1182,11 @@ class DQNAgent(base_agent.BaseAgent):
             if rl_action in [6, 7, 8, 9]:  # One of the camera actions
                 self.last_camera_action = rl_action
 
+            # Add the action to our tiny 3-element queue
+            self.last_three_actions.append(rl_action)
+
             # DEBUG : Print all exclusions!
-            print("Our excluded actions for this step are: ", excluded_actions)
+            # print("Our excluded actions for this step are: ", excluded_actions)
             # print("The model chose: ", rl_action)
 
             # using reference code for smart action implementation

commit a640697d48b0877e0484846e7940815c033d0cca
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 28 09:27:29 2023 -0400

    Fixed camera movement logic and spurious logging

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index fe40c55e..f6182189 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -30,7 +30,9 @@ from torch.cuda.amp import autocast, GradScaler
 # Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
 # The static learning rate of 0.01 was...quite poor excellent.
 from torch.optim.lr_scheduler import CosineAnnealingLR
-
+from collections import Counter
+# Using itertools to slice deque's efficiently
+import itertools
 
 from pysc2.agents import base_agent
 from pysc2.lib import actions
@@ -250,6 +252,7 @@ print("--------------------")
 
 
 # Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+# Technically I think it's called a "Dual-Input Mixed Precision DQN with CNN for Spatial Data."
 class DQNModel(nn.Module):
     def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.01, gamma=0.95, e_greedy=0.90, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
@@ -277,23 +280,23 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v9'
+        self.writer_path = 'runs/dqn-cnn-agent-v10'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 100000
+        self.training_buffer_requirement = 200000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
-            0: -0.5,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
-            1: 0.25,  # 'buildsupplydepot'
-            2: 0.5,   # 'buildbarracks'
-            3: 0.25,  # 'buildmarine'
-            4: 0.75,  # 'attackunit' - Highest reward, attack visible units
+            0: -0.25,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
+            1: 1,  # 'buildsupplydepot'
+            2: 1,   # 'buildbarracks'
+            3: 1,  # 'buildmarine'
+            4: 0.8,  # 'attackunit' - High reward, attack visible units
             5: 0.5,   # 'buildscv'
-            6: 0.5,   # 'resetcamera' - High reward as is required when available
+            6: 0.6,   # 'resetcamera' - High reward as is required when available
             7: 0.05,   # 'mcselfexpansion' - checks our expansion
             8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
-            9: 0.25,   # 'mcenemyprimary' - checks enemy primary base
+            9: 0.15,   # 'mcenemyprimary' - checks enemy primary base
             10: 0,    # 'attack_4_4'
             11: 0,    # 'attack_12_4'
             12: 0,    # 'attack_4_12'
@@ -441,8 +444,7 @@ class DQNModel(nn.Module):
         return random_sample(self.buffer, batch_size)
 
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
-    # Algorithm that Steven Brown (PySC2 Dev) created
-    # It's visible here
+    # Implementation that Steven Brown (PySC2 Dev) created here:
     # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
     # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
     def choose_action(self, current_state, excluded_actions=[]):
@@ -459,7 +461,6 @@ class DQNModel(nn.Module):
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
-        
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
@@ -494,7 +495,7 @@ class DQNModel(nn.Module):
 
         # print("The action we chose was: ", action)
         # print("The excluded actions were: ", excluded_actions)
-        
+
         # Turn training mode back on
         self.train()
         return action
@@ -579,25 +580,43 @@ class DQNModel(nn.Module):
         # Update the learning rate based on CosineAnnealing scheduling
         self.scheduler.step()
 
-        # Logging various metrics for visualization and debugging
-        with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Loss/train', loss.item(),
-                              self.global_training_steps)
+        # These logs are generated less frequently (every 25 training runs)
+        if self.global_training_steps % 25 == 0:
+                    # Logging various metrics for visualization and debugging
+            with SummaryWriter(self.writer_path) as writer:
+                writer.add_scalar('Loss/train', loss.item(),
+                                self.global_training_steps)
 
-        with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Epsilon/value', self.epsilon,
-                              self.global_training_steps)
+            with SummaryWriter(self.writer_path) as writer:
+                writer.add_scalar('Epsilon/value', self.epsilon,
+                                self.global_training_steps)
 
-        with SummaryWriter(self.writer_path) as writer:
-            writer.add_histogram(
-                'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            with SummaryWriter(self.writer_path) as writer:
+                writer.add_histogram(
+                    'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+
+            with SummaryWriter(self.writer_path) as writer:
+                writer.add_scalar(
+                    'Learning Rate', self.optimizer.param_groups[0]['lr'], self.global_training_steps)
 
-        # Log the histograms of model weights every 100 iterations
-        if self.global_training_steps % 100 == 0:
+            # Log the histograms of model weights
             with SummaryWriter(self.writer_path) as writer:
                 for name, param in self.named_parameters():
                     writer.add_histogram(name, param.clone().cpu(
                     ).data.numpy(), self.global_training_steps)
+            # Also create an action frequency histogram of the last 1000 actions in the replay buffer
+            last_1000_actions = list(itertools.islice(
+                self.buffer, len(self.buffer) - 1000, len(self.buffer)))
+            actions = [transition[1] for transition in last_1000_actions]
+            action_frequencies = Counter(actions)
+            # Ensure all actions have an entry in the Counter (histogram wasn't rendering properly...).
+            for i in range(len(smart_actions)):
+                action_frequencies[i] = action_frequencies.get(i, 0)
+            action_list_for_histogram = [
+                action for action, freq in action_frequencies.items() for _ in range(freq)]
+            with SummaryWriter(self.writer_path) as writer:
+                writer.add_histogram(
+                    'Actions/Frequency', np.array(action_list_for_histogram), self.global_training_steps)
 
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint weights
@@ -788,6 +807,8 @@ class DQNAgent(base_agent.BaseAgent):
         self.camera_move_timer = 0
         # Reset camera once to help agent place buildings outside of the mineral patch
         self.camera_reset_required = 1
+        self.last_camera_action = None
+
 
         self.command_center = []
 
@@ -1092,14 +1113,17 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
-            if supply_free > 6 or self.supply_delay_timer < 15:
+            # We guard supply depot builds by camera location - need to make sure we're at home base before starting
+
+            if supply_free > 6 or self.supply_delay_timer < 15 or self.last_camera_action != 6:
                 excluded_actions.append(1)
 
-            if barracks_count > 4:
+            # We guard barracks builds by camera location - need to make sure we're at home base before starting
+            if barracks_count > 4 or self.last_camera_action != 6:
                 excluded_actions.append(2)
 
             # Exclude marinies from the build queue
-            if supply_free == 0 or barracks_count == 0:
+            if supply_free == 0 or barracks_count == 0 or self.last_camera_action != 6:
                 # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
@@ -1112,15 +1136,16 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(4)
 
             # SCV Checks
-            if worker_supply > 15 or self.scv_delay_timer < 7:
+            if worker_supply > 15 or self.scv_delay_timer < 7 or self.last_camera_action != 6:
                 excluded_actions.append(5)
 
             # Camera reset handling
-            if self.camera_reset_required <= 0 and self.camera_move_timer < 5:
+            if (self.camera_reset_required <= 0 or self.last_camera_action == 6) and self.camera_move_timer < 3:
+                print("self.last_camera_action == 6")
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10 or barracks_count < 5:
+            if self.camera_move_timer < 10 or barracks_count < 5 or self.last_camera_action != 6:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -1146,7 +1171,12 @@ class DQNAgent(base_agent.BaseAgent):
             self.previous_state = current_state
             self.previous_action = rl_action
 
-            # print("Our excluded actions for this step are: ", excluded_actions)
+            # Set our last camera action:
+            if rl_action in [6, 7, 8, 9]:  # One of the camera actions
+                self.last_camera_action = rl_action
+
+            # DEBUG : Print all exclusions!
+            print("Our excluded actions for this step are: ", excluded_actions)
             # print("The model chose: ", rl_action)
 
             # using reference code for smart action implementation
@@ -1204,9 +1234,9 @@ class DQNAgent(base_agent.BaseAgent):
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
             # Coordinate flipping isn't ideal for camera movement unfortunately...
             elif smart_action == ACTION_RESET_CAMERA:
-                self.camera_reset_required = 0
                 # print("Camera counter is at: ",
                 #       self.camera_reset_required, "resetting camera")
+                self.camera_move_timer = 0
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1223,7 +1253,6 @@ class DQNAgent(base_agent.BaseAgent):
             # Move camera to our expansion
             elif smart_action == ACTION_MOVE_CAMERA_SELF_EXPANSION:
                 # reset our counter so we go back to home base eventually
-                self.camera_reset_required += 1
                 self.camera_move_timer = 0
 
                 if self.base_top_left:
@@ -1236,7 +1265,6 @@ class DQNAgent(base_agent.BaseAgent):
             # Move camera to enemy's expansion
             elif smart_action == ACTION_MOVE_CAMERA_ENEMY_EXPANSION:
                 # reset our counter so we go back to home base eventually
-                self.camera_reset_required += 1
                 self.camera_move_timer = 0
                 if self.base_top_left:
                     # Bottom left quadrant center
@@ -1248,7 +1276,6 @@ class DQNAgent(base_agent.BaseAgent):
             # Move camera to enemy's primary base
             elif smart_action == ACTION_MOVE_CAMERA_ENEMY_PRIMARY:
                 # reset our counter so we go back to home base eventually
-                self.camera_reset_required += 1
                 self.camera_move_timer = 0
                 if self.base_top_left:
                     # Bottom right quadrant center
@@ -1309,6 +1336,7 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # print("Trying to build a barracks at:", target)
 
+
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
             # CUSTOM SCV Build Logiic
@@ -1370,7 +1398,6 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # Reset our timer to 0
                         self.unit_attack_delay_timer = 0
-                        self.camera_reset_required += 1
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit 05bf850ead81920c2934f451030a592d7ee72d65
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 27 16:50:39 2023 -0400

    Significant Model Size Increase (10GB VRAM)

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index f400ec13..fe40c55e 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-6.pt'
+DATA_FILE = 'dqn-cnn-agent-model-7.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -246,7 +246,6 @@ print("--------------------")
 # --------------------
 
 
-
 ################################## End of BoilerPlate Code #####################################################
 
 
@@ -278,7 +277,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v8'
+        self.writer_path = 'runs/dqn-cnn-agent-v9'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 100000
@@ -317,14 +316,23 @@ class DQNModel(nn.Module):
         self.device = torch.device(
             "cuda" if torch.cuda.is_available() else "cpu")
 
-        # CNN for RGB minimap
+        # CNN for RGB minimap with normalisation
         self.conv = nn.Sequential(
             nn.Conv2d(in_channels=3, out_channels=32,
                       kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(32),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
+
             nn.Conv2d(in_channels=32, out_channels=64,
                       kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(64),
+            nn.ReLU(),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+
+            nn.Conv2d(in_channels=64, out_channels=128,
+                      kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(128),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
         ).to(self.device)  # Moving to GPU if available
@@ -332,21 +340,27 @@ class DQNModel(nn.Module):
         # Calculate the size of our flattened output after convolutional layers
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
-        # Fully Connected Network (FCN) for non-spatial data
+        # Fully Connected Network (FCN) for non-spatial data with normalisation
         self.fc_non_spatial = nn.Sequential(
-            nn.Linear(non_spatial_state_size, 512),
+            nn.Linear(non_spatial_state_size, 256),
+            nn.BatchNorm1d(256),
+            nn.ReLU(),
+            nn.Linear(256, 512),
+            nn.BatchNorm1d(512),
             nn.ReLU(),
-            # nn.Dropout(0.2)
         ).to(self.device)  # Moving to GPU if available
 
         # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
         self.fc_decision = nn.Sequential(
-            nn.Linear(self.conv_out_size + 512, 1024),
+            nn.Linear(self.conv_out_size + 512, 2048),
             nn.ReLU(),
-            # nn.Dropout(0.2),
-            nn.Linear(1024, 2048),
+
+            nn.Linear(2048, 4096),
             nn.ReLU(),
-            # nn.Dropout(0.2),
+
+            nn.Linear(4096, 2048),
+            nn.ReLU(),
+
             nn.Linear(2048, len(self.actions))
         ).to(self.device)  # Moving to GPU if available
 
@@ -378,7 +392,7 @@ class DQNModel(nn.Module):
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
     def get_writer(self):
         return SummaryWriter(self.writer_path)
-    
+
     # This is where we store items for our replay buffer
     # s: The current state of the environment.
     # a: The action taken by the agent in state s.
@@ -436,6 +450,8 @@ class DQNModel(nn.Module):
         non_spatial_data = current_state["non_spatial"]
         rgb_minimap = current_state["rgb_minimap"]
 
+        # Set eval mode 'on' to avoid breaking BatchNorm - avoids learning
+        self.eval()
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(
             non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
@@ -443,6 +459,7 @@ class DQNModel(nn.Module):
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
             2, 0, 1).unsqueeze(0).to(self.device)
+        
 
         # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
@@ -477,11 +494,14 @@ class DQNModel(nn.Module):
 
         # print("The action we chose was: ", action)
         # print("The excluded actions were: ", excluded_actions)
+        
+        # Turn training mode back on
+        self.train()
         return action
 
     # This is where we train the model
     # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
-    def train(self):
+    def learn(self):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         if len(self.buffer) < self.training_buffer_requirement:
             print("Replay buffer is currently too small to conduct training...")
@@ -634,7 +654,6 @@ class DQNModel(nn.Module):
         # 25: 'attack_44_44'
         # --------------------
 
-
         # Incentive multipliers for attack logic
         attack_opposite_quadrant = 0.5
         attack_adjacent_quadrant = 0.25
@@ -840,6 +859,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
             if base_reward == 1:  # 1 indicates a win
                 extra_steps = max(0, episode_steps - 10000)
+                # print("Extra steps are: ", extra_steps)
+                # print("total sets were: ", episode_steps)
                 step_penalty = extra_steps // 1000 * 0.1
                 step_reward = 1.0 - step_penalty
                 combined_reward = base_reward + step_reward
@@ -883,7 +904,7 @@ class DQNAgent(base_agent.BaseAgent):
                 print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
             # Internal logic checks size of replay buffer, we don't need to check here also
-            self.dqn_model.train()
+            self.dqn_model.learn()
             # Log our reward over time if we've started training
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
                 print("Training complete")
@@ -1033,7 +1054,7 @@ class DQNAgent(base_agent.BaseAgent):
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("Beginning in-game training for the model.")
                 self.in_game_training_iterations += 1
-                self.dqn_model.train()
+                self.dqn_model.learn()
 
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []
@@ -1069,10 +1090,9 @@ class DQNAgent(base_agent.BaseAgent):
             # 25: 'attack_44_44'
             # --------------------
 
-
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
-            if supply_free > 6 or self.supply_delay_timer < 30:
+            if supply_free > 6 or self.supply_delay_timer < 15:
                 excluded_actions.append(1)
 
             if barracks_count > 4:
@@ -1086,8 +1106,8 @@ class DQNAgent(base_agent.BaseAgent):
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
-            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 8))
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 8):
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 2) or (army_supply < 8):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -1096,11 +1116,11 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(5)
 
             # Camera reset handling
-            if self.camera_reset_required <= 0 and self.camera_move_timer < 6:
+            if self.camera_reset_required <= 0 and self.camera_move_timer < 5:
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 5 or barracks_count < 4:
+            if self.camera_move_timer < 10 or barracks_count < 5:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -1207,7 +1227,7 @@ class DQNAgent(base_agent.BaseAgent):
                 self.camera_move_timer = 0
 
                 if self.base_top_left:
-                    # Top right quadrant center 
+                    # Top right quadrant center
                     return actions.FUNCTIONS.move_camera((43, 18))
                 else:
                     # Bottom left quadrant center
@@ -1231,14 +1251,12 @@ class DQNAgent(base_agent.BaseAgent):
                 self.camera_reset_required += 1
                 self.camera_move_timer = 0
                 if self.base_top_left:
-                    # Bottom right quadrant center 
+                    # Bottom right quadrant center
                     return actions.FUNCTIONS.move_camera((43, 51))
                 else:
-                    # Top left quadrant center 
+                    # Top left quadrant center
                     return actions.FUNCTIONS.move_camera((22, 18))
 
-
-
         elif self.move_number == 1:
             self.move_number += 1
 

commit ba9ee2102c344e542dd78332e4a791fa57b59975
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 27 14:32:54 2023 -0400

    bumped learning rate now that it's bootstrapped

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index a947b94e..f400ec13 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -7,7 +7,7 @@
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
 # Train against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 # file-descriptor limits need to be artificially raised in /etc/security/limits.conf
 # craig		soft	nofile		8192
 # craig 	hard	nofile		1048576
@@ -252,7 +252,7 @@ print("--------------------")
 
 # Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.85, buffer_capacity=2000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.01, gamma=0.95, e_greedy=0.90, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -260,7 +260,7 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.01
+        self.final_epsilon = 0.05
         # We decay over 1M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
             self.epsilon - self.final_epsilon) / 1000000
@@ -278,10 +278,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v7'
+        self.writer_path = 'runs/dqn-cnn-agent-v8'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 50000
+        self.training_buffer_requirement = 100000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -378,12 +378,12 @@ class DQNModel(nn.Module):
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
     def get_writer(self):
         return SummaryWriter(self.writer_path)
+    
     # This is where we store items for our replay buffer
     # s: The current state of the environment.
     # a: The action taken by the agent in state s.
     # r: The reward received after taking action a in state s.
     # s_next: The resulting state after taking
-
     def store_transition(self, s, a, r, s_next):
         # Transition is a tuple (s, a, r, s_next)
         transition = (s, a, r, s_next)
@@ -871,25 +871,26 @@ class DQNAgent(base_agent.BaseAgent):
             print("Our rolling-average reward is: ", avg_reward)
             print("Latest game reward was: ", combined_reward)
             print("Number of steps were: ", episode_steps)
-            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
-                print("Number of in-game model updates: ",
-                      self.in_game_training_iterations)
             # Backpropagate the final reward to previous actions
             print("Backpropagating reward updates across",
                   self.actual_root_level_steps_taken, "root level actions.")
             self.dqn_model.backpropagate_final_reward(
                 final_reward_multiplier, self.actual_root_level_steps_taken)
-
+            # Print statements if our buffer is large enough to train on...
+            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                print("Number of in-game model updates: ",
+                      self.in_game_training_iterations)
+                print("Training the model after game completion...")
             # Learn after every game, not just the successful ones:
-            print("Training the model after game completion...")
+            # Internal logic checks size of replay buffer, we don't need to check here also
             self.dqn_model.train()
             # Log our reward over time if we've started training
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                print("Training complete")
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
                     writer.add_scalar(
                         'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
 
-            print("Training complete")
             # Zero out the remaining, episode-specific counters
             print("------------------------------------------------------")
             self.previous_avg_reward = avg_reward
@@ -1099,7 +1100,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 5 or barracks_count < 3:
+            if self.camera_move_timer < 5 or barracks_count < 4:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)

commit ae2259201d31a88de20bdb815cebd586278e4762
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 27 10:51:41 2023 -0400

    Fixed minimap-attack rewards, adjusted timers, bootstrapping config

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index c2f5ff25..a947b94e 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-5.pt'
+DATA_FILE = 'dqn-cnn-agent-model-6.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -252,7 +252,7 @@ print("--------------------")
 
 # Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.8, buffer_capacity=2000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.85, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -278,10 +278,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v6'
+        self.writer_path = 'runs/dqn-cnn-agent-v7'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 250000
+        self.training_buffer_requirement = 50000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -292,9 +292,9 @@ class DQNModel(nn.Module):
             4: 0.75,  # 'attackunit' - Highest reward, attack visible units
             5: 0.5,   # 'buildscv'
             6: 0.5,   # 'resetcamera' - High reward as is required when available
-            7: 0.1,   # 'mcselfexpansion' - checks our expansion
+            7: 0.05,   # 'mcselfexpansion' - checks our expansion
             8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
-            9: 0.1,   # 'mcenemyprimary' - checks enemy primary base
+            9: 0.25,   # 'mcenemyprimary' - checks enemy primary base
             10: 0,    # 'attack_4_4'
             11: 0,    # 'attack_12_4'
             12: 0,    # 'attack_4_12'
@@ -642,10 +642,10 @@ class DQNModel(nn.Module):
         penalty_home_expansion = -0.25
 
         # Define which actions correspond to which quadrants
-        top_left_actions = [7, 8, 9, 10]
-        top_right_actions = [11, 12, 13, 14]
-        bottom_left_actions = [15, 16, 17, 18]
-        bottom_right_actions = [19, 20, 21, 22]
+        top_left_actions = [10, 11, 12, 13]
+        top_right_actions = [14, 15, 16, 17]
+        bottom_left_actions = [18, 19, 20, 21]
+        bottom_right_actions = [22, 23, 24, 25]
 
         # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
         # Also de-incentivize attacking home base & expansion
@@ -1099,7 +1099,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10 or barracks_count < 3:
+            if self.camera_move_timer < 5 or barracks_count < 3:
                 # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)

commit d250da823a8202970666e12334f594b7cf097ad9
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 27 08:08:00 2023 -0400

    fixed a few bugs with attacking

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 3c308d5b..c2f5ff25 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -977,15 +977,17 @@ class DQNAgent(base_agent.BaseAgent):
         # BEGIN CUSTOM CODE
         # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
+        # print("army_supply is set to: ", army_supply)
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
 
+        # To do - prioritize enemy units over structures...
         enemy_terran_structures = [
             unit for unit in obs.observation.feature_units
             if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
         ]
-        if len(enemy_terran_structures) > 0:
-            print("Enemy structures are set to: ", enemy_terran_structures)
+        # if len(enemy_terran_structures) > 0:
+        #     print("Enemy structures are set to: ", enemy_terran_structures)
 
         self_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
@@ -1082,24 +1084,23 @@ class DQNAgent(base_agent.BaseAgent):
 
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
-            # print("Length of enemy_units is: ",len(enemy_units))
-            # print(enemy_units)
-            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 1):
+            # print("Before our conditional check on attacking, enemy unit length is: ", len(enemy_units), "our timer is: ", self.unit_attack_delay_timer, " and our supply is: ", army_supply)
+            # print("This evalutes to: ", (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 8))
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 8):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
             # SCV Checks
             if worker_supply > 15 or self.scv_delay_timer < 7:
                 excluded_actions.append(5)
-            # END OF CUSTOM
 
             # Camera reset handling
             if self.camera_reset_required <= 0 and self.camera_move_timer < 6:
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10 and army_supply < 2:
-                print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
+            if self.camera_move_timer < 10 or barracks_count < 3:
+                # print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
                 excluded_actions.append(9)
@@ -1109,9 +1110,9 @@ class DQNAgent(base_agent.BaseAgent):
             # Post-bootstrap, it may be possible to relax this
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
             if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 50):
-                # Add actions from 7-23 to excluded_actions if they are not present
+                # Add actions from 10-25 to excluded_actions if they are not present
                 excluded_actions.extend(x for x in range(
-                    8, 23) if x not in excluded_actions)
+                    10, 26) if x not in excluded_actions)
                 # if 4 not in excluded_actions:
                 #     print("Bypassing attack actions as we can attack a unit directly")
 

commit 9d5d043455e4e7bf4ef018cbe706d388af050c49
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 27 07:37:45 2023 -0400

    added 3 distinct camera move actions

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 3b0aee9a..3c308d5b 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -4,7 +4,7 @@
 # All of the RL algorithms were implemented by me
 
 # Train the agent against a easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
@@ -147,7 +147,9 @@ ACTION_ATTACK = 'attack'
 ACTION_BUILD_SCV = 'buildscv'
 ACTION_ATTACK_UNIT = 'attackunit'
 ACTION_RESET_CAMERA = 'resetcamera'
-ACTION_MOVE_CAMERA = 'movecamera'
+ACTION_MOVE_CAMERA_SELF_EXPANSION = 'mcselfexpansion'
+ACTION_MOVE_CAMERA_ENEMY_EXPANSION = 'mcenemyexpansion'
+ACTION_MOVE_CAMERA_ENEMY_PRIMARY = 'mcenemyprimary'
 # END OF CUSTOM
 
 smart_actions = [
@@ -158,7 +160,9 @@ smart_actions = [
     ACTION_ATTACK_UNIT,
     ACTION_BUILD_SCV,
     ACTION_RESET_CAMERA,
-    ACTION_MOVE_CAMERA,
+    ACTION_MOVE_CAMERA_SELF_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_EXPANSION,
+    ACTION_MOVE_CAMERA_ENEMY_PRIMARY,
 ]
 
 # DQN State size
@@ -220,26 +224,29 @@ print("--------------------")
 # 4: 'attackunit'
 # 5: 'buildscv'
 # 6: 'resetcamera'
-# 7: 'movecamera'
-# 8: 'attack_4_4'
-# 9: 'attack_12_4'
-# 10: 'attack_4_12'
-# 11: 'attack_12_12'
-# 12: 'attack_36_4'
-# 13: 'attack_44_4'
-# 14: 'attack_36_12'
-# 15: 'attack_44_12'
-# 16: 'attack_4_36'
-# 17: 'attack_12_36'
-# 18: 'attack_4_44'
-# 19: 'attack_12_44'
-# 20: 'attack_36_36'
-# 21: 'attack_44_36'
-# 22: 'attack_36_44'
-# 23: 'attack_44_44'
+# 7: 'mcselfexpansion'
+# 8: 'mcenemyexpansion'
+# 9: 'mcenemyprimary'
+# 10: 'attack_4_4'
+# 11: 'attack_12_4'
+# 12: 'attack_4_12'
+# 13: 'attack_12_12'
+# 14: 'attack_36_4'
+# 15: 'attack_44_4'
+# 16: 'attack_36_12'
+# 17: 'attack_44_12'
+# 18: 'attack_4_36'
+# 19: 'attack_12_36'
+# 20: 'attack_4_44'
+# 21: 'attack_12_44'
+# 22: 'attack_36_36'
+# 23: 'attack_44_36'
+# 24: 'attack_36_44'
+# 25: 'attack_44_44'
 # --------------------
 
 
+
 ################################## End of BoilerPlate Code #####################################################
 
 
@@ -285,24 +292,25 @@ class DQNModel(nn.Module):
             4: 0.75,  # 'attackunit' - Highest reward, attack visible units
             5: 0.5,   # 'buildscv'
             6: 0.5,   # 'resetcamera' - High reward as is required when available
-            # 'movecamera' - we can't identify enemy units without moving the camera to inspect them.
-            7: 0.1,
-            8: 0,     # 'attack_4_4'
-            9: 0,     # 'attack_12_4'
-            10: 0,    # 'attack_4_12'
-            11: 0,    # 'attack_12_12'
-            12: 0,    # 'attack_36_4'
-            13: 0,    # 'attack_44_4'
-            14: 0,    # 'attack_36_12'
-            15: 0,    # 'attack_44_12'
-            16: 0,    # 'attack_4_36'
-            17: 0,    # 'attack_12_36'
-            18: 0,    # 'attack_4_44'
-            19: 0,    # 'attack_12_44'
-            20: 0,    # 'attack_36_36'
-            21: 0,    # 'attack_44_36'
-            22: 0,    # 'attack_36_44'
-            23: 0     # 'attack_44_44'
+            7: 0.1,   # 'mcselfexpansion' - checks our expansion
+            8: 0.1,   # 'mcenemyexpansion' - checks enemy expansion
+            9: 0.1,   # 'mcenemyprimary' - checks enemy primary base
+            10: 0,    # 'attack_4_4'
+            11: 0,    # 'attack_12_4'
+            12: 0,    # 'attack_4_12'
+            13: 0,    # 'attack_12_12'
+            14: 0,    # 'attack_36_4'
+            15: 0,    # 'attack_44_4'
+            16: 0,    # 'attack_36_12'
+            17: 0,    # 'attack_44_12'
+            18: 0,    # 'attack_4_36'
+            19: 0,    # 'attack_12_36'
+            20: 0,    # 'attack_4_44'
+            21: 0,    # 'attack_12_44'
+            22: 0,    # 'attack_36_36'
+            23: 0,    # 'attack_44_36'
+            24: 0,    # 'attack_36_44'
+            25: 0     # 'attack_44_44'
         }
 
         # Attempt GPU acceleration
@@ -596,30 +604,36 @@ class DQNModel(nn.Module):
         # Normalize the score to be between 0 and 1
         normalized_reward = min(score / max_score, 1)
 
-        # # Action Mapping Reference
-        # 0: 0,  # 'donothing'
-        # 1: 0.25,  # 'buildsupplydepot'
-        # 2: 0.5,  # 'buildbarracks'
-        # 3: 0.25,  # 'buildmarine'
-        # 4: 0.5,  # 'attackunit'
-        # 5: 0.5,  # 'buildscv'
-        # 6: 0,  # 'resetcamera'
-        # 7: 0,  # 'attack_4_4'
-        # 8: 0,  # 'attack_12_4'
-        # 9: 0,  # 'attack_4_12'
-        # 10: 0,  # 'attack_12_12'
-        # 11: 0,  # 'attack_36_4'
-        # 12: 0,  # 'attack_44_4'
-        # 13: 0,  # 'attack_36_12'
-        # 14: 0,  # 'attack_44_12'
-        # 15: 0,  # 'attack_4_36'
-        # 16: 0,  # 'attack_12_36'
-        # 17: 0,  # 'attack_4_44'
-        # 18: 0,  # 'attack_12_44'
-        # 19: 0,  # 'attack_36_36'
-        # 20: 0,  # 'attack_44_36'
-        # 21: 0,  # 'attack_36_44'
-        # 22: 0,  # 'attack_44_44'
+        # --------------------
+        # Action Mapping Reference
+        # 0: 'donothing'
+        # 1: 'buildsupplydepot'
+        # 2: 'buildbarracks'
+        # 3: 'buildmarine'
+        # 4: 'attackunit'
+        # 5: 'buildscv'
+        # 6: 'resetcamera'
+        # 7: 'mcselfexpansion'
+        # 8: 'mcenemyexpansion'
+        # 9: 'mcenemyprimary'
+        # 10: 'attack_4_4'
+        # 11: 'attack_12_4'
+        # 12: 'attack_4_12'
+        # 13: 'attack_12_12'
+        # 14: 'attack_36_4'
+        # 15: 'attack_44_4'
+        # 16: 'attack_36_12'
+        # 17: 'attack_44_12'
+        # 18: 'attack_4_36'
+        # 19: 'attack_12_36'
+        # 20: 'attack_4_44'
+        # 21: 'attack_12_44'
+        # 22: 'attack_36_36'
+        # 23: 'attack_44_36'
+        # 24: 'attack_36_44'
+        # 25: 'attack_44_44'
+        # --------------------
+
 
         # Incentive multipliers for attack logic
         attack_opposite_quadrant = 0.5
@@ -1031,25 +1045,28 @@ class DQNAgent(base_agent.BaseAgent):
             # 4: 'attackunit'
             # 5: 'buildscv'
             # 6: 'resetcamera'
-            # 7: 'movecamera'
-            # 8: 'attack_4_4'
-            # 9: 'attack_12_4'
-            # 10: 'attack_4_12'
-            # 11: 'attack_12_12'
-            # 12: 'attack_36_4'
-            # 13: 'attack_44_4'
-            # 14: 'attack_36_12'
-            # 15: 'attack_44_12'
-            # 16: 'attack_4_36'
-            # 17: 'attack_12_36'
-            # 18: 'attack_4_44'
-            # 19: 'attack_12_44'
-            # 20: 'attack_36_36'
-            # 21: 'attack_44_36'
-            # 22: 'attack_36_44'
-            # 23: 'attack_44_44'
+            # 7: 'mcselfexpansion'
+            # 8: 'mcenemyexpansion'
+            # 9: 'mcenemyprimary'
+            # 10: 'attack_4_4'
+            # 11: 'attack_12_4'
+            # 12: 'attack_4_12'
+            # 13: 'attack_12_12'
+            # 14: 'attack_36_4'
+            # 15: 'attack_44_4'
+            # 16: 'attack_36_12'
+            # 17: 'attack_44_12'
+            # 18: 'attack_4_36'
+            # 19: 'attack_12_36'
+            # 20: 'attack_4_44'
+            # 21: 'attack_12_44'
+            # 22: 'attack_36_36'
+            # 23: 'attack_44_36'
+            # 24: 'attack_36_44'
+            # 25: 'attack_44_44'
             # --------------------
 
+
             # Modified, self-generated code to scale supply depot creation
             # Includes sleep timer so bot doesn't build them in a loop
             if supply_free > 6 or self.supply_delay_timer < 30:
@@ -1077,12 +1094,15 @@ class DQNAgent(base_agent.BaseAgent):
             # END OF CUSTOM
 
             # Camera reset handling
-            if self.camera_reset_required <= 0:
+            if self.camera_reset_required <= 0 and self.camera_move_timer < 6:
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10 and barracks_count > 3:
+            if self.camera_move_timer < 10 and army_supply < 2:
+                print("Not moving camera, timer is: ", self.camera_move_timer, "and army_supply is: ", army_supply)
                 excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
 
             # modified original logic, waits for 8 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
@@ -1163,8 +1183,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Coordinate flipping isn't ideal for camera movement unfortunately...
             elif smart_action == ACTION_RESET_CAMERA:
                 self.camera_reset_required = 0
-                print("Camera counter is at: ",
-                      self.camera_reset_required, "resetting camera")
+                # print("Camera counter is at: ",
+                #       self.camera_reset_required, "resetting camera")
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1174,35 +1194,47 @@ class DQNAgent(base_agent.BaseAgent):
                     return actions.FUNCTIONS.move_camera((43, 51))
 
             # Camera move logic - need to move the camera to 'see' enemy units to attack them directly
-            elif smart_action == ACTION_MOVE_CAMERA:
+            # Self expansion
+            # Enemy Primary
+            # Enemy expansion
+
+            # Move camera to our expansion
+            elif smart_action == ACTION_MOVE_CAMERA_SELF_EXPANSION:
                 # reset our counter so we go back to home base eventually
                 self.camera_reset_required += 1
-                
-                # If spawned in the top left quadrant
+                self.camera_move_timer = 0
+
                 if self.base_top_left:
-                    
-                    # Randomly choose between opposite quadrant and adjacent opposite quadrant
-                    choice = random.choice(['opposite', 'adjacent_opposite'])
-                    
-                    if choice == 'opposite':
-                        # Move to Bottom right quadrant center (Opposite X-wise)
-                        return actions.FUNCTIONS.move_camera((43, 18))
-                    else:
-                        # Move to Bottom left quadrant center (Same Y, opposite X-wise)
-                        return actions.FUNCTIONS.move_camera((22, 18))
-                
-                # If spawned in the bottom right quadrant
+                    # Top right quadrant center 
+                    return actions.FUNCTIONS.move_camera((43, 18))
                 else:
-                    
-                    # Randomly choose between opposite quadrant and adjacent opposite quadrant
-                    choice = random.choice(['opposite', 'adjacent_opposite'])
-                    
-                    if choice == 'opposite':
-                        # Move to Top left quadrant center (Opposite X-wise)
-                        return actions.FUNCTIONS.move_camera((22, 51))
-                    else:
-                        # Move to Top right quadrant center (Same Y, opposite X-wise)
-                        return actions.FUNCTIONS.move_camera((43, 51))
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+
+            # Move camera to enemy's expansion
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_EXPANSION:
+                # reset our counter so we go back to home base eventually
+                self.camera_reset_required += 1
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom left quadrant center
+                    return actions.FUNCTIONS.move_camera((22, 51))
+                else:
+                    # Top right quadrant center
+                    return actions.FUNCTIONS.move_camera((43, 18))
+
+            # Move camera to enemy's primary base
+            elif smart_action == ACTION_MOVE_CAMERA_ENEMY_PRIMARY:
+                # reset our counter so we go back to home base eventually
+                self.camera_reset_required += 1
+                self.camera_move_timer = 0
+                if self.base_top_left:
+                    # Bottom right quadrant center 
+                    return actions.FUNCTIONS.move_camera((43, 51))
+                else:
+                    # Top left quadrant center 
+                    return actions.FUNCTIONS.move_camera((22, 18))
+
 
 
         elif self.move_number == 1:

commit e9375b94cd52ea0af4608ca74a498d983f63e122
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Aug 26 14:04:37 2023 -0400

    further work on camera movement - not perfect but getting better

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 286a7d91..3b0aee9a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-4.pt'
+DATA_FILE = 'dqn-cnn-agent-model-5.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -104,6 +104,39 @@ TERRAN_BUILDINGS = [
     58,  # Planetary Fortress
 ]
 
+TERRAN_UNITS = {
+    # Worker
+    45: "SCV",
+
+    # Basic
+    48: "MULE",
+    51: "Marine",
+    53: "Marauder",
+    54: "Reaper",
+    55: "Ghost",
+
+    # Factory
+    57: "Hellion",
+    58: "SiegeTank",
+    59: "Cyclone",
+    62: "Thor",
+    64: "Hellbat",
+
+    # Starport
+    35: "VikingFighter",
+    36: "VikingAssault",
+    67: "Medivac",
+    68: "Banshee",
+    69: "Raven",
+    70: "Battlecruiser",
+    132: "Liberator",
+    71: "AutoTurret",  # Spawned by Raven
+
+    # Other
+    102: "WidowMine",
+    488: "LiberatorAG"
+}
+
 
 ACTION_DO_NOTHING = 'donothing'
 ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
@@ -138,7 +171,7 @@ quadrants = [
     (0, 32),      # Bottom left quadrant
     (32, 32)      # Bottom right quadrant
 ]
-#######################3
+# 3
 # Steven Brown's implementation for spawn-location-agnostic quadrant attacks
 # for mm_x in range(0, 64):
 #     for mm_y in range(0, 64):
@@ -147,8 +180,10 @@ quadrants = [
 #                                  str(mm_x - 16) + '_' + str(mm_y - 16))
 ################
 
-# Calculate the offset points for each quadrant 
+# Calculate the offset points for each quadrant
 # This is used in the smart_attack functions (16 locations with offsets)
+
+
 def calculate_quadrant_points(top_left_x, top_left_y):
     offset = 4
     midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
@@ -156,8 +191,11 @@ def calculate_quadrant_points(top_left_x, top_left_y):
         (top_left_x + offset, top_left_y + offset),
         (top_left_x + midpoint_offset - offset, top_left_y + offset),
         (top_left_x + offset, top_left_y + midpoint_offset - offset),
-        (top_left_x + midpoint_offset - offset, top_left_y + midpoint_offset - offset)
+        (top_left_x + midpoint_offset - offset,
+         top_left_y + midpoint_offset - offset)
     ]
+
+
 # For each quadrant, calculate the offset points and append the attack action
 for quad in quadrants:
     points = calculate_quadrant_points(*quad)
@@ -247,7 +285,8 @@ class DQNModel(nn.Module):
             4: 0.75,  # 'attackunit' - Highest reward, attack visible units
             5: 0.5,   # 'buildscv'
             6: 0.5,   # 'resetcamera' - High reward as is required when available
-            7: 0.1,   # 'movecamera' - we can't identify enemy units without moving the camera to inspect them.
+            # 'movecamera' - we can't identify enemy units without moving the camera to inspect them.
+            7: 0.1,
             8: 0,     # 'attack_4_4'
             9: 0,     # 'attack_12_4'
             10: 0,    # 'attack_4_12'
@@ -264,19 +303,20 @@ class DQNModel(nn.Module):
             21: 0,    # 'attack_44_36'
             22: 0,    # 'attack_36_44'
             23: 0     # 'attack_44_44'
-                }
+        }
 
         # Attempt GPU acceleration
         self.device = torch.device(
             "cuda" if torch.cuda.is_available() else "cpu")
 
-
         # CNN for RGB minimap
         self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),
+            nn.Conv2d(in_channels=3, out_channels=32,
+                      kernel_size=3, stride=1, padding=1),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
-            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
+            nn.Conv2d(in_channels=32, out_channels=64,
+                      kernel_size=3, stride=1, padding=1),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
         ).to(self.device)  # Moving to GPU if available
@@ -309,15 +349,15 @@ class DQNModel(nn.Module):
         # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
         self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
 
-    # 
+    #
     def _get_conv_out(self, shape):
         o = self.conv(torch.zeros(1, *shape).to(self.device))
         return int(np.prod(o.size()))
-    
+
     def forward(self, non_spatial_data, minimap):
         non_spatial_data = non_spatial_data.to(self.device)
         minimap = minimap.to(self.device)
-        
+
         # Separate treatments for different types of data
         conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
         fc_out = self.fc_non_spatial(non_spatial_data)
@@ -389,15 +429,17 @@ class DQNModel(nn.Module):
         rgb_minimap = current_state["rgb_minimap"]
 
         # Convert data to tensors and move them to the GPU
-        non_spatial_data_tensor = torch.tensor(non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        non_spatial_data_tensor = torch.tensor(
+            non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
         # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
         # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
-        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.device)
-
+        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(
+            2, 0, 1).unsqueeze(0).to(self.device)
 
-        # Epsilon-based exploration 
+        # Epsilon-based exploration
         if np.random.uniform() < self.epsilon:
-            available_actions = [a for a in self.actions if a not in excluded_actions]
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
 
             # This is where we keep the logic for epsilon decay (linear)
@@ -414,7 +456,8 @@ class DQNModel(nn.Module):
             non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
 
             # Concatenate the two outputs
-            combined_output = torch.cat((conv_output, non_spatial_output), dim=1)
+            combined_output = torch.cat(
+                (conv_output, non_spatial_output), dim=1)
 
             # Finally, pass through the decision-making layers
             q_values = self.fc_decision(combined_output)
@@ -448,25 +491,32 @@ class DQNModel(nn.Module):
         non_spatial_states = [state["non_spatial"] for state in states]
         rgb_minimap_states = [state["rgb_minimap"] for state in states]
 
-
-        non_spatial_next_states = [state["non_spatial"] for state in next_states]
-        rgb_minimap_next_states = [state["rgb_minimap"] for state in next_states]
+        non_spatial_next_states = [state["non_spatial"]
+                                   for state in next_states]
+        rgb_minimap_next_states = [state["rgb_minimap"]
+                                   for state in next_states]
 
         # Convert the zipped values to numpy arrays
         non_spatial_states_np = np.array(non_spatial_states, dtype=np.float32)
         rgb_minimap_states_np = np.array(rgb_minimap_states, dtype=np.float32)
-        non_spatial_next_states_np = np.array(non_spatial_next_states, dtype=np.float32)
-        rgb_minimap_next_states_np = np.array(rgb_minimap_next_states, dtype=np.float32)
+        non_spatial_next_states_np = np.array(
+            non_spatial_next_states, dtype=np.float32)
+        rgb_minimap_next_states_np = np.array(
+            rgb_minimap_next_states, dtype=np.float32)
         actions_np = np.array(actions, dtype=np.int64)
         rewards_np = np.array(rewards, dtype=np.float32)
 
         # Convert numpy arrays to tensors
-        non_spatial_states = torch.tensor(non_spatial_states_np).to(self.device)
+        non_spatial_states = torch.tensor(
+            non_spatial_states_np).to(self.device)
         # Minimap requires permutation to meet PyTorch expectations
-        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(self.device).permute(0, 3, 1, 2)
-        non_spatial_next_states = torch.tensor(non_spatial_next_states_np).to(self.device)
+        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(
+            self.device).permute(0, 3, 1, 2)
+        non_spatial_next_states = torch.tensor(
+            non_spatial_next_states_np).to(self.device)
         # Minimap requires permutation to meet PyTorch expectations
-        rgb_minimap_next_states = torch.tensor(rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2) 
+        rgb_minimap_next_states = torch.tensor(
+            rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2)
 
         actions = torch.tensor(actions_np).to(self.device)
         rewards = torch.tensor(rewards_np).to(self.device)
@@ -478,7 +528,8 @@ class DQNModel(nn.Module):
             q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
 
             # Compute the target Q-values
-            next_q_values = self(non_spatial_next_states, rgb_minimap_next_states)
+            next_q_values = self(non_spatial_next_states,
+                                 rgb_minimap_next_states)
             max_next_q_values = next_q_values.max(1)[0]
             q_target = rewards + self.gamma * max_next_q_values
 
@@ -502,19 +553,23 @@ class DQNModel(nn.Module):
 
         # Logging various metrics for visualization and debugging
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
+            writer.add_scalar('Loss/train', loss.item(),
+                              self.global_training_steps)
 
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
+            writer.add_scalar('Epsilon/value', self.epsilon,
+                              self.global_training_steps)
 
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            writer.add_histogram(
+                'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
 
         # Log the histograms of model weights every 100 iterations
         if self.global_training_steps % 100 == 0:
             with SummaryWriter(self.writer_path) as writer:
                 for name, param in self.named_parameters():
-                    writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
+                    writer.add_histogram(name, param.clone().cpu(
+                    ).data.numpy(), self.global_training_steps)
 
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint weights
@@ -542,29 +597,29 @@ class DQNModel(nn.Module):
         normalized_reward = min(score / max_score, 1)
 
         # # Action Mapping Reference
-            # 0: 0,  # 'donothing'
-            # 1: 0.25,  # 'buildsupplydepot'
-            # 2: 0.5,  # 'buildbarracks'
-            # 3: 0.25,  # 'buildmarine'
-            # 4: 0.5,  # 'attackunit'
-            # 5: 0.5,  # 'buildscv'
-            # 6: 0,  # 'resetcamera'
-            # 7: 0,  # 'attack_4_4'
-            # 8: 0,  # 'attack_12_4'
-            # 9: 0,  # 'attack_4_12'
-            # 10: 0,  # 'attack_12_12'
-            # 11: 0,  # 'attack_36_4'
-            # 12: 0,  # 'attack_44_4'
-            # 13: 0,  # 'attack_36_12'
-            # 14: 0,  # 'attack_44_12'
-            # 15: 0,  # 'attack_4_36'
-            # 16: 0,  # 'attack_12_36'
-            # 17: 0,  # 'attack_4_44'
-            # 18: 0,  # 'attack_12_44'
-            # 19: 0,  # 'attack_36_36'
-            # 20: 0,  # 'attack_44_36'
-            # 21: 0,  # 'attack_36_44'
-            # 22: 0,  # 'attack_44_44'
+        # 0: 0,  # 'donothing'
+        # 1: 0.25,  # 'buildsupplydepot'
+        # 2: 0.5,  # 'buildbarracks'
+        # 3: 0.25,  # 'buildmarine'
+        # 4: 0.5,  # 'attackunit'
+        # 5: 0.5,  # 'buildscv'
+        # 6: 0,  # 'resetcamera'
+        # 7: 0,  # 'attack_4_4'
+        # 8: 0,  # 'attack_12_4'
+        # 9: 0,  # 'attack_4_12'
+        # 10: 0,  # 'attack_12_12'
+        # 11: 0,  # 'attack_36_4'
+        # 12: 0,  # 'attack_44_4'
+        # 13: 0,  # 'attack_36_12'
+        # 14: 0,  # 'attack_44_12'
+        # 15: 0,  # 'attack_4_36'
+        # 16: 0,  # 'attack_12_36'
+        # 17: 0,  # 'attack_4_44'
+        # 18: 0,  # 'attack_12_44'
+        # 19: 0,  # 'attack_36_36'
+        # 20: 0,  # 'attack_44_36'
+        # 21: 0,  # 'attack_36_44'
+        # 22: 0,  # 'attack_44_44'
 
         # Incentive multipliers for attack logic
         attack_opposite_quadrant = 0.5
@@ -578,7 +633,6 @@ class DQNModel(nn.Module):
         bottom_left_actions = [15, 16, 17, 18]
         bottom_right_actions = [19, 20, 21, 22]
 
-
         # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
         # Also de-incentivize attacking home base & expansion
         if base_top_left:
@@ -607,7 +661,6 @@ class DQNModel(nn.Module):
             elif previous_action in bottom_left_actions:
                 normalized_reward += penalty_home_expansion
 
-
         # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
         action_reward = self.action_rewards.get(previous_action, 0)
         # Add the action reward, ensuring the total reward stays between 0 and 1
@@ -633,7 +686,8 @@ class DQNModel(nn.Module):
             if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
                 return command_center.x, command_center.y
             else:
-                print(f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                print(
+                    f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
                 return None, None
         else:
             print("Command Center not found.")
@@ -654,10 +708,8 @@ class DQNModel(nn.Module):
         else:
             # Leave the minimap as it is
             transformed_minimap = minimap_data
-            
-        return transformed_minimap
-    
 
+        return transformed_minimap
 
 
 # Agent Implementation
@@ -674,7 +726,7 @@ class DQNAgent(base_agent.BaseAgent):
         print("State Size:", STATE_SIZE)
 
         self.dqn_model = DQNModel(
-            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3,64,64))
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3, 64, 64))
 
         self.previous_action = None
         self.previous_state = {
@@ -682,9 +734,6 @@ class DQNAgent(base_agent.BaseAgent):
             "rgb_minimap": None
         }
 
-        
-
-
         self.cc_y = None
         self.cc_x = None
 
@@ -705,7 +754,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.scv_delay_timer = 0
         self.camera_move_timer = 0
         # Reset camera once to help agent place buildings outside of the mineral patch
-        self.camera_reset_required  = 1 
+        self.camera_reset_required = 1
 
         self.command_center = []
 
@@ -760,11 +809,10 @@ class DQNAgent(base_agent.BaseAgent):
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_move_timer += 1
-        
+
         # Visuals
         rgb_minimap = obs.observation["rgb_minimap"]
 
-
         # Check our current score, just for debugging
         # print("Current score is: ", self.get_normalized_reward(obs))
 
@@ -811,7 +859,7 @@ class DQNAgent(base_agent.BaseAgent):
             print("Number of steps were: ", episode_steps)
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
                 print("Number of in-game model updates: ",
-                    self.in_game_training_iterations)
+                      self.in_game_training_iterations)
             # Backpropagate the final reward to previous actions
             print("Backpropagating reward updates across",
                   self.actual_root_level_steps_taken, "root level actions.")
@@ -876,7 +924,6 @@ class DQNAgent(base_agent.BaseAgent):
 
         # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
 
-
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # supply_depot_count = int(round(len(depot_y) / 69))
         all_supply_depots = self.dqn_model.get_units_by_type(
@@ -891,9 +938,11 @@ class DQNAgent(base_agent.BaseAgent):
             obs, units.Terran.Barracks)
         barracks_count = len(
             [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
-        
-        all_command_centers = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
-        command_center_count = len([unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
+
+        all_command_centers = self.dqn_model.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+        command_center_count = len(
+            [unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
 
         # Find our command centers:
         # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
@@ -916,9 +965,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
-        
+
         enemy_terran_structures = [
-            unit for unit in obs.observation.feature_units 
+            unit for unit in obs.observation.feature_units
             if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
         ]
         if len(enemy_terran_structures) > 0:
@@ -933,7 +982,6 @@ class DQNAgent(base_agent.BaseAgent):
         if self.move_number == 0:
             self.move_number += 1
 
-
             current_state = {
                 "non_spatial": np.zeros(7),
                 "rgb_minimap": None
@@ -946,8 +994,10 @@ class DQNAgent(base_agent.BaseAgent):
             # Custom State Add-on
             current_state["non_spatial"][4] = len(self_units)
             current_state["non_spatial"][5] = len(enemy_units)
-            current_state["non_spatial"][6] = self.base_top_left # spawn location boolean seems critical for learning
-            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(rgb_minimap, self.base_top_left)
+            # spawn location boolean seems critical for learning
+            current_state["non_spatial"][6] = self.base_top_left
+            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(
+                rgb_minimap, self.base_top_left)
             # current_state["rgb_minimap"] = rgb_minimap
 
             # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
@@ -1027,11 +1077,11 @@ class DQNAgent(base_agent.BaseAgent):
             # END OF CUSTOM
 
             # Camera reset handling
-            if self.camera_reset_required  <= 0:
+            if self.camera_reset_required <= 0:
                 excluded_actions.append(6)
 
             # Camera move handling
-            if self.camera_move_timer < 10:
+            if self.camera_move_timer < 10 and barracks_count > 3:
                 excluded_actions.append(7)
 
             # modified original logic, waits for 8 marines before attacking
@@ -1040,7 +1090,8 @@ class DQNAgent(base_agent.BaseAgent):
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
             if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 50):
                 # Add actions from 7-23 to excluded_actions if they are not present
-                excluded_actions.extend(x for x in range(8, 23) if x not in excluded_actions)
+                excluded_actions.extend(x for x in range(
+                    8, 23) if x not in excluded_actions)
                 # if 4 not in excluded_actions:
                 #     print("Bypassing attack actions as we can attack a unit directly")
 
@@ -1065,7 +1116,7 @@ class DQNAgent(base_agent.BaseAgent):
                 if unit_y.any():
                     i = random.randint(0, len(unit_y) - 1)
                     target = [unit_x[i], unit_y[i]]
-                    
+
                     # Checks to avoid out-of-bounds crashing
                     # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
                     if 0 <= target[0] < 84 and 0 <= target[1] < 84:
@@ -1084,7 +1135,6 @@ class DQNAgent(base_agent.BaseAgent):
                     else:
                         print(f"Barracks coordinates out of range: {target}")
 
-
             elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
                 if _SELECT_ARMY in obs.observation['available_actions']:
                     return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
@@ -1100,7 +1150,6 @@ class DQNAgent(base_agent.BaseAgent):
                     target = safe_cc_x, safe_cc_y
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
-                
 
             # To Do:
             # If get_command_center_coordinates returns None, None
@@ -1113,8 +1162,9 @@ class DQNAgent(base_agent.BaseAgent):
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
             # Coordinate flipping isn't ideal for camera movement unfortunately...
             elif smart_action == ACTION_RESET_CAMERA:
-                self.camera_reset_required  -= 1
-                print("Camera counter is at: ", self.camera_reset_required, "resetting camera")
+                self.camera_reset_required = 0
+                print("Camera counter is at: ",
+                      self.camera_reset_required, "resetting camera")
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1123,6 +1173,38 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Spawned bottom right - moving camera")
                     return actions.FUNCTIONS.move_camera((43, 51))
 
+            # Camera move logic - need to move the camera to 'see' enemy units to attack them directly
+            elif smart_action == ACTION_MOVE_CAMERA:
+                # reset our counter so we go back to home base eventually
+                self.camera_reset_required += 1
+                
+                # If spawned in the top left quadrant
+                if self.base_top_left:
+                    
+                    # Randomly choose between opposite quadrant and adjacent opposite quadrant
+                    choice = random.choice(['opposite', 'adjacent_opposite'])
+                    
+                    if choice == 'opposite':
+                        # Move to Bottom right quadrant center (Opposite X-wise)
+                        return actions.FUNCTIONS.move_camera((43, 18))
+                    else:
+                        # Move to Bottom left quadrant center (Same Y, opposite X-wise)
+                        return actions.FUNCTIONS.move_camera((22, 18))
+                
+                # If spawned in the bottom right quadrant
+                else:
+                    
+                    # Randomly choose between opposite quadrant and adjacent opposite quadrant
+                    choice = random.choice(['opposite', 'adjacent_opposite'])
+                    
+                    if choice == 'opposite':
+                        # Move to Top left quadrant center (Opposite X-wise)
+                        return actions.FUNCTIONS.move_camera((22, 51))
+                    else:
+                        # Move to Top right quadrant center (Same Y, opposite X-wise)
+                        return actions.FUNCTIONS.move_camera((43, 51))
+
+
         elif self.move_number == 1:
             self.move_number += 1
 

commit 74abc532727cf3057c3cea0d5afadea48bc4bb8c
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Aug 26 13:49:16 2023 -0400

    partial camera move fix

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 35f4a2df..286a7d91 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -82,6 +82,28 @@ _NOT_QUEUED = [0]
 _QUEUED = [1]
 _SELECT_ALL = [2]
 
+# Custom library of all terran buildings
+TERRAN_BUILDINGS = [
+    18,  # Command Center
+    19,  # Supply Depot
+    20,  # Refinery
+    21,  # Barracks
+    22,  # Orbital Command
+    23,  # Factory
+    24,  # Starport
+    25,  # Engineering Bay
+    26,  # Fusion Core
+    27,  # Tech Lab (Barracks)
+    28,  # Tech Lab (Factory)
+    29,  # Tech Lab (Starport)
+    30,  # Reactor (generic, as the building morphs)
+    37,  # Sensor Tower
+    38,  # Bunker
+    39,  # Missile Turret
+    40,  # Auto-turret (from Raven)
+    58,  # Planetary Fortress
+]
+
 
 ACTION_DO_NOTHING = 'donothing'
 ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
@@ -92,6 +114,7 @@ ACTION_ATTACK = 'attack'
 ACTION_BUILD_SCV = 'buildscv'
 ACTION_ATTACK_UNIT = 'attackunit'
 ACTION_RESET_CAMERA = 'resetcamera'
+ACTION_MOVE_CAMERA = 'movecamera'
 # END OF CUSTOM
 
 smart_actions = [
@@ -102,6 +125,7 @@ smart_actions = [
     ACTION_ATTACK_UNIT,
     ACTION_BUILD_SCV,
     ACTION_RESET_CAMERA,
+    ACTION_MOVE_CAMERA,
 ]
 
 # DQN State size
@@ -150,7 +174,7 @@ for index, action in enumerate(smart_actions):
 print("--------------------")
 
 # --------------------
-# Action Mapping
+# Action Mapping Reference
 # 0: 'donothing'
 # 1: 'buildsupplydepot'
 # 2: 'buildbarracks'
@@ -158,22 +182,23 @@ print("--------------------")
 # 4: 'attackunit'
 # 5: 'buildscv'
 # 6: 'resetcamera'
-# 7: 'attack_4_4'
-# 8: 'attack_12_4'
-# 9: 'attack_4_12'
-# 10: 'attack_12_12'
-# 11: 'attack_36_4'
-# 12: 'attack_44_4'
-# 13: 'attack_36_12'
-# 14: 'attack_44_12'
-# 15: 'attack_4_36'
-# 16: 'attack_12_36'
-# 17: 'attack_4_44'
-# 18: 'attack_12_44'
-# 19: 'attack_36_36'
-# 20: 'attack_44_36'
-# 21: 'attack_36_44'
-# 22: 'attack_44_44'
+# 7: 'movecamera'
+# 8: 'attack_4_4'
+# 9: 'attack_12_4'
+# 10: 'attack_4_12'
+# 11: 'attack_12_12'
+# 12: 'attack_36_4'
+# 13: 'attack_44_4'
+# 14: 'attack_36_12'
+# 15: 'attack_44_12'
+# 16: 'attack_4_36'
+# 17: 'attack_12_36'
+# 18: 'attack_4_44'
+# 19: 'attack_12_44'
+# 20: 'attack_36_36'
+# 21: 'attack_44_36'
+# 22: 'attack_36_44'
+# 23: 'attack_44_44'
 # --------------------
 
 
@@ -208,38 +233,38 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v5'
+        self.writer_path = 'runs/dqn-cnn-agent-v6'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 50000
+        self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
             0: -0.5,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
             1: 0.25,  # 'buildsupplydepot'
-            2: 0.5,  # 'buildbarracks'
+            2: 0.5,   # 'buildbarracks'
             3: 0.25,  # 'buildmarine'
             4: 0.75,  # 'attackunit' - Highest reward, attack visible units
-            5: 0.5,  # 'buildscv'
-            6: 0.5,  # 'resetcamera' - High reward as is required when available
-            7: 0,  # 'attack_4_4'
-            8: 0,  # 'attack_12_4'
-            9: 0,  # 'attack_4_12'
-            10: 0,  # 'attack_12_12'
-            11: 0,  # 'attack_36_4'
-            12: 0,  # 'attack_44_4'
-            13: 0,  # 'attack_36_12'
-            14: 0,  # 'attack_44_12'
-            15: 0,  # 'attack_4_36'
-            16: 0,  # 'attack_12_36'
-            17: 0,  # 'attack_4_44'
-            18: 0,  # 'attack_12_44'
-            19: 0,  # 'attack_36_36'
-            20: 0,  # 'attack_44_36'
-            21: 0,  # 'attack_36_44'
-            22: 0,  # 'attack_44_44'
-        }
-
+            5: 0.5,   # 'buildscv'
+            6: 0.5,   # 'resetcamera' - High reward as is required when available
+            7: 0.1,   # 'movecamera' - we can't identify enemy units without moving the camera to inspect them.
+            8: 0,     # 'attack_4_4'
+            9: 0,     # 'attack_12_4'
+            10: 0,    # 'attack_4_12'
+            11: 0,    # 'attack_12_12'
+            12: 0,    # 'attack_36_4'
+            13: 0,    # 'attack_44_4'
+            14: 0,    # 'attack_36_12'
+            15: 0,    # 'attack_44_12'
+            16: 0,    # 'attack_4_36'
+            17: 0,    # 'attack_12_36'
+            18: 0,    # 'attack_4_44'
+            19: 0,    # 'attack_12_44'
+            20: 0,    # 'attack_36_36'
+            21: 0,    # 'attack_44_36'
+            22: 0,    # 'attack_36_44'
+            23: 0     # 'attack_44_44'
+                }
 
         # Attempt GPU acceleration
         self.device = torch.device(
@@ -678,6 +703,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
         self.scv_delay_timer = 0
+        self.camera_move_timer = 0
         # Reset camera once to help agent place buildings outside of the mineral patch
         self.camera_reset_required  = 1 
 
@@ -733,6 +759,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer += 1
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
+        self.camera_move_timer += 1
         
         # Visuals
         rgb_minimap = obs.observation["rgb_minimap"]
@@ -885,9 +912,18 @@ class DQNAgent(base_agent.BaseAgent):
         army_supply = obs.observation['player'][5]
         worker_supply = obs.observation['player'][6]
         # BEGIN CUSTOM CODE
+        # unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
         
+        enemy_terran_structures = [
+            unit for unit in obs.observation.feature_units 
+            if unit.unit_type in TERRAN_BUILDINGS and unit.alliance != features.PlayerRelative.SELF
+        ]
+        if len(enemy_terran_structures) > 0:
+            print("Enemy structures are set to: ", enemy_terran_structures)
+
         self_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
         # END CUSTOM CODE
@@ -945,22 +981,23 @@ class DQNAgent(base_agent.BaseAgent):
             # 4: 'attackunit'
             # 5: 'buildscv'
             # 6: 'resetcamera'
-            # 7: 'attack_4_4'
-            # 8: 'attack_12_4'
-            # 9: 'attack_4_12'
-            # 10: 'attack_12_12'
-            # 11: 'attack_36_4'
-            # 12: 'attack_44_4'
-            # 13: 'attack_36_12'
-            # 14: 'attack_44_12'
-            # 15: 'attack_4_36'
-            # 16: 'attack_12_36'
-            # 17: 'attack_4_44'
-            # 18: 'attack_12_44'
-            # 19: 'attack_36_36'
-            # 20: 'attack_44_36'
-            # 21: 'attack_36_44'
-            # 22: 'attack_44_44'
+            # 7: 'movecamera'
+            # 8: 'attack_4_4'
+            # 9: 'attack_12_4'
+            # 10: 'attack_4_12'
+            # 11: 'attack_12_12'
+            # 12: 'attack_36_4'
+            # 13: 'attack_44_4'
+            # 14: 'attack_36_12'
+            # 15: 'attack_44_12'
+            # 16: 'attack_4_36'
+            # 17: 'attack_12_36'
+            # 18: 'attack_4_44'
+            # 19: 'attack_12_44'
+            # 20: 'attack_36_36'
+            # 21: 'attack_44_36'
+            # 22: 'attack_36_44'
+            # 23: 'attack_44_44'
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
@@ -993,17 +1030,21 @@ class DQNAgent(base_agent.BaseAgent):
             if self.camera_reset_required  <= 0:
                 excluded_actions.append(6)
 
+            # Camera move handling
+            if self.camera_move_timer < 10:
+                excluded_actions.append(7)
+
             # modified original logic, waits for 8 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax this
             # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
             if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 50):
                 # Add actions from 7-23 to excluded_actions if they are not present
-                excluded_actions.extend(x for x in range(7, 23) if x not in excluded_actions)
+                excluded_actions.extend(x for x in range(8, 23) if x not in excluded_actions)
                 # if 4 not in excluded_actions:
                 #     print("Bypassing attack actions as we can attack a unit directly")
 
-            print("Our exclusions are set to: ", excluded_actions)
+            # print("Our exclusions are set to: ", excluded_actions)
 
             # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(

commit 609291cccce0a1e013034351c1e76b3b40769efe
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Aug 26 12:32:27 2023 -0400

    optimized attack logic to ensure units are being attacked while visible

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 7095c2ed..35f4a2df 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -219,7 +219,7 @@ class DQNModel(nn.Module):
             1: 0.25,  # 'buildsupplydepot'
             2: 0.5,  # 'buildbarracks'
             3: 0.25,  # 'buildmarine'
-            4: 0.5,  # 'attackunit'
+            4: 0.75,  # 'attackunit' - Highest reward, attack visible units
             5: 0.5,  # 'buildscv'
             6: 0.5,  # 'resetcamera' - High reward as is required when available
             7: 0,  # 'attack_4_4'
@@ -678,7 +678,8 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
         self.scv_delay_timer = 0
-        self.camera_reset_required  = 0
+        # Reset camera once to help agent place buildings outside of the mineral patch
+        self.camera_reset_required  = 1 
 
         self.command_center = []
 
@@ -963,7 +964,8 @@ class DQNAgent(base_agent.BaseAgent):
             # --------------------
 
             # Modified, self-generated code to scale supply depot creation
-            if supply_free > 7 or self.supply_delay_timer < 25:
+            # Includes sleep timer so bot doesn't build them in a loop
+            if supply_free > 6 or self.supply_delay_timer < 30:
                 excluded_actions.append(1)
 
             if barracks_count > 4:
@@ -978,7 +980,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Length of enemy_units is: ",len(enemy_units))
             # print(enemy_units)
-            if len(enemy_units) == 0 or self.unit_attack_delay_timer < 4:
+            if (len(enemy_units) == 0) or (self.unit_attack_delay_timer < 4) or (army_supply < 1):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -994,11 +996,14 @@ class DQNAgent(base_agent.BaseAgent):
             # modified original logic, waits for 8 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax this
-            if army_supply < 10 or self.attack_delay_timer < 60:
-                excluded_actions.extend(range(7, 23))
-
-
-            # print("Our exclusions are set to: ", excluded_actions)
+            # Additionally, makes sure there are no visible units (action 4) that we can attack directly...
+            if 4 not in excluded_actions or (army_supply < 10 or self.attack_delay_timer < 50):
+                # Add actions from 7-23 to excluded_actions if they are not present
+                excluded_actions.extend(x for x in range(7, 23) if x not in excluded_actions)
+                # if 4 not in excluded_actions:
+                #     print("Bypassing attack actions as we can attack a unit directly")
+
+            print("Our exclusions are set to: ", excluded_actions)
 
             # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(

commit 48cdf9f547e13519ac0bbf6228acd01109a6699b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sat Aug 26 11:50:36 2023 -0400

    removed dropout, optimized camera reset, bumped version

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index bd50c254..7095c2ed 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -3,8 +3,8 @@
 # The RL Algorithm can properly interact with StarCraft II / PySC2
 # All of the RL algorithms were implemented by me
 
-# Train the agent against a medium Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
+# Train the agent against a easy Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty easy --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-3.pt'
+DATA_FILE = 'dqn-cnn-agent-model-4.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -182,7 +182,7 @@ print("--------------------")
 
 # Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=2000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.8, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -190,8 +190,8 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.05
-        # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.final_epsilon = 0.01
+        # We decay over 1M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
             self.epsilon - self.final_epsilon) / 1000000
         self.action_counter = 0
@@ -208,20 +208,20 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v3'
+        self.writer_path = 'runs/dqn-cnn-agent-v5'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 50000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
-            0: 0,  # 'donothing'
+            0: -0.5,  # 'donothing' - Need to negatively reinforce 'doing nothing' as it sees it as a 'safe' move in perpetuity
             1: 0.25,  # 'buildsupplydepot'
             2: 0.5,  # 'buildbarracks'
             3: 0.25,  # 'buildmarine'
             4: 0.5,  # 'attackunit'
             5: 0.5,  # 'buildscv'
-            6: 0,  # 'resetcamera'
+            6: 0.5,  # 'resetcamera' - High reward as is required when available
             7: 0,  # 'attack_4_4'
             8: 0,  # 'attack_12_4'
             9: 0,  # 'attack_4_12'
@@ -263,17 +263,17 @@ class DQNModel(nn.Module):
         self.fc_non_spatial = nn.Sequential(
             nn.Linear(non_spatial_state_size, 512),
             nn.ReLU(),
-            nn.Dropout(0.2)
+            # nn.Dropout(0.2)
         ).to(self.device)  # Moving to GPU if available
 
         # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
         self.fc_decision = nn.Sequential(
             nn.Linear(self.conv_out_size + 512, 1024),
             nn.ReLU(),
-            nn.Dropout(0.2),
+            # nn.Dropout(0.2),
             nn.Linear(1024, 2048),
             nn.ReLU(),
-            nn.Dropout(0.2),
+            # nn.Dropout(0.2),
             nn.Linear(2048, len(self.actions))
         ).to(self.device)  # Moving to GPU if available
 
@@ -678,7 +678,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
         self.scv_delay_timer = 0
-        self.camera_delay_timer = 0
+        self.camera_reset_required  = 0
 
         self.command_center = []
 
@@ -732,7 +732,6 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer += 1
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
-        self.camera_delay_timer += 1
         
         # Visuals
         rgb_minimap = obs.observation["rgb_minimap"]
@@ -989,13 +988,13 @@ class DQNAgent(base_agent.BaseAgent):
             # END OF CUSTOM
 
             # Camera reset handling
-            if self.camera_delay_timer < 10:
+            if self.camera_reset_required  <= 0:
                 excluded_actions.append(6)
 
             # modified original logic, waits for 8 marines before attacking
             # Excludes all minimap attack actions if we just issued one (at least...for now)
             # Post-bootstrap, it may be possible to relax this
-            if army_supply < 10 or self.attack_delay_timer < 35:
+            if army_supply < 10 or self.attack_delay_timer < 60:
                 excluded_actions.extend(range(7, 23))
 
 
@@ -1009,6 +1008,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.previous_action = rl_action
 
             # print("Our excluded actions for this step are: ", excluded_actions)
+            # print("The model chose: ", rl_action)
 
             # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)
@@ -1067,7 +1067,8 @@ class DQNAgent(base_agent.BaseAgent):
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
             # Coordinate flipping isn't ideal for camera movement unfortunately...
             elif smart_action == ACTION_RESET_CAMERA:
-                self.camera_delay_timer = 0
+                self.camera_reset_required  -= 1
+                print("Camera counter is at: ", self.camera_reset_required, "resetting camera")
 
                 if self.base_top_left:
                     # print("Spawned top left - moving camera")
@@ -1189,6 +1190,7 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # Reset our timer to 0
                         self.unit_attack_delay_timer = 0
+                        self.camera_reset_required += 1
 
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 

commit 7c6e9ec9d46f0344b76417882d32dd00bfcef0ef
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 25 13:21:35 2023 -0400

    Updated excluded_actions to properly handle new attack actions

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index d08a1ae3..bd50c254 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -105,7 +105,7 @@ smart_actions = [
 ]
 
 # DQN State size
-STATE_SIZE = 6
+STATE_SIZE = 7
 
 # Define the top-left coordinates for each quadrant
 quadrants = [
@@ -150,30 +150,30 @@ for index, action in enumerate(smart_actions):
 print("--------------------")
 
 # --------------------
-# # Action Mapping
-# # 0: 'donothing'
-# # 1: 'buildsupplydepot'
-# # 2: 'buildbarracks'
-# # 3: 'buildmarine'
-# # 4: 'attackunit'
-# # 5: 'buildscv'
-# # 6: 'resetcamera'
-# # 7: 'attack_4_4'
-# # 8: 'attack_12_4'
-# # 9: 'attack_4_12'
-# # 10: 'attack_12_12'
-# # 11: 'attack_36_4'
-# # 12: 'attack_44_4'
-# # 13: 'attack_36_12'
-# # 14: 'attack_44_12'
-# # 15: 'attack_4_36'
-# # 16: 'attack_12_36'
-# # 17: 'attack_4_44'
-# # 18: 'attack_12_44'
-# # 19: 'attack_36_36'
-# # 20: 'attack_44_36'
-# # 21: 'attack_36_44'
-# # 22: 'attack_44_44'
+# Action Mapping
+# 0: 'donothing'
+# 1: 'buildsupplydepot'
+# 2: 'buildbarracks'
+# 3: 'buildmarine'
+# 4: 'attackunit'
+# 5: 'buildscv'
+# 6: 'resetcamera'
+# 7: 'attack_4_4'
+# 8: 'attack_12_4'
+# 9: 'attack_4_12'
+# 10: 'attack_12_12'
+# 11: 'attack_36_4'
+# 12: 'attack_44_4'
+# 13: 'attack_36_12'
+# 14: 'attack_44_12'
+# 15: 'attack_4_36'
+# 16: 'attack_12_36'
+# 17: 'attack_4_44'
+# 18: 'attack_12_44'
+# 19: 'attack_36_36'
+# 20: 'attack_44_36'
+# 21: 'attack_36_44'
+# 22: 'attack_44_44'
 # --------------------
 
 
@@ -211,7 +211,7 @@ class DQNModel(nn.Module):
         self.writer_path = 'runs/dqn-cnn-agent-v3'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 250000
+        self.training_buffer_requirement = 50000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -899,7 +899,7 @@ class DQNAgent(base_agent.BaseAgent):
 
 
             current_state = {
-                "non_spatial": np.zeros(6),
+                "non_spatial": np.zeros(7),
                 "rgb_minimap": None
             }
             current_state["non_spatial"][0] = cc_count
@@ -910,6 +910,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Custom State Add-on
             current_state["non_spatial"][4] = len(self_units)
             current_state["non_spatial"][5] = len(enemy_units)
+            current_state["non_spatial"][6] = self.base_top_left # spawn location boolean seems critical for learning
             current_state["rgb_minimap"] = self.dqn_model.transform_minimap(rgb_minimap, self.base_top_left)
             # current_state["rgb_minimap"] = rgb_minimap
 
@@ -935,7 +936,8 @@ class DQNAgent(base_agent.BaseAgent):
             excluded_actions = []
             # print("excluded_actions at the start are set to: ", excluded_actions)
 
-            # Action Mapping
+            # --------------------
+            # Action Mapping Reference
             # 0: 'donothing'
             # 1: 'buildsupplydepot'
             # 2: 'buildbarracks'
@@ -943,10 +945,23 @@ class DQNAgent(base_agent.BaseAgent):
             # 4: 'attackunit'
             # 5: 'buildscv'
             # 6: 'resetcamera'
-            # 7: 'attack_15_15'
-            # 8: 'attack_15_47'
-            # 9: 'attack_47_15'
-            # 10: 'attack_47_47'
+            # 7: 'attack_4_4'
+            # 8: 'attack_12_4'
+            # 9: 'attack_4_12'
+            # 10: 'attack_12_12'
+            # 11: 'attack_36_4'
+            # 12: 'attack_44_4'
+            # 13: 'attack_36_12'
+            # 14: 'attack_44_12'
+            # 15: 'attack_4_36'
+            # 16: 'attack_12_36'
+            # 17: 'attack_4_44'
+            # 18: 'attack_12_44'
+            # 19: 'attack_36_36'
+            # 20: 'attack_44_36'
+            # 21: 'attack_36_44'
+            # 22: 'attack_44_44'
+            # --------------------
 
             # Modified, self-generated code to scale supply depot creation
             if supply_free > 7 or self.supply_delay_timer < 25:
@@ -978,11 +993,11 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # modified original logic, waits for 8 marines before attacking
+            # Excludes all minimap attack actions if we just issued one (at least...for now)
+            # Post-bootstrap, it may be possible to relax this
             if army_supply < 10 or self.attack_delay_timer < 35:
-                excluded_actions.append(7)
-                excluded_actions.append(8)
-                excluded_actions.append(9)
-                excluded_actions.append(10)
+                excluded_actions.extend(range(7, 23))
+
 
             # print("Our exclusions are set to: ", excluded_actions)
 

commit 9ccfe7250e538f9c3cdd942b00c5cc9a76df2af6
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 25 12:57:12 2023 -0400

    Added 16 attack points versus 4 of legacy

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index 7d154040..d08a1ae3 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-cnn-agent-model-1.pt'
+DATA_FILE = 'dqn-cnn-agent-model-3.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -107,14 +107,75 @@ smart_actions = [
 # DQN State size
 STATE_SIZE = 6
 
+# Define the top-left coordinates for each quadrant
+quadrants = [
+    (0, 0),       # Top left quadrant
+    (32, 0),      # Top right quadrant
+    (0, 32),      # Bottom left quadrant
+    (32, 32)      # Bottom right quadrant
+]
+#######################3
 # Steven Brown's implementation for spawn-location-agnostic quadrant attacks
-for mm_x in range(0, 64):
-    for mm_y in range(0, 64):
-        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
-            smart_actions.append(ACTION_ATTACK + '_' +
-                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+# for mm_x in range(0, 64):
+#     for mm_y in range(0, 64):
+#         if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+#             smart_actions.append(ACTION_ATTACK + '_' +
+#                                  str(mm_x - 16) + '_' + str(mm_y - 16))
+################
+
+# Calculate the offset points for each quadrant 
+# This is used in the smart_attack functions (16 locations with offsets)
+def calculate_quadrant_points(top_left_x, top_left_y):
+    offset = 4
+    midpoint_offset = 16  # minimap is 64x64, so half of a quadrant is 16...
+    return [
+        (top_left_x + offset, top_left_y + offset),
+        (top_left_x + midpoint_offset - offset, top_left_y + offset),
+        (top_left_x + offset, top_left_y + midpoint_offset - offset),
+        (top_left_x + midpoint_offset - offset, top_left_y + midpoint_offset - offset)
+    ]
+# For each quadrant, calculate the offset points and append the attack action
+for quad in quadrants:
+    points = calculate_quadrant_points(*quad)
+    for x, y in points:
+        smart_actions.append(ACTION_ATTACK + '_' + str(x) + '_' + str(y))
+
+
+# print("smart_actions is set to: ", smart_actions)
+print("--------------------")
+
+print("# Action Mapping")
+for index, action in enumerate(smart_actions):
+    print(f"# {index}: '{action}'")
+print("--------------------")
+
+# --------------------
+# # Action Mapping
+# # 0: 'donothing'
+# # 1: 'buildsupplydepot'
+# # 2: 'buildbarracks'
+# # 3: 'buildmarine'
+# # 4: 'attackunit'
+# # 5: 'buildscv'
+# # 6: 'resetcamera'
+# # 7: 'attack_4_4'
+# # 8: 'attack_12_4'
+# # 9: 'attack_4_12'
+# # 10: 'attack_12_12'
+# # 11: 'attack_36_4'
+# # 12: 'attack_44_4'
+# # 13: 'attack_36_12'
+# # 14: 'attack_44_12'
+# # 15: 'attack_4_36'
+# # 16: 'attack_12_36'
+# # 17: 'attack_4_44'
+# # 18: 'attack_12_44'
+# # 19: 'attack_36_36'
+# # 20: 'attack_44_36'
+# # 21: 'attack_36_44'
+# # 22: 'attack_44_44'
+# --------------------
 
-print("smart_actions is set to: ", smart_actions)
 
 ################################## End of BoilerPlate Code #####################################################
 
@@ -147,7 +208,7 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v2'
+        self.writer_path = 'runs/dqn-cnn-agent-v3'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
         self.training_buffer_requirement = 250000
@@ -161,12 +222,25 @@ class DQNModel(nn.Module):
             4: 0.5,  # 'attackunit'
             5: 0.5,  # 'buildscv'
             6: 0,  # 'resetcamera'
-            7: 0,  # 'attack_15_15'
-            8: 0,  # 'attack_15_47'
-            9: 0,  # 'attack_47_15'
-            10: 0  # 'attack_47_47'
+            7: 0,  # 'attack_4_4'
+            8: 0,  # 'attack_12_4'
+            9: 0,  # 'attack_4_12'
+            10: 0,  # 'attack_12_12'
+            11: 0,  # 'attack_36_4'
+            12: 0,  # 'attack_44_4'
+            13: 0,  # 'attack_36_12'
+            14: 0,  # 'attack_44_12'
+            15: 0,  # 'attack_4_36'
+            16: 0,  # 'attack_12_36'
+            17: 0,  # 'attack_4_44'
+            18: 0,  # 'attack_12_44'
+            19: 0,  # 'attack_36_36'
+            20: 0,  # 'attack_44_36'
+            21: 0,  # 'attack_36_44'
+            22: 0,  # 'attack_44_44'
         }
 
+
         # Attempt GPU acceleration
         self.device = torch.device(
             "cuda" if torch.cuda.is_available() else "cpu")
@@ -443,40 +517,71 @@ class DQNModel(nn.Module):
         normalized_reward = min(score / max_score, 1)
 
         # # Action Mapping Reference
-        #     0: 0, # 'donothing'
-        #     1: 0.25, # 'buildsupplydepot'
-        #     2: 0.5, # 'buildbarracks'
-        #     3: 0.25, # 'buildmarine'
-        #     4: 0.5, # 'attackunit'
-        #     5: 0.5, # 'buildscv'
-        #     6: 0, # 'resetcamera'
-        #     7: 0, # 'attack_15_15'
-        #     8: 0, # 'attack_15_47'
-        #     9: 0, # 'attack_47_15'
-        #     10: 0 # 'attack_47_47'
+            # 0: 0,  # 'donothing'
+            # 1: 0.25,  # 'buildsupplydepot'
+            # 2: 0.5,  # 'buildbarracks'
+            # 3: 0.25,  # 'buildmarine'
+            # 4: 0.5,  # 'attackunit'
+            # 5: 0.5,  # 'buildscv'
+            # 6: 0,  # 'resetcamera'
+            # 7: 0,  # 'attack_4_4'
+            # 8: 0,  # 'attack_12_4'
+            # 9: 0,  # 'attack_4_12'
+            # 10: 0,  # 'attack_12_12'
+            # 11: 0,  # 'attack_36_4'
+            # 12: 0,  # 'attack_44_4'
+            # 13: 0,  # 'attack_36_12'
+            # 14: 0,  # 'attack_44_12'
+            # 15: 0,  # 'attack_4_36'
+            # 16: 0,  # 'attack_12_36'
+            # 17: 0,  # 'attack_4_44'
+            # 18: 0,  # 'attack_12_44'
+            # 19: 0,  # 'attack_36_36'
+            # 20: 0,  # 'attack_44_36'
+            # 21: 0,  # 'attack_36_44'
+            # 22: 0,  # 'attack_44_44'
+
+        # Incentive multipliers for attack logic
+        attack_opposite_quadrant = 0.5
+        attack_adjacent_quadrant = 0.25
+        penalty_home_quadrant = -0.5
+        penalty_home_expansion = -0.25
+
+        # Define which actions correspond to which quadrants
+        top_left_actions = [7, 8, 9, 10]
+        top_right_actions = [11, 12, 13, 14]
+        bottom_left_actions = [15, 16, 17, 18]
+        bottom_right_actions = [19, 20, 21, 22]
+
 
         # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
         # Also de-incentivize attacking home base & expansion
         if base_top_left:
-            if previous_action == 10:
-                normalized_reward += 0.5
-            elif previous_action == 9:
-                normalized_reward += 0.25
-            elif previous_action == 7:
-                normalized_reward -= 0.5
-            elif previous_action == 8:
-                normalized_reward -= 0.25
+            # Reward attacking the bottom right the most
+            if previous_action in bottom_right_actions:
+                normalized_reward += attack_opposite_quadrant
+            # Reward attacking the bottom left less
+            elif previous_action in bottom_left_actions:
+                normalized_reward += attack_adjacent_quadrant
+            # Penalize attacking the top left (home base) the most
+            elif previous_action in top_left_actions:
+                normalized_reward += penalty_home_quadrant
+            # Penalize attacking the top right (home expansion) less
+            elif previous_action in top_right_actions:
+                normalized_reward += penalty_home_expansion
         # Otherwise, we spawned in the bottom right, reward attacks in the top left
         # Also de-incentivize attacking home base
         else:
-            if previous_action == 7:
-                normalized_reward += 0.5
-            elif previous_action == 8:
-                normalized_reward += 0.25
-            elif previous_action == 10:
-                normalized_reward -= 0.5
-            elif previous_action == 9:
-                normalized_reward -= 0.25
+            # Same logic, but inverted since base is in the bottom right
+            if previous_action in top_left_actions:
+                normalized_reward += attack_opposite_quadrant
+            elif previous_action in top_right_actions:
+                normalized_reward += attack_adjacent_quadrant
+            elif previous_action in bottom_right_actions:
+                normalized_reward += penalty_home_quadrant
+            elif previous_action in bottom_left_actions:
+                normalized_reward += penalty_home_expansion
+
 
         # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
         action_reward = self.action_rewards.get(previous_action, 0)
@@ -526,6 +631,8 @@ class DQNModel(nn.Module):
             transformed_minimap = minimap_data
             
         return transformed_minimap
+    
+
 
 
 # Agent Implementation
@@ -1032,17 +1139,18 @@ class DQNAgent(base_agent.BaseAgent):
                     do_it = False
 
                 if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
-                    x_offset = random.randint(-1, 1)
-                    y_offset = random.randint(-1, 1)
+                    # x_offset = random.randint(-1, 1)
+                    # y_offset = random.randint(-1, 1)
 
                     # Debugs
                     # print("Our base is top left: ", self.base_top_left)
                     # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
-
+                    # print("Attacking: ", self.transformLocation(int(x), int(y)))
                     # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
                     self.attack_delay_timer = 0
 
-                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                    # return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
 
             # Custom implementation to attack random enemy unit
             elif smart_action == ACTION_ATTACK_UNIT:

commit 95272a1975687690ac7e14ba25f8215fbd4e54ba
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 25 10:12:35 2023 -0400

    lowered replay-buffer size and fixed camera reset locations on new map size

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index ae4eb730..7d154040 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -119,9 +119,9 @@ print("smart_actions is set to: ", smart_actions)
 ################################## End of BoilerPlate Code #####################################################
 
 
-# Custom DQN Agent implementation with a replay buffer of 10M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+# Custom DQN Agent implementation with a replay buffer of 2M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=10000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=2000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -871,7 +871,7 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(6)
 
             # modified original logic, waits for 8 marines before attacking
-            if army_supply < 10 or self.attack_delay_timer < 60:
+            if army_supply < 10 or self.attack_delay_timer < 35:
                 excluded_actions.append(7)
                 excluded_actions.append(8)
                 excluded_actions.append(9)
@@ -948,9 +948,11 @@ class DQNAgent(base_agent.BaseAgent):
                 self.camera_delay_timer = 0
 
                 if self.base_top_left:
-                    return actions.FUNCTIONS.move_camera((23, 25))
+                    # print("Spawned top left - moving camera")
+                    return actions.FUNCTIONS.move_camera((22, 18))
                 else:
-                    return actions.FUNCTIONS.move_camera((38, 46))
+                    # print("Spawned bottom right - moving camera")
+                    return actions.FUNCTIONS.move_camera((43, 51))
 
         elif self.move_number == 1:
             self.move_number += 1
@@ -1015,7 +1017,7 @@ class DQNAgent(base_agent.BaseAgent):
                     self.scv_delay_timer = 0
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
 
-            # start of boiler plate code (small modifications like delay timers, that's it)
+            # start of boiler plate code (small modifications like delay timers)
             elif smart_action == ACTION_BUILD_MARINE:
                 if _TRAIN_MARINE in obs.observation['available_actions']:
                     return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
@@ -1041,12 +1043,6 @@ class DQNAgent(base_agent.BaseAgent):
                     self.attack_delay_timer = 0
 
                     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
-                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
-                #     # Debugs
-                #     # print("Our base is top left: ", self.base_top_left)
-                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
-
-                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
 
             # Custom implementation to attack random enemy unit
             elif smart_action == ACTION_ATTACK_UNIT:

commit aff199d3143d86f190e6ead7d3ac625ba3508496
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 24 16:26:33 2023 -0400

    Normalized minimap and cleaned up code

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index e4eb7a97..ae4eb730 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -132,7 +132,7 @@ class DQNModel(nn.Module):
         self.final_epsilon = 0.05
         # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 1500000
+            self.epsilon - self.final_epsilon) / 1000000
         self.action_counter = 0
         #
         self.state_size = non_spatial_state_size
@@ -147,10 +147,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-cnn-agent-v1'
+        self.writer_path = 'runs/dqn-cnn-agent-v2'
         # Our replay buffer training theshold size
         # self.training_buffer_requirement = 500000
-        self.training_buffer_requirement = 100000
+        self.training_buffer_requirement = 250000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -174,26 +174,25 @@ class DQNModel(nn.Module):
 
         # CNN for RGB minimap
         self.conv = nn.Sequential(
-            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),  # Assuming RGB minimap
+            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
             nn.ReLU(),
             nn.MaxPool2d(kernel_size=2, stride=2),
-            # ... [Add more conv layers if required]
         ).to(self.device)  # Moving to GPU if available
 
-        # Calculate size of flattened output after convolutional layers
+        # Calculate the size of our flattened output after convolutional layers
         self.conv_out_size = self._get_conv_out(minimap_shape)
 
-        # FCN for non-spatial data
+        # Fully Connected Network (FCN) for non-spatial data
         self.fc_non_spatial = nn.Sequential(
             nn.Linear(non_spatial_state_size, 512),
             nn.ReLU(),
             nn.Dropout(0.2)
         ).to(self.device)  # Moving to GPU if available
 
-        # Decision-making layers
+        # Decision-making layers (takes processed outputs from CNN and FCN and concatenates & processes them)
         self.fc_decision = nn.Sequential(
             nn.Linear(self.conv_out_size + 512, 1024),
             nn.ReLU(),
@@ -292,7 +291,8 @@ class DQNModel(nn.Module):
 
         # Convert data to tensors and move them to the GPU
         non_spatial_data_tensor = torch.tensor(non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
-        # Minimap needs to have its dimensions changed slightly...
+        # Minimap needs to have its dimensions changed slightly as PyTorch expects colours first (3,64,64) versus (64,64,3)
+        # Unsqueeze is also required as PyTorch expects batch size in the first position. In our case, that's: [1,3,64,64]
         rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.device)
 
 
@@ -518,7 +518,9 @@ class DQNModel(nn.Module):
     def transform_minimap(self, minimap_data, base_top_left):
         if not base_top_left:
             # Flip only the spatial dimensions, leaving the color channels unchanged
-            transformed_minimap = np.flip(minimap_data, axis=(0, 1))
+            # Copy required due to oddities in the return values..
+            # PyTorch can't deal with negative strides AFAIK
+            transformed_minimap = np.flip(minimap_data, axis=(0, 1)).copy()
         else:
             # Leave the minimap as it is
             transformed_minimap = minimap_data
@@ -673,8 +675,9 @@ class DQNAgent(base_agent.BaseAgent):
             print("Our rolling-average reward is: ", avg_reward)
             print("Latest game reward was: ", combined_reward)
             print("Number of steps were: ", episode_steps)
-            print("Number of in-game model updates: ",
-                  self.in_game_training_iterations)
+            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                print("Number of in-game model updates: ",
+                    self.in_game_training_iterations)
             # Backpropagate the final reward to previous actions
             print("Backpropagating reward updates across",
                   self.actual_root_level_steps_taken, "root level actions.")
@@ -800,52 +803,11 @@ class DQNAgent(base_agent.BaseAgent):
             # Custom State Add-on
             current_state["non_spatial"][4] = len(self_units)
             current_state["non_spatial"][5] = len(enemy_units)
-            # current_state["rgb_minimap"] = self.dqn_model.transform_minimap(rgb_minimap, self.base_top_left)
-            current_state["rgb_minimap"] = rgb_minimap
+            current_state["rgb_minimap"] = self.dqn_model.transform_minimap(rgb_minimap, self.base_top_left)
+            # current_state["rgb_minimap"] = rgb_minimap
 
             # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
 
-            #####
-            # shouldn't need any of this with minimap conv. net
-
-            # hot_squares = np.zeros(4)
-            # enemy_y, enemy_x = (
-            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
-            # for i in range(0, len(enemy_y)):
-            #     # Had to adjust due to out of range errors - screen is actually 84x84
-            #     y = int(math.ceil((enemy_y[i] + 1) / 42))
-            #     x = int(math.ceil((enemy_x[i] + 1) / 42))
-
-            #     index = ((y - 1) * 2) + (x - 1)
-
-            #     hot_squares[index] = 1
-
-            # if not self.base_top_left:
-            #     hot_squares = hot_squares[::-1]
-
-            # for i in range(0, 4):
-            #     # print("Hot Squares in current_state",
-            #     #       i+5, "is set to: ", hot_squares[i])
-            #     current_state[i + 5] = hot_squares[i]
-
-            # green_squares = np.zeros(4)
-            # friendly_y, friendly_x = (
-            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            # for i in range(0, len(friendly_y)):
-            #     # Screen is actually 84x84, not 32x or 64x
-            #     y = int(math.ceil((friendly_y[i] + 1) / 42))
-            #     x = int(math.ceil((friendly_x[i] + 1) / 42))
-
-            #     index = ((y - 1) * 2) + (x - 1)
-
-            #     green_squares[index] = 1
-
-            # if not self.base_top_left:
-            #     green_squares = green_squares[::-1]
-
-            # for i in range(0, 4):
-            #     current_state[i + 9] = green_squares[i]
-
             # print("State at the end of the step is set to: ", current_state)
 
             # Push s/a/r/s_next our replay buffer
@@ -856,25 +818,15 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.store_transition(self.previous_state,
                                                 self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, self.base_top_left), current_state)
 
-            # Do in-game training of the model for every 100 root actions it takes:
+            # Do in-game training of the model for every 100 root actions the agent takes:
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("Beginning in-game training for the model.")
                 self.in_game_training_iterations += 1
                 self.dqn_model.train()
 
-                # Log our reward over time if we've started training
-                # if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
-                #     with SummaryWriter(self.dqn_model.writer_path) as writer:
-                #         writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
-
-                # print("Training complete.")
-
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []
             # print("excluded_actions at the start are set to: ", excluded_actions)
-            # Original Code:
-            # # if supply_depot_count == 2 or worker_supply == 0:
-            #     excluded_actions.append(1)
 
             # Action Mapping
             # 0: 'donothing'
@@ -992,24 +944,9 @@ class DQNAgent(base_agent.BaseAgent):
             # AI can only act on units on its screen (unless using hotkeys/mappings...)
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
             # Coordinate flipping isn't ideal for camera movement unfortunately...
-
             elif smart_action == ACTION_RESET_CAMERA:
                 self.camera_delay_timer = 0
 
-                # # This logic...usually works, but often fails spectacularly.
-                # # Using hardcoded logic for now
-                # # Will revisit later if possible.
-                # if not obs.first() and self.command_center[0].x is not None and self.command_center[0].y is not None:
-                #      # Using unpacking *, do inline transformation
-                #     x, y = self.transformLocation(*self.dqn_model.translate_coordinates(self.command_center[0].x, self.command_center[0].y))
-                #     print("Original CC coordinates were: ", self.command_center[0].x, self.command_center[0].y)
-                #     print("Translated coordinates are: ", x, y)
-                #     return actions.FUNCTIONS.move_camera((x, y))
-
-                # else:
-                #     print("Command Center coordinates not found. Unable to reset camera to home base.")
-                #     # Handle the case where the coordinates are not found, e.g., return a default action
-
                 if self.base_top_left:
                     return actions.FUNCTIONS.move_camera((23, 25))
                 else:

commit 12ef07ec51bf2705c72172e189bdffdd5f2bdd4d
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 24 14:15:05 2023 -0400

    Updated DQN to use CNN

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
index eba0645a..e4eb7a97 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -4,7 +4,7 @@
 # All of the RL algorithms were implemented by me
 
 # Train the agent against a medium Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v14.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay --action_space RGB --rgb_minimap_size 64 --rgb_screen_size 84
 
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-22.pt'
+DATA_FILE = 'dqn-cnn-agent-model-1.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -105,7 +105,7 @@ smart_actions = [
 ]
 
 # DQN State size
-STATE_SIZE = 13
+STATE_SIZE = 6
 
 # Steven Brown's implementation for spawn-location-agnostic quadrant attacks
 for mm_x in range(0, 64):
@@ -121,7 +121,7 @@ print("smart_actions is set to: ", smart_actions)
 
 # Custom DQN Agent implementation with a replay buffer of 10M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=10000000, batch_size=2048):
+    def __init__(self, actions, non_spatial_state_size, minimap_shape, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=10000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -135,7 +135,7 @@ class DQNModel(nn.Module):
             self.epsilon - self.final_epsilon) / 1500000
         self.action_counter = 0
         #
-        self.state_size = state_size
+        self.state_size = non_spatial_state_size
         self.disallowed_actions = {}
         # Replay buffer
         self.buffer_capacity = buffer_capacity
@@ -147,9 +147,10 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-agent-v22'
+        self.writer_path = 'runs/dqn-cnn-agent-v1'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 500000
+        # self.training_buffer_requirement = 500000
+        self.training_buffer_requirement = 100000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -170,30 +171,64 @@ class DQNModel(nn.Module):
         self.device = torch.device(
             "cuda" if torch.cuda.is_available() else "cpu")
 
-        self.model = nn.Sequential(
-            nn.Linear(state_size, 512),
+
+        # CNN for RGB minimap
+        self.conv = nn.Sequential(
+            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),  # Assuming RGB minimap
             nn.ReLU(),
-            nn.Dropout(0.2),
-            nn.Linear(512, 1024),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),
             nn.ReLU(),
-            nn.Dropout(0.2),
-            nn.Linear(1024, 2048),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+            # ... [Add more conv layers if required]
+        ).to(self.device)  # Moving to GPU if available
+
+        # Calculate size of flattened output after convolutional layers
+        self.conv_out_size = self._get_conv_out(minimap_shape)
+
+        # FCN for non-spatial data
+        self.fc_non_spatial = nn.Sequential(
+            nn.Linear(non_spatial_state_size, 512),
             nn.ReLU(),
-            nn.Dropout(0.2),
-            nn.Linear(2048, 1024),
+            nn.Dropout(0.2)
+        ).to(self.device)  # Moving to GPU if available
+
+        # Decision-making layers
+        self.fc_decision = nn.Sequential(
+            nn.Linear(self.conv_out_size + 512, 1024),
             nn.ReLU(),
             nn.Dropout(0.2),
-            nn.Linear(1024, 512),
+            nn.Linear(1024, 2048),
             nn.ReLU(),
-            nn.Linear(512, len(self.actions))
-        ).to(self.device)  # Move model to GPU if available
-        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+            nn.Dropout(0.2),
+            nn.Linear(2048, len(self.actions))
+        ).to(self.device)  # Moving to GPU if available
+
+        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
         # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
         self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
 
+    # 
+    def _get_conv_out(self, shape):
+        o = self.conv(torch.zeros(1, *shape).to(self.device))
+        return int(np.prod(o.size()))
+    
+    def forward(self, non_spatial_data, minimap):
+        non_spatial_data = non_spatial_data.to(self.device)
+        minimap = minimap.to(self.device)
+        
+        # Separate treatments for different types of data
+        conv_out = self.conv(minimap).reshape(minimap.size()[0], -1)
+        fc_out = self.fc_non_spatial(non_spatial_data)
+
+        # Combine both outputs
+        combined = torch.cat((conv_out, fc_out), dim=1)
+
+        return self.fc_decision(combined)
+
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
     def get_writer(self):
         return SummaryWriter(self.writer_path)
@@ -245,20 +280,25 @@ class DQNModel(nn.Module):
     def random_sample(self, batch_size):
         return random_sample(self.buffer, batch_size)
 
-    def forward(self, x):
-        return self.model(x)
-
     # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
     # Algorithm that Steven Brown (PySC2 Dev) created
     # It's visible here
     # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
-    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
-    def choose_action(self, observation, excluded_actions=[]):
-        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(
-            0).to(self.device)  # Move observation to GPU
+    # I've of course modified it to use linear decay, a DQN with minimap-CNN (via torch) instead of Q-Learning with basic hot-squares, etc but...the initial work is his
+    def choose_action(self, current_state, excluded_actions=[]):
+        # Extract non_spatial_data and rgb_minimap from current_state
+        non_spatial_data = current_state["non_spatial"]
+        rgb_minimap = current_state["rgb_minimap"]
+
+        # Convert data to tensors and move them to the GPU
+        non_spatial_data_tensor = torch.tensor(non_spatial_data, dtype=torch.float32).unsqueeze(0).to(self.device)
+        # Minimap needs to have its dimensions changed slightly...
+        rgb_minimap_tensor = torch.tensor(rgb_minimap, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(self.device)
+
+
+        # Epsilon-based exploration 
         if np.random.uniform() < self.epsilon:
-            available_actions = [
-                a for a in self.actions if a not in excluded_actions]
+            available_actions = [a for a in self.actions if a not in excluded_actions]
             action = np.random.choice(available_actions)
 
             # This is where we keep the logic for epsilon decay (linear)
@@ -266,21 +306,34 @@ class DQNModel(nn.Module):
             self.epsilon -= self.epsilon_decay_rate
             self.epsilon = max(self.final_epsilon, self.epsilon)
         else:
-            q_values = self.model(observation_tensor)
+            # Pass the minimap data through the CNN
+            conv_output = self.conv(rgb_minimap_tensor)
+            # Flatten the output for fully connected layers
+            conv_output = conv_output.view(conv_output.size(0), -1)
+
+            # Pass the non-spatial data through its layers
+            non_spatial_output = self.fc_non_spatial(non_spatial_data_tensor)
+
+            # Concatenate the two outputs
+            combined_output = torch.cat((conv_output, non_spatial_output), dim=1)
+
+            # Finally, pass through the decision-making layers
+            q_values = self.fc_decision(combined_output)
+
             # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
             for action in excluded_actions:
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
 
+        # print("The action we chose was: ", action)
+        # print("The excluded actions were: ", excluded_actions)
         return action
 
     # This is where we train the model
     # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
     def train(self):
         # Check if the replay buffer has enough samples (using 500K as minimum)
-        # If the buffer is too small, we'll overfit. This is essentially the 'warm up' period for the model
         if len(self.buffer) < self.training_buffer_requirement:
-            # if len(self.buffer) < 1000:
             print("Replay buffer is currently too small to conduct training...")
             return
 
@@ -292,29 +345,41 @@ class DQNModel(nn.Module):
         # Unzip the transitions into separate variables
         states, actions, rewards, next_states = zip(*transitions)
 
+        # Separate non_spatial and rgb_minimap components of states
+        non_spatial_states = [state["non_spatial"] for state in states]
+        rgb_minimap_states = [state["rgb_minimap"] for state in states]
+
+
+        non_spatial_next_states = [state["non_spatial"] for state in next_states]
+        rgb_minimap_next_states = [state["rgb_minimap"] for state in next_states]
+
         # Convert the zipped values to numpy arrays
-        # Library throws a repeated warning if we convert directly to tensors. E.g
-        # train_dqn_agent_v13.py:262: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.
-        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.
-        states_np = np.array(states, dtype=np.float32)
+        non_spatial_states_np = np.array(non_spatial_states, dtype=np.float32)
+        rgb_minimap_states_np = np.array(rgb_minimap_states, dtype=np.float32)
+        non_spatial_next_states_np = np.array(non_spatial_next_states, dtype=np.float32)
+        rgb_minimap_next_states_np = np.array(rgb_minimap_next_states, dtype=np.float32)
         actions_np = np.array(actions, dtype=np.int64)
         rewards_np = np.array(rewards, dtype=np.float32)
-        next_states_np = np.array(next_states, dtype=np.float32)
 
-        # Convert the numpy arrays to tensors (PyTorch complains about doing this itself...)
-        states = torch.tensor(states_np).to(self.device)
+        # Convert numpy arrays to tensors
+        non_spatial_states = torch.tensor(non_spatial_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_states = torch.tensor(rgb_minimap_states_np).to(self.device).permute(0, 3, 1, 2)
+        non_spatial_next_states = torch.tensor(non_spatial_next_states_np).to(self.device)
+        # Minimap requires permutation to meet PyTorch expectations
+        rgb_minimap_next_states = torch.tensor(rgb_minimap_next_states_np).to(self.device).permute(0, 3, 1, 2) 
+
         actions = torch.tensor(actions_np).to(self.device)
         rewards = torch.tensor(rewards_np).to(self.device)
-        next_states = torch.tensor(next_states_np).to(self.device)
 
         # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
         with autocast():
             # Compute the Q-values for the current states
-            q_values = self.model(states)
+            q_values = self(non_spatial_states, rgb_minimap_states)
             q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
 
             # Compute the target Q-values
-            next_q_values = self.model(next_states)
+            next_q_values = self(non_spatial_next_states, rgb_minimap_next_states)
             max_next_q_values = next_q_values.max(1)[0]
             q_target = rewards + self.gamma * max_next_q_values
 
@@ -337,40 +402,32 @@ class DQNModel(nn.Module):
         self.scheduler.step()
 
         # Logging various metrics for visualization and debugging
-        # These are separate calls due to a bug - File Descriptors are exhausted if they're not opened/closed individually... :( 
-        # Log training loss
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Loss/train', loss.item(),
-                              self.global_training_steps)
+            writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
 
-        # Log epsilon over time
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Epsilon/value', self.epsilon,
-                              self.global_training_steps)
+            writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
 
-        # Log our Q Values
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_histogram(
-                'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+            writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
 
         # Log the histograms of model weights every 100 iterations
         if self.global_training_steps % 100 == 0:
             with SummaryWriter(self.writer_path) as writer:
-                for name, param in self.model.named_parameters():
-                    writer.add_histogram(name, param.clone().cpu(
-                    ).data.numpy(), self.global_training_steps)
+                for name, param in self.named_parameters():
+                    writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
 
     def save_model(self, file_path, episode_count, reward):
-        # Save our checkpoint
+        # Save our checkpoint weights
         save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
-        torch.save(self.model.state_dict(), save_path)
+        torch.save(self.state_dict(), save_path)
 
         # Also overwrite the top of tree so that we always have the latest to load if necessary:
-        torch.save(self.model.state_dict(), file_path)
+        torch.save(self.state_dict(), file_path)
 
     def load_model(self, file_path):
         print("Loading last checkpoint: ", file_path)
-        self.model.load_state_dict(torch.load(file_path))
+        self.load_state_dict(torch.load(file_path))
 
     # This function identifies all units of a specific desired type
     def get_units_by_type(self, obs, unit_type):
@@ -457,6 +514,18 @@ class DQNModel(nn.Module):
         scale_factor = target_size / original_size
         return int(x * scale_factor), int(y * scale_factor)
 
+    # To ensure consistency across spawn locations, we invert the minimap locations based on base_top_left
+    def transform_minimap(self, minimap_data, base_top_left):
+        if not base_top_left:
+            # Flip only the spatial dimensions, leaving the color channels unchanged
+            transformed_minimap = np.flip(minimap_data, axis=(0, 1))
+        else:
+            # Leave the minimap as it is
+            transformed_minimap = minimap_data
+            
+        return transformed_minimap
+
+
 # Agent Implementation
 
 
@@ -471,10 +540,16 @@ class DQNAgent(base_agent.BaseAgent):
         print("State Size:", STATE_SIZE)
 
         self.dqn_model = DQNModel(
-            actions=initial_actions, state_size=STATE_SIZE)
+            actions=initial_actions, non_spatial_state_size=STATE_SIZE, minimap_shape=(3,64,64))
 
         self.previous_action = None
-        self.previous_state = None
+        self.previous_state = {
+            "non_spatial": None,
+            "rgb_minimap": None
+        }
+
+        
+
 
         self.cc_y = None
         self.cc_x = None
@@ -498,6 +573,9 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.command_center = []
 
+        # Visuals
+        rgb_minimap = []
+
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
             self.dqn_model.load_model(DATA_FILE)
@@ -546,6 +624,10 @@ class DQNAgent(base_agent.BaseAgent):
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
         self.camera_delay_timer += 1
+        
+        # Visuals
+        rgb_minimap = obs.observation["rgb_minimap"]
+
 
         # Check our current score, just for debugging
         # print("Current score is: ", self.get_normalized_reward(obs))
@@ -657,6 +739,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
 
+
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # supply_depot_count = int(round(len(depot_y) / 69))
         all_supply_depots = self.dqn_model.get_units_by_type(
@@ -671,6 +754,9 @@ class DQNAgent(base_agent.BaseAgent):
             obs, units.Terran.Barracks)
         barracks_count = len(
             [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+        
+        all_command_centers = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        command_center_count = len([unit for unit in all_command_centers if unit.alliance == features.PlayerRelative.SELF])
 
         # Find our command centers:
         # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
@@ -691,6 +777,9 @@ class DQNAgent(base_agent.BaseAgent):
         # BEGIN CUSTOM CODE
         enemy_units = [
             unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+        
+        self_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.SELF]
         # END CUSTOM CODE
 
         supply_free = supply_limit - supply_used
@@ -698,59 +787,64 @@ class DQNAgent(base_agent.BaseAgent):
         if self.move_number == 0:
             self.move_number += 1
 
-            current_state = np.zeros(13)
-            current_state[0] = cc_count
-            current_state[1] = supply_depot_count
-            current_state[2] = barracks_count
-            current_state[3] = army_supply
+
+            current_state = {
+                "non_spatial": np.zeros(6),
+                "rgb_minimap": None
+            }
+            current_state["non_spatial"][0] = cc_count
+            current_state["non_spatial"][1] = supply_depot_count
+            current_state["non_spatial"][2] = barracks_count
+            current_state["non_spatial"][3] = army_supply
+            current_state["non_spatial"][4] = command_center_count
             # Custom State Add-on
-            current_state[4] = len(enemy_units)
+            current_state["non_spatial"][4] = len(self_units)
+            current_state["non_spatial"][5] = len(enemy_units)
+            # current_state["rgb_minimap"] = self.dqn_model.transform_minimap(rgb_minimap, self.base_top_left)
+            current_state["rgb_minimap"] = rgb_minimap
 
-            # print("Current state is set to:", current_state)
-            # print("Player data is set to:", obs.observation['player'])
-            # print("Enemy units are: ",  [unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY])
-            # for unit in enemy_units:
-            #     x = unit.x
-            #     y = unit.y
-            #     print(f"Enemy unit of type {unit.unit_type} is at coordinates ({x}, {y})")
+            # print("Minimap shape is: ", current_state["rgb_minimap"].shape)
 
-            hot_squares = np.zeros(4)
-            enemy_y, enemy_x = (
-                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
-            for i in range(0, len(enemy_y)):
-                # Had to adjust due to out of range errors - screen is actually 84x84
-                y = int(math.ceil((enemy_y[i] + 1) / 42))
-                x = int(math.ceil((enemy_x[i] + 1) / 42))
+            #####
+            # shouldn't need any of this with minimap conv. net
 
-                index = ((y - 1) * 2) + (x - 1)
+            # hot_squares = np.zeros(4)
+            # enemy_y, enemy_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            # for i in range(0, len(enemy_y)):
+            #     # Had to adjust due to out of range errors - screen is actually 84x84
+            #     y = int(math.ceil((enemy_y[i] + 1) / 42))
+            #     x = int(math.ceil((enemy_x[i] + 1) / 42))
 
-                hot_squares[index] = 1
+            #     index = ((y - 1) * 2) + (x - 1)
 
-            if not self.base_top_left:
-                hot_squares = hot_squares[::-1]
+            #     hot_squares[index] = 1
 
-            for i in range(0, 4):
-                # print("Hot Squares in current_state",
-                #       i+5, "is set to: ", hot_squares[i])
-                current_state[i + 5] = hot_squares[i]
+            # if not self.base_top_left:
+            #     hot_squares = hot_squares[::-1]
 
-            green_squares = np.zeros(4)
-            friendly_y, friendly_x = (
-                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            for i in range(0, len(friendly_y)):
-                # Screen is actually 84x84, not 32x or 64x
-                y = int(math.ceil((friendly_y[i] + 1) / 42))
-                x = int(math.ceil((friendly_x[i] + 1) / 42))
+            # for i in range(0, 4):
+            #     # print("Hot Squares in current_state",
+            #     #       i+5, "is set to: ", hot_squares[i])
+            #     current_state[i + 5] = hot_squares[i]
+
+            # green_squares = np.zeros(4)
+            # friendly_y, friendly_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # for i in range(0, len(friendly_y)):
+            #     # Screen is actually 84x84, not 32x or 64x
+            #     y = int(math.ceil((friendly_y[i] + 1) / 42))
+            #     x = int(math.ceil((friendly_x[i] + 1) / 42))
 
-                index = ((y - 1) * 2) + (x - 1)
+            #     index = ((y - 1) * 2) + (x - 1)
 
-                green_squares[index] = 1
+            #     green_squares[index] = 1
 
-            if not self.base_top_left:
-                green_squares = green_squares[::-1]
+            # if not self.base_top_left:
+            #     green_squares = green_squares[::-1]
 
-            for i in range(0, 4):
-                current_state[i + 9] = green_squares[i]
+            # for i in range(0, 4):
+            #     current_state[i + 9] = green_squares[i]
 
             # print("State at the end of the step is set to: ", current_state)
 

commit 8850c956865ba73f18d668f316941c06047d4133
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 24 14:14:21 2023 -0400

    Create train_dqn_agent_v14.py

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
new file mode 100644
index 00000000..eba0645a
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v14.py
@@ -0,0 +1,1067 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Train the agent against a medium Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay
+
+# Train against Kane-AI with this string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
+# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
+# craig		soft	nofile		8192
+# craig 	hard	nofile		1048576
+# Validated with: ulimit -s -H && ulimit -n -S
+# required as after 1k episodes we run out of FD's:
+# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
+
+import random
+from random import sample as random_sample
+import math
+import os
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+# Using TensorBoard for model performance tracking & visualizations
+from torch.utils.tensorboard import SummaryWriter
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...quite poor excellent.
+from torch.optim.lr_scheduler import CosineAnnealingLR
+
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+from pysc2.lib import units
+# queue used for replay buffer
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-agent-model-22.pt'
+
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+# CUSTOM
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+# END CUSTOM
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+# CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
+# END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
+]
+
+# DQN State size
+STATE_SIZE = 13
+
+# Steven Brown's implementation for spawn-location-agnostic quadrant attacks
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code #####################################################
+
+
+# Custom DQN Agent implementation with a replay buffer of 10M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=10000000, batch_size=2048):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = gamma
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.05
+        # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (
+            self.epsilon - self.final_epsilon) / 1500000
+        self.action_counter = 0
+        #
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque(maxlen=buffer_capacity)
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+        # Counter for tracking how frequently training is run
+        self.global_training_steps = 0
+        # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
+        self.writer_path = 'runs/dqn-agent-v22'
+        # Our replay buffer training theshold size
+        self.training_buffer_requirement = 500000
+        # This is our custom reward/action mapping dictionary
+        # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        self.action_rewards = {
+            0: 0,  # 'donothing'
+            1: 0.25,  # 'buildsupplydepot'
+            2: 0.5,  # 'buildbarracks'
+            3: 0.25,  # 'buildmarine'
+            4: 0.5,  # 'attackunit'
+            5: 0.5,  # 'buildscv'
+            6: 0,  # 'resetcamera'
+            7: 0,  # 'attack_15_15'
+            8: 0,  # 'attack_15_47'
+            9: 0,  # 'attack_47_15'
+            10: 0  # 'attack_47_47'
+        }
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 2048),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(2048, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+        # Our learning rate scheduler is enabled here (CosineAnnealing)
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
+
+    # Using a method to handle writes to ensure things are closed properly in the event of a crash
+    def get_writer(self):
+        return SummaryWriter(self.writer_path)
+    # This is where we store items for our replay buffer
+    # s: The current state of the environment.
+    # a: The action taken by the agent in state s.
+    # r: The reward received after taking action a in state s.
+    # s_next: The resulting state after taking
+
+    def store_transition(self, s, a, r, s_next):
+        # Transition is a tuple (s, a, r, s_next)
+        transition = (s, a, r, s_next)
+        # Append the transition to the replay buffer
+        self.buffer.append(transition)
+
+    # This function goes back and tweaks the rewards associated with a given game
+    # Based on the final tangible reward we get from PySC2
+    # Win/loss/draw are multipliers
+    def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
+        # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
+        steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
+
+        # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
+        for i in range(-steps_to_iterate, 0):
+            # Skip if the buffer index result is None
+            if self.buffer[i] is None:
+                continue
+            # Fetch the transition
+            s, a, r, s_next = self.buffer[i]
+
+            # Modify the reward to include the final reward
+            new_reward = r * final_reward
+            # print("Before backpropagation: ", r)
+
+            # Replace the transition in the replay buffer
+            self.buffer[i] = (s, a, new_reward, s_next)
+            # print("After backpropagation: ", self.buffer[i][2])
+
+        print("Replay buffer currently has: ", len(self.buffer), "entries")
+
+    # # sample transitions from replay buffer queue for the last game
+    # def sample(self, batch_size):
+    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+    #     return list(self.buffer)[-batch_size:]
+
+    # Sample random transitions from replay buffer
+    # Hopefully in a stochastic manner...
+
+    def random_sample(self, batch_size):
+        return random_sample(self.buffer, batch_size)
+
+    def forward(self, x):
+        return self.model(x)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Algorithm that Steven Brown (PySC2 Dev) created
+    # It's visible here
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
+    def choose_action(self, observation, excluded_actions=[]):
+        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+            # This is where we keep the logic for epsilon decay (linear)
+            self.action_counter += 1
+            self.epsilon -= self.epsilon_decay_rate
+            self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+            q_values = self.model(observation_tensor)
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+        return action
+
+    # This is where we train the model
+    # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
+    def train(self):
+        # Check if the replay buffer has enough samples (using 500K as minimum)
+        # If the buffer is too small, we'll overfit. This is essentially the 'warm up' period for the model
+        if len(self.buffer) < self.training_buffer_requirement:
+            # if len(self.buffer) < 1000:
+            print("Replay buffer is currently too small to conduct training...")
+            return
+
+        # Increment our training counter
+        self.global_training_steps += 1
+
+        # Sample a mini-batch of transitions from the buffer
+        transitions = self.random_sample(self.batch_size)
+        # Unzip the transitions into separate variables
+        states, actions, rewards, next_states = zip(*transitions)
+
+        # Convert the zipped values to numpy arrays
+        # Library throws a repeated warning if we convert directly to tensors. E.g
+        # train_dqn_agent_v13.py:262: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.
+        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.
+        states_np = np.array(states, dtype=np.float32)
+        actions_np = np.array(actions, dtype=np.int64)
+        rewards_np = np.array(rewards, dtype=np.float32)
+        next_states_np = np.array(next_states, dtype=np.float32)
+
+        # Convert the numpy arrays to tensors (PyTorch complains about doing this itself...)
+        states = torch.tensor(states_np).to(self.device)
+        actions = torch.tensor(actions_np).to(self.device)
+        rewards = torch.tensor(rewards_np).to(self.device)
+        next_states = torch.tensor(next_states_np).to(self.device)
+
+        # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
+        with autocast():
+            # Compute the Q-values for the current states
+            q_values = self.model(states)
+            q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+
+            # Compute the target Q-values
+            next_q_values = self.model(next_states)
+            max_next_q_values = next_q_values.max(1)[0]
+            q_target = rewards + self.gamma * max_next_q_values
+
+            # Compute the loss and update the model's weights
+            loss = self.loss_fn(q_predict, q_target)
+
+        # Clear accumulated gradients before back propagation
+        self.optimizer.zero_grad()
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        # Update the learning rate based on CosineAnnealing scheduling
+        self.scheduler.step()
+
+        # Logging various metrics for visualization and debugging
+        # These are separate calls due to a bug - File Descriptors are exhausted if they're not opened/closed individually... :( 
+        # Log training loss
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_scalar('Loss/train', loss.item(),
+                              self.global_training_steps)
+
+        # Log epsilon over time
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_scalar('Epsilon/value', self.epsilon,
+                              self.global_training_steps)
+
+        # Log our Q Values
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_histogram(
+                'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+
+        # Log the histograms of model weights every 100 iterations
+        if self.global_training_steps % 100 == 0:
+            with SummaryWriter(self.writer_path) as writer:
+                for name, param in self.model.named_parameters():
+                    writer.add_histogram(name, param.clone().cpu(
+                    ).data.numpy(), self.global_training_steps)
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs, previous_action, base_top_left):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything about 12k score results in a full score being provided to the model
+        max_score = 12000
+        # Normalize the score to be between 0 and 1
+        normalized_reward = min(score / max_score, 1)
+
+        # # Action Mapping Reference
+        #     0: 0, # 'donothing'
+        #     1: 0.25, # 'buildsupplydepot'
+        #     2: 0.5, # 'buildbarracks'
+        #     3: 0.25, # 'buildmarine'
+        #     4: 0.5, # 'attackunit'
+        #     5: 0.5, # 'buildscv'
+        #     6: 0, # 'resetcamera'
+        #     7: 0, # 'attack_15_15'
+        #     8: 0, # 'attack_15_47'
+        #     9: 0, # 'attack_47_15'
+        #     10: 0 # 'attack_47_47'
+
+        # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if base_top_left:
+            if previous_action == 10:
+                normalized_reward += 0.5
+            elif previous_action == 9:
+                normalized_reward += 0.25
+            elif previous_action == 7:
+                normalized_reward -= 0.5
+            elif previous_action == 8:
+                normalized_reward -= 0.25
+        # Otherwise, we spawned in the bottom right, reward attacks in the top left
+        # Also de-incentivize attacking home base
+        else:
+            if previous_action == 7:
+                normalized_reward += 0.5
+            elif previous_action == 8:
+                normalized_reward += 0.25
+            elif previous_action == 10:
+                normalized_reward -= 0.5
+            elif previous_action == 9:
+                normalized_reward -= 0.25
+
+        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
+        action_reward = self.action_rewards.get(previous_action, 0)
+        # Add the action reward, ensuring the total reward stays between 0 and 1
+        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+
+        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
+
+        return normalized_reward
+
+    # Provide the x/y coordinates of our command center(s)
+    def get_command_center_coordinates(self, obs):
+        # Get Command Centers using the get_units_by_type function
+        command_centers = self.get_units_by_type(
+            obs, units.Terran.CommandCenter)
+
+        # If there's a Command Center, return its coordinates
+        if command_centers:
+            # Grabbing the first for now (no expansion support for cameras)
+            command_center = command_centers[0]
+
+            # Checks to avoid out-of-bounds crashing
+            # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+            if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
+                return command_center.x, command_center.y
+            else:
+                print(f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                return None, None
+        else:
+            print("Command Center not found.")
+            return None, None
+
+    # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
+    def translate_coordinates(self, x, y, original_size=84, target_size=64):
+        scale_factor = target_size / original_size
+        return int(x * scale_factor), int(y * scale_factor)
+
+# Agent Implementation
+
+
+class DQNAgent(base_agent.BaseAgent):
+
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Available actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+        self.actual_root_level_steps_taken = 0
+        self.in_game_training_iterations = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.unit_attack_delay_timer = 0
+        self.supply_delay_timer = 0
+        self.scv_delay_timer = 0
+        self.camera_delay_timer = 0
+
+        self.command_center = []
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.unit_attack_delay_timer += 1
+        self.supply_delay_timer += 1
+        self.scv_delay_timer += 1
+        self.camera_delay_timer += 1
+
+        # Check our current score, just for debugging
+        # print("Current score is: ", self.get_normalized_reward(obs))
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+                # Ensure combined_reward never goes below 1.1 for a win
+                combined_reward = max(combined_reward, 1.1)
+                final_reward_multiplier = 1
+            elif base_reward == 0:  # 0 indicates a draw
+                combined_reward = base_reward
+                final_reward_multiplier = 0.95  # Slight decrease in score for a draw
+            else:  # -1 indicates a loss
+                combined_reward = base_reward
+                final_reward_multiplier = 0.5
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the rolling average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            print("------------------------------------------------------")
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            # Checkpoint the model every 200 games
+            if self.episode_count % 200 == 0:
+                print("Checkpointing our model...")
+                self.dqn_model.save_model(
+                    DATA_FILE, self.episode_count, avg_reward)
+
+            print("Previous average reward was: ",
+                  self.previous_avg_reward)
+            print("Our rolling-average reward is: ", avg_reward)
+            print("Latest game reward was: ", combined_reward)
+            print("Number of steps were: ", episode_steps)
+            print("Number of in-game model updates: ",
+                  self.in_game_training_iterations)
+            # Backpropagate the final reward to previous actions
+            print("Backpropagating reward updates across",
+                  self.actual_root_level_steps_taken, "root level actions.")
+            self.dqn_model.backpropagate_final_reward(
+                final_reward_multiplier, self.actual_root_level_steps_taken)
+
+            # Learn after every game, not just the successful ones:
+            print("Training the model after game completion...")
+            self.dqn_model.train()
+            # Log our reward over time if we've started training
+            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar(
+                        'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+
+            print("Training complete")
+            # Zero out the remaining, episode-specific counters
+            print("------------------------------------------------------")
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+            self.actual_root_level_steps_taken = 0
+            self.in_game_training_iterations = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+
+            # Original logic, doesn't work properly...
+            # player_y, player_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            # # print("Player x and y are set to: ", self.player_x, self.player_y)
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+            # Using similar approach to Kane AI to Figure out where our home base is
+            # Original reference is here of course:
+            # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+            if obs.first():
+
+                player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                      features.PlayerRelative.SELF).nonzero()
+                xmean = player_x.mean()
+                ymean = player_y.mean()
+
+                if xmean <= 31 and ymean <= 31:
+                    self.base_top_left = True
+                else:
+                    self.base_top_left = False
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # supply_depot_count = int(round(len(depot_y) / 69))
+        all_supply_depots = self.dqn_model.get_units_by_type(
+            obs, units.Terran.SupplyDepot)
+        supply_depot_count = len(
+            [unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        all_barracks = self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks)
+        barracks_count = len(
+            [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+
+        # Find our command centers:
+        # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
+
+        # If we haven't set our global CC yet, add it here
+        if not self.command_center:
+            self.command_center = self.dqn_model.get_units_by_type(
+                obs, units.Terran.CommandCenter)
+
+        # if self.command_center:
+        #     print("Command centers are set to:", self.command_center)
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        # BEGIN CUSTOM CODE
+        enemy_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+        # END CUSTOM CODE
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(13)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = army_supply
+            # Custom State Add-on
+            current_state[4] = len(enemy_units)
+
+            # print("Current state is set to:", current_state)
+            # print("Player data is set to:", obs.observation['player'])
+            # print("Enemy units are: ",  [unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY])
+            # for unit in enemy_units:
+            #     x = unit.x
+            #     y = unit.y
+            #     print(f"Enemy unit of type {unit.unit_type} is at coordinates ({x}, {y})")
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                # print("Hot Squares in current_state",
+                #       i+5, "is set to: ", hot_squares[i])
+                current_state[i + 5] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 9] = green_squares[i]
+
+            # print("State at the end of the step is set to: ", current_state)
+
+            # Push s/a/r/s_next our replay buffer
+            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
+            if self.previous_action is not None:
+                self.actual_root_level_steps_taken += 1
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
+                self.dqn_model.store_transition(self.previous_state,
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, self.base_top_left), current_state)
+
+            # Do in-game training of the model for every 100 root actions it takes:
+            if self.actual_root_level_steps_taken % 100 == 0:
+                # print("Beginning in-game training for the model.")
+                self.in_game_training_iterations += 1
+                self.dqn_model.train()
+
+                # Log our reward over time if we've started training
+                # if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+                #         writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
+
+                # print("Training complete.")
+
+            # this is where we store arbitrary actions which the agent is not allowed to take this game step
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            # Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
+            # 6: 'resetcamera'
+            # 7: 'attack_15_15'
+            # 8: 'attack_15_47'
+            # 9: 'attack_47_15'
+            # 10: 'attack_47_47'
+
+            # Modified, self-generated code to scale supply depot creation
+            if supply_free > 7 or self.supply_delay_timer < 25:
+                excluded_actions.append(1)
+
+            if barracks_count > 4:
+                excluded_actions.append(2)
+
+            # Exclude marinies from the build queue
+            if supply_free == 0 or barracks_count == 0:
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                excluded_actions.append(3)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Length of enemy_units is: ",len(enemy_units))
+            # print(enemy_units)
+            if len(enemy_units) == 0 or self.unit_attack_delay_timer < 4:
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            # SCV Checks
+            if worker_supply > 15 or self.scv_delay_timer < 7:
+                excluded_actions.append(5)
+            # END OF CUSTOM
+
+            # Camera reset handling
+            if self.camera_delay_timer < 10:
+                excluded_actions.append(6)
+
+            # modified original logic, waits for 8 marines before attacking
+            if army_supply < 10 or self.attack_delay_timer < 60:
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+                excluded_actions.append(10)
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN - let the model  select the action
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # print("Our excluded actions for this step are: ", excluded_actions)
+
+            # using reference code for smart action implementation
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+                    
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    else:
+                        print(f"SCV coordinates out of range: {target}")
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                    else:
+                        print(f"Barracks coordinates out of range: {target}")
+
+
+            elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("base is top left: ", self.base_top_left)
+                safe_cc_x, safe_cc_y = self.dqn_model.get_command_center_coordinates(
+                    obs)
+                # print("New target should be: ", safe_cc_x, safe_cc_y)
+
+                if safe_cc_x:
+                    target = safe_cc_x, safe_cc_y
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                
+
+            # To Do:
+            # If get_command_center_coordinates returns None, None
+            # This means we've probably lost our command center and need to build a new one
+            # New action required
+            # Alternative -> Try and repair it if it's taking damage
+
+            # Custom camera reset code
+            # AI can only act on units on its screen (unless using hotkeys/mappings...)
+            # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            # Coordinate flipping isn't ideal for camera movement unfortunately...
+
+            elif smart_action == ACTION_RESET_CAMERA:
+                self.camera_delay_timer = 0
+
+                # # This logic...usually works, but often fails spectacularly.
+                # # Using hardcoded logic for now
+                # # Will revisit later if possible.
+                # if not obs.first() and self.command_center[0].x is not None and self.command_center[0].y is not None:
+                #      # Using unpacking *, do inline transformation
+                #     x, y = self.transformLocation(*self.dqn_model.translate_coordinates(self.command_center[0].x, self.command_center[0].y))
+                #     print("Original CC coordinates were: ", self.command_center[0].x, self.command_center[0].y)
+                #     print("Translated coordinates are: ", x, y)
+                #     return actions.FUNCTIONS.move_camera((x, y))
+
+                # else:
+                #     print("Command Center coordinates not found. Unable to reset camera to home base.")
+                #     # Handle the case where the coordinates are not found, e.g., return a default action
+
+                if self.base_top_left:
+                    return actions.FUNCTIONS.move_camera((23, 25))
+                else:
+                    return actions.FUNCTIONS.move_camera((38, 46))
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("SCV Smart Action Set")
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    # print("Trying to train an SCV")
+                    # Zero out our build timer
+                    self.scv_delay_timer = 0
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+
+            # start of boiler plate code (small modifications like delay timers, that's it)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        print(
+                            "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+
+                        # Reset our timer to 0
+                        self.unit_attack_delay_timer = 0
+
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit ccc0021ee765528b2f733d43d8b09ce53c7ac476
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 24 12:24:05 2023 -0400

    fixed gradient zeroing, changed learning rate and buffer size

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index f1de76d4..eba0645a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-20-b4096.pt'
+DATA_FILE = 'dqn-agent-model-22.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -119,9 +119,9 @@ print("smart_actions is set to: ", smart_actions)
 ################################## End of BoilerPlate Code #####################################################
 
 
-# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play)
+# Custom DQN Agent implementation with a replay buffer of 10M, gamma of 0.95 (hopefully prioritizing longer term play), batch_size (confirmed optimal) of 2048
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.9, buffer_capacity=1000000, batch_size=4096):
+    def __init__(self, actions, state_size, learning_rate=0.005, gamma=0.95, e_greedy=0.9, buffer_capacity=10000000, batch_size=2048):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -129,10 +129,10 @@ class DQNModel(nn.Module):
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
-        self.final_epsilon = 0.10
-        # We decay over 1M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.final_epsilon = 0.05
+        # We decay over 1.5M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 1000000
+            self.epsilon - self.final_epsilon) / 1500000
         self.action_counter = 0
         #
         self.state_size = state_size
@@ -147,9 +147,9 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-agent-v21-b4096'
+        self.writer_path = 'runs/dqn-agent-v22'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 350000
+        self.training_buffer_requirement = 500000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -195,7 +195,6 @@ class DQNModel(nn.Module):
         self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
 
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
-
     def get_writer(self):
         return SummaryWriter(self.writer_path)
     # This is where we store items for our replay buffer
@@ -268,16 +267,11 @@ class DQNModel(nn.Module):
             self.epsilon = max(self.final_epsilon, self.epsilon)
         else:
             q_values = self.model(observation_tensor)
-            # Set our excluded actions for the step to negative infinity, ensuring they're not selected
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected by the model
             for action in excluded_actions:
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
 
-        # Add the action to the replay buffer
-        # deprecated - using full s/a/r/s_next instead now
-        # self.buffer.append(action)
-        # print("Action Selected This Step:", action)
-
         return action
 
     # This is where we train the model
@@ -307,7 +301,7 @@ class DQNModel(nn.Module):
         rewards_np = np.array(rewards, dtype=np.float32)
         next_states_np = np.array(next_states, dtype=np.float32)
 
-        # Convert to tensors
+        # Convert the numpy arrays to tensors (PyTorch complains about doing this itself...)
         states = torch.tensor(states_np).to(self.device)
         actions = torch.tensor(actions_np).to(self.device)
         rewards = torch.tensor(rewards_np).to(self.device)
@@ -327,6 +321,9 @@ class DQNModel(nn.Module):
             # Compute the loss and update the model's weights
             loss = self.loss_fn(q_predict, q_target)
 
+        # Clear accumulated gradients before back propagation
+        self.optimizer.zero_grad()
+
         # Backward pass with scaling
         self.scaler.scale(loss).backward()
 
@@ -339,8 +336,8 @@ class DQNModel(nn.Module):
         # Update the learning rate based on CosineAnnealing scheduling
         self.scheduler.step()
 
-        self.optimizer.zero_grad()
-
+        # Logging various metrics for visualization and debugging
+        # These are separate calls due to a bug - File Descriptors are exhausted if they're not opened/closed individually... :( 
         # Log training loss
         with SummaryWriter(self.writer_path) as writer:
             writer.add_scalar('Loss/train', loss.item(),

commit 59e746aa85ce19aef28dcf949fae223c4bff3aac
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 20 06:58:34 2023 -0400

    bounds crash fix and hyperparameter search for batch_size

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index d2ac4b95..f1de76d4 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -47,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-18.pt'
+DATA_FILE = 'dqn-agent-model-20-b4096.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -121,7 +121,7 @@ print("smart_actions is set to: ", smart_actions)
 
 # Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play)
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.9, buffer_capacity=1000000, batch_size=512):
+    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.9, buffer_capacity=1000000, batch_size=4096):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -147,9 +147,9 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-agent-v20'
+        self.writer_path = 'runs/dqn-agent-v21-b4096'
         # Our replay buffer training theshold size
-        self.training_buffer_requirement = 250000
+        self.training_buffer_requirement = 350000
         # This is our custom reward/action mapping dictionary
         # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
         self.action_rewards = {
@@ -434,19 +434,23 @@ class DQNModel(nn.Module):
         return normalized_reward
 
     # Provide the x/y coordinates of our command center(s)
-
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
             obs, units.Terran.CommandCenter)
 
-        # print("command center is set to:", command_centers)
-
         # If there's a Command Center, return its coordinates
         if command_centers:
             # Grabbing the first for now (no expansion support for cameras)
             command_center = command_centers[0]
-            return command_center.x, command_center.y
+
+            # Checks to avoid out-of-bounds crashing
+            # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+            if 0 <= command_center.x < 84 and 0 <= command_center.y < 84:
+                return command_center.x, command_center.y
+            else:
+                print(f"Command Center coordinates out of range: ({command_center.x}, {command_center.y})")
+                return None, None
         else:
             print("Command Center not found.")
             return None, None
@@ -841,8 +845,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # print("Our excluded actions for this step are: ", excluded_actions)
 
-            # boiler plate again
-
+            # using reference code for smart action implementation
             smart_action, x, y = self.splitAction(self.previous_action)
 
             if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
@@ -851,15 +854,25 @@ class DQNAgent(base_agent.BaseAgent):
                 if unit_y.any():
                     i = random.randint(0, len(unit_y) - 1)
                     target = [unit_x[i], unit_y[i]]
-
-                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                    else:
+                        print(f"SCV coordinates out of range: {target}")
 
             elif smart_action == ACTION_BUILD_MARINE:
                 if barracks_y.any():
                     i = random.randint(0, len(barracks_y) - 1)
                     target = [barracks_x[i], barracks_y[i]]
+                    # Checks to avoid out-of-bounds crashing
+                    # E.g ValueError: Argument is out of range for 2/select_point (6/select_point_act [4]; 0/screen [0, 0]), got: [[2], (-8, 15)]
+                    if 0 <= target[0] < 84 and 0 <= target[1] < 84:
+                        return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                    else:
+                        print(f"Barracks coordinates out of range: {target}")
 
-                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
 
             elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
                 if _SELECT_ARMY in obs.observation['available_actions']:
@@ -876,6 +889,13 @@ class DQNAgent(base_agent.BaseAgent):
                     target = safe_cc_x, safe_cc_y
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                
+
+            # To Do:
+            # If get_command_center_coordinates returns None, None
+            # This means we've probably lost our command center and need to build a new one
+            # New action required
+            # Alternative -> Try and repair it if it's taking damage
 
             # Custom camera reset code
             # AI can only act on units on its screen (unless using hotkeys/mappings...)

commit d64d0caaee23dffceea58c9f548c528ef5d31ea4
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 18 08:22:24 2023 -0400

    More Robust Rewards, Fixed SCV Build Stalls

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index ba8782ef..d2ac4b95 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -147,10 +147,24 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-agent-v19'
+        self.writer_path = 'runs/dqn-agent-v20'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
-
+        # This is our custom reward/action mapping dictionary
+        # This is leveraged within the `get_normalized_reward` function to ensure useful actions are appropriately rewarded
+        self.action_rewards = {
+            0: 0,  # 'donothing'
+            1: 0.25,  # 'buildsupplydepot'
+            2: 0.5,  # 'buildbarracks'
+            3: 0.25,  # 'buildmarine'
+            4: 0.5,  # 'attackunit'
+            5: 0.5,  # 'buildscv'
+            6: 0,  # 'resetcamera'
+            7: 0,  # 'attack_15_15'
+            8: 0,  # 'attack_15_47'
+            9: 0,  # 'attack_47_15'
+            10: 0  # 'attack_47_47'
+        }
 
         # Attempt GPU acceleration
         self.device = torch.device(
@@ -177,11 +191,11 @@ class DQNModel(nn.Module):
         self.loss_fn = nn.MSELoss()
 
         # Our learning rate scheduler is enabled here (CosineAnnealing)
-        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate) 
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate)
         self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
 
-
     # Using a method to handle writes to ensure things are closed properly in the event of a crash
+
     def get_writer(self):
         return SummaryWriter(self.writer_path)
     # This is where we store items for our replay buffer
@@ -189,6 +203,7 @@ class DQNModel(nn.Module):
     # a: The action taken by the agent in state s.
     # r: The reward received after taking action a in state s.
     # s_next: The resulting state after taking
+
     def store_transition(self, s, a, r, s_next):
         # Transition is a tuple (s, a, r, s_next)
         transition = (s, a, r, s_next)
@@ -253,6 +268,7 @@ class DQNModel(nn.Module):
             self.epsilon = max(self.final_epsilon, self.epsilon)
         else:
             q_values = self.model(observation_tensor)
+            # Set our excluded actions for the step to negative infinity, ensuring they're not selected
             for action in excluded_actions:
                 q_values[0][action] = float('-inf')
             action = torch.argmax(q_values).item()
@@ -270,10 +286,10 @@ class DQNModel(nn.Module):
         # Check if the replay buffer has enough samples (using 500K as minimum)
         # If the buffer is too small, we'll overfit. This is essentially the 'warm up' period for the model
         if len(self.buffer) < self.training_buffer_requirement:
-        # if len(self.buffer) < 1000:
+            # if len(self.buffer) < 1000:
             print("Replay buffer is currently too small to conduct training...")
             return
-        
+
         # Increment our training counter
         self.global_training_steps += 1
 
@@ -327,21 +343,25 @@ class DQNModel(nn.Module):
 
         # Log training loss
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
-        
+            writer.add_scalar('Loss/train', loss.item(),
+                              self.global_training_steps)
+
         # Log epsilon over time
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
-        
+            writer.add_scalar('Epsilon/value', self.epsilon,
+                              self.global_training_steps)
+
         # Log our Q Values
         with SummaryWriter(self.writer_path) as writer:
-            writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
-        
+            writer.add_histogram(
+                'Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+
         # Log the histograms of model weights every 100 iterations
-        if self.global_training_steps % 100 == 0:  
+        if self.global_training_steps % 100 == 0:
             with SummaryWriter(self.writer_path) as writer:
                 for name, param in self.model.named_parameters():
-                    writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
+                    writer.add_histogram(name, param.clone().cpu(
+                    ).data.numpy(), self.global_training_steps)
 
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint
@@ -360,17 +380,61 @@ class DQNModel(nn.Module):
         return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
 
     # We identify the current per-step reward based on in-game score and normalize it
-    def get_normalized_reward(self, obs):
+    def get_normalized_reward(self, obs, previous_action, base_top_left):
         # Extract the cumulative score from the observation
         score = obs.observation.score_cumulative.score
-        # Anything about 10k score results in a full score being provided to the model
-        max_score = 10000
+        # Anything about 12k score results in a full score being provided to the model
+        max_score = 12000
         # Normalize the score to be between 0 and 1
         normalized_reward = min(score / max_score, 1)
 
+        # # Action Mapping Reference
+        #     0: 0, # 'donothing'
+        #     1: 0.25, # 'buildsupplydepot'
+        #     2: 0.5, # 'buildbarracks'
+        #     3: 0.25, # 'buildmarine'
+        #     4: 0.5, # 'attackunit'
+        #     5: 0.5, # 'buildscv'
+        #     6: 0, # 'resetcamera'
+        #     7: 0, # 'attack_15_15'
+        #     8: 0, # 'attack_15_47'
+        #     9: 0, # 'attack_47_15'
+        #     10: 0 # 'attack_47_47'
+
+        # If we spawned in the top left, incentivize attacks to opposite quadrant & their expansion
+        # Also de-incentivize attacking home base & expansion
+        if base_top_left:
+            if previous_action == 10:
+                normalized_reward += 0.5
+            elif previous_action == 9:
+                normalized_reward += 0.25
+            elif previous_action == 7:
+                normalized_reward -= 0.5
+            elif previous_action == 8:
+                normalized_reward -= 0.25
+        # Otherwise, we spawned in the bottom right, reward attacks in the top left
+        # Also de-incentivize attacking home base
+        else:
+            if previous_action == 7:
+                normalized_reward += 0.5
+            elif previous_action == 8:
+                normalized_reward += 0.25
+            elif previous_action == 10:
+                normalized_reward -= 0.5
+            elif previous_action == 9:
+                normalized_reward -= 0.25
+
+        # Now, we check if any special actions occurred (like building an SCV, Barracks, marine)
+        action_reward = self.action_rewards.get(previous_action, 0)
+        # Add the action reward, ensuring the total reward stays between 0 and 1
+        normalized_reward = min(max(normalized_reward + action_reward, 0), 1)
+
+        # print("normalized reward is set to: ", normalized_reward, " and previous_action was: ", previous_action)
+
         return normalized_reward
 
     # Provide the x/y coordinates of our command center(s)
+
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
         command_centers = self.get_units_by_type(
@@ -392,9 +456,9 @@ class DQNModel(nn.Module):
         scale_factor = target_size / original_size
         return int(x * scale_factor), int(y * scale_factor)
 
-
 # Agent Implementation
 
+
 class DQNAgent(base_agent.BaseAgent):
 
     def __init__(self):
@@ -536,12 +600,13 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Learn after every game, not just the successful ones:
             print("Training the model after game completion...")
-            self.dqn_model.train()                  
+            self.dqn_model.train()
             # Log our reward over time if we've started training
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
-                    writer.add_scalar('Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
-        
+                    writer.add_scalar(
+                        'Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
+
             print("Training complete")
             # Zero out the remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -689,12 +754,12 @@ class DQNAgent(base_agent.BaseAgent):
             # print("State at the end of the step is set to: ", current_state)
 
             # Push s/a/r/s_next our replay buffer
+            # Per-Step rewards are calculated using the get_normalized_reward function - see it for details
             if self.previous_action is not None:
                 self.actual_root_level_steps_taken += 1
-                # Debugs
-                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action), current_state)
                 self.dqn_model.store_transition(self.previous_state,
-                                                self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs, self.previous_action, self.base_top_left), current_state)
 
             # Do in-game training of the model for every 100 root actions it takes:
             if self.actual_root_level_steps_taken % 100 == 0:
@@ -706,7 +771,7 @@ class DQNAgent(base_agent.BaseAgent):
                 # if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
                 #     with SummaryWriter(self.dqn_model.writer_path) as writer:
                 #         writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
-            
+
                 # print("Training complete.")
 
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
@@ -730,7 +795,7 @@ class DQNAgent(base_agent.BaseAgent):
             # 10: 'attack_47_47'
 
             # Modified, self-generated code to scale supply depot creation
-            if supply_free > 7 or self.supply_delay_timer < 15:
+            if supply_free > 7 or self.supply_delay_timer < 25:
                 excluded_actions.append(1)
 
             if barracks_count > 4:
@@ -802,10 +867,13 @@ class DQNAgent(base_agent.BaseAgent):
 
             # CUSTOM SCV Build Code
             elif smart_action == ACTION_BUILD_SCV:
-                if self.cc_y.any():
-                    i = random.randint(0, len(self.cc_y) - 1)
-                    target = [self.cc_x[i], self.cc_y[i]]
-                    # print("Next step will build an SCV at: ", target)
+                # print("base is top left: ", self.base_top_left)
+                safe_cc_x, safe_cc_y = self.dqn_model.get_command_center_coordinates(
+                    obs)
+                # print("New target should be: ", safe_cc_x, safe_cc_y)
+
+                if safe_cc_x:
+                    target = safe_cc_x, safe_cc_y
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
 
@@ -832,9 +900,9 @@ class DQNAgent(base_agent.BaseAgent):
                 #     # Handle the case where the coordinates are not found, e.g., return a default action
 
                 if self.base_top_left:
-                    return actions.FUNCTIONS.move_camera((25, 25))
+                    return actions.FUNCTIONS.move_camera((23, 25))
                 else:
-                    return actions.FUNCTIONS.move_camera((36, 46))
+                    return actions.FUNCTIONS.move_camera((38, 46))
 
         elif self.move_number == 1:
             self.move_number += 1

commit 0a4c6a09b0342b7b074f1ecc78df2a75fa6b0707
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 17 10:19:10 2023 -0400

    Increased epsilon decay rate, fixed model checkpointing, and implemented fix for file-descriptor issues

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 18b9d2ff..ba8782ef 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -8,10 +8,12 @@
 
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
-# ulimit -n 8192
-# required as after 1k episodes we run out of FD's, will still crash after 8k episodes with:
+# file-descriptor limits need to be artificially raised in /etc/security/limits.conf
+# craig		soft	nofile		8192
+# craig 	hard	nofile		1048576
+# Validated with: ulimit -s -H && ulimit -n -S
+# required as after 1k episodes we run out of FD's:
 # FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
-# If per-epside replay saving is enabled, crashes will occur at 1K regardless of ulimit setting unfortunately
 
 import random
 from random import sample as random_sample
@@ -45,7 +47,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-17.pt'
+DATA_FILE = 'dqn-agent-model-18.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -119,7 +121,7 @@ print("smart_actions is set to: ", smart_actions)
 
 # Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play)
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.75, buffer_capacity=1000000, batch_size=512):
+    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.9, buffer_capacity=1000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -128,9 +130,9 @@ class DQNModel(nn.Module):
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
         self.final_epsilon = 0.10
-        # We decay over 10M actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        # We decay over 1M root-level actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
         self.epsilon_decay_rate = (
-            self.epsilon - self.final_epsilon) / 10000000
+            self.epsilon - self.final_epsilon) / 1000000
         self.action_counter = 0
         #
         self.state_size = state_size
@@ -145,10 +147,11 @@ class DQNModel(nn.Module):
         # Counter for tracking how frequently training is run
         self.global_training_steps = 0
         # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
-        self.writer_path = 'runs/dqn-agent-v17'
+        self.writer_path = 'runs/dqn-agent-v19'
         # Our replay buffer training theshold size
         self.training_buffer_requirement = 250000
 
+
         # Attempt GPU acceleration
         self.device = torch.device(
             "cuda" if torch.cuda.is_available() else "cpu")
@@ -512,9 +515,9 @@ class DQNAgent(base_agent.BaseAgent):
             print("------------------------------------------------------")
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            # Only save if our reward exceeds previous rolling reward by +0.1
-            if avg_reward > self.previous_avg_reward + 0.1:
-                print("Average reward was great enough to checkpoint the model!")
+            # Checkpoint the model every 200 games
+            if self.episode_count % 200 == 0:
+                print("Checkpointing our model...")
                 self.dqn_model.save_model(
                     DATA_FILE, self.episode_count, avg_reward)
 
@@ -537,7 +540,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Log our reward over time if we've started training
             if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
                 with SummaryWriter(self.dqn_model.writer_path) as writer:
-                    writer.add_scalar('Reward/value', combined_reward, self.dqn_model.global_training_steps)
+                    writer.add_scalar('Average Reward/value', avg_reward, self.dqn_model.global_training_steps)
         
             print("Training complete")
             # Zero out the remaining, episode-specific counters
@@ -700,9 +703,9 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.train()
 
                 # Log our reward over time if we've started training
-                if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
-                    with SummaryWriter(self.dqn_model.writer_path) as writer:
-                        writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
+                # if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                #     with SummaryWriter(self.dqn_model.writer_path) as writer:
+                #         writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
             
                 # print("Training complete.")
 

commit 6442c1263a537a1cdd19e258e0f9096d865fffdf
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Aug 16 20:24:35 2023 -0400

    Learning Rate Optimizer, Tensorboard, ReplayBufferMinimum

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index b0896925..18b9d2ff 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -3,31 +3,32 @@
 # The RL Algorithm can properly interact with StarCraft II / PySC2
 # All of the RL algorithms were implemented by me
 
-# Train the agent against a very_easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+# Train the agent against a medium Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty medium --agent2_race terran --nosave_replay
 
 # Train against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran --nosave_replay
 # ulimit -n 8192
 # required as after 1k episodes we run out of FD's, will still crash after 8k episodes with:
 # FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
-
+# If per-epside replay saving is enabled, crashes will occur at 1K regardless of ulimit setting unfortunately
 
 import random
 from random import sample as random_sample
 import math
 import os
+import numpy as np
 import torch
 import torch.nn as nn
 import torch.optim as optim
-# from torchsummary import summary
+# Using TensorBoard for model performance tracking & visualizations
+from torch.utils.tensorboard import SummaryWriter
 # Using amp for mixed-precision (FP16) to improve RTX 4090 performance
 from torch.cuda.amp import autocast, GradScaler
 # Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
-# The static learning rate of 0.01 was...not excellent.
-# from torch.optim.lr_scheduler import CosineAnnealingLR
-import numpy as np
-import pandas as pd
+# The static learning rate of 0.01 was...quite poor excellent.
+from torch.optim.lr_scheduler import CosineAnnealingLR
+
 
 from pysc2.agents import base_agent
 from pysc2.lib import actions
@@ -44,7 +45,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-15.pt'
+DATA_FILE = 'dqn-agent-model-17.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -116,13 +117,13 @@ print("smart_actions is set to: ", smart_actions)
 ################################## End of BoilerPlate Code #####################################################
 
 
-# Custom DQN Agent implementation with a replay buffer of 1M
+# Custom DQN Agent implementation with a replay buffer of 1M, gamma of 0.95 (hopefully prioritizing longer term play)
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.90, buffer_capacity=1000000, batch_size=512):
+    def __init__(self, actions, state_size, learning_rate=0.01, gamma=0.95, e_greedy=0.75, buffer_capacity=1000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
-        self.gamma = reward_decay
+        self.gamma = gamma
         # Using linear epsilon decay to reduce random action probability over time
         self.epsilon = e_greedy
         # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
@@ -141,6 +142,12 @@ class DQNModel(nn.Module):
         self.batch_size = batch_size
         # Mixed Precision
         self.scaler = GradScaler()
+        # Counter for tracking how frequently training is run
+        self.global_training_steps = 0
+        # Using a method for Tensorboard writer to avoid having issues when PySC2 crashes...
+        self.writer_path = 'runs/dqn-agent-v17'
+        # Our replay buffer training theshold size
+        self.training_buffer_requirement = 250000
 
         # Attempt GPU acceleration
         self.device = torch.device(
@@ -166,6 +173,14 @@ class DQNModel(nn.Module):
         self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
+        # Our learning rate scheduler is enabled here (CosineAnnealing)
+        # The goal is to help the model move out of local minima (this happened a lot with the static learning rate) 
+        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=1000000)
+
+
+    # Using a method to handle writes to ensure things are closed properly in the event of a crash
+    def get_writer(self):
+        return SummaryWriter(self.writer_path)
     # This is where we store items for our replay buffer
     # s: The current state of the environment.
     # a: The action taken by the agent in state s.
@@ -249,9 +264,15 @@ class DQNModel(nn.Module):
     # This is where we train the model
     # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
     def train(self):
-        # Check if the replay buffer has enough samples
-        if len(self.buffer) < self.batch_size:
+        # Check if the replay buffer has enough samples (using 500K as minimum)
+        # If the buffer is too small, we'll overfit. This is essentially the 'warm up' period for the model
+        if len(self.buffer) < self.training_buffer_requirement:
+        # if len(self.buffer) < 1000:
+            print("Replay buffer is currently too small to conduct training...")
             return
+        
+        # Increment our training counter
+        self.global_training_steps += 1
 
         # Sample a mini-batch of transitions from the buffer
         transitions = self.random_sample(self.batch_size)
@@ -273,17 +294,19 @@ class DQNModel(nn.Module):
         rewards = torch.tensor(rewards_np).to(self.device)
         next_states = torch.tensor(next_states_np).to(self.device)
 
-        # Compute the Q-values for the current states
-        q_values = self.model(states)
-        q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+        # Using autocast for the forward pass for mixed-precision/FP16 performance improvements
+        with autocast():
+            # Compute the Q-values for the current states
+            q_values = self.model(states)
+            q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
 
-        # Compute the target Q-values
-        next_q_values = self.model(next_states)
-        max_next_q_values = next_q_values.max(1)[0]
-        q_target = rewards + self.gamma * max_next_q_values
+            # Compute the target Q-values
+            next_q_values = self.model(next_states)
+            max_next_q_values = next_q_values.max(1)[0]
+            q_target = rewards + self.gamma * max_next_q_values
 
-        # Compute the loss and update the model's weights
-        loss = self.loss_fn(q_predict, q_target)
+            # Compute the loss and update the model's weights
+            loss = self.loss_fn(q_predict, q_target)
 
         # Backward pass with scaling
         self.scaler.scale(loss).backward()
@@ -294,8 +317,29 @@ class DQNModel(nn.Module):
         # Update the scale for next iteration
         self.scaler.update()
 
+        # Update the learning rate based on CosineAnnealing scheduling
+        self.scheduler.step()
+
         self.optimizer.zero_grad()
 
+        # Log training loss
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_scalar('Loss/train', loss.item(), self.global_training_steps)
+        
+        # Log epsilon over time
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_scalar('Epsilon/value', self.epsilon, self.global_training_steps)
+        
+        # Log our Q Values
+        with SummaryWriter(self.writer_path) as writer:
+            writer.add_histogram('Q-Values', q_values.detach().cpu().numpy(), self.global_training_steps)
+        
+        # Log the histograms of model weights every 100 iterations
+        if self.global_training_steps % 100 == 0:  
+            with SummaryWriter(self.writer_path) as writer:
+                for name, param in self.model.named_parameters():
+                    writer.add_histogram(name, param.clone().cpu().data.numpy(), self.global_training_steps)
+
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint
         save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
@@ -468,7 +512,8 @@ class DQNAgent(base_agent.BaseAgent):
             print("------------------------------------------------------")
 
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
-            if avg_reward > self.previous_avg_reward:
+            # Only save if our reward exceeds previous rolling reward by +0.1
+            if avg_reward > self.previous_avg_reward + 0.1:
                 print("Average reward was great enough to checkpoint the model!")
                 self.dqn_model.save_model(
                     DATA_FILE, self.episode_count, avg_reward)
@@ -488,7 +533,12 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Learn after every game, not just the successful ones:
             print("Training the model after game completion...")
-            self.dqn_model.train()
+            self.dqn_model.train()                  
+            # Log our reward over time if we've started training
+            if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                with SummaryWriter(self.dqn_model.writer_path) as writer:
+                    writer.add_scalar('Reward/value', combined_reward, self.dqn_model.global_training_steps)
+        
             print("Training complete")
             # Zero out the remaining, episode-specific counters
             print("------------------------------------------------------")
@@ -648,6 +698,12 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("Beginning in-game training for the model.")
                 self.in_game_training_iterations += 1
                 self.dqn_model.train()
+
+                # Log our reward over time if we've started training
+                if len(self.dqn_model.buffer) > self.dqn_model.training_buffer_requirement:
+                    with SummaryWriter(self.dqn_model.writer_path) as writer:
+                        writer.add_scalar('Reward/value', self.last_rewards[-1], self.dqn_model.global_training_steps)
+            
                 # print("Training complete.")
 
             # this is where we store arbitrary actions which the agent is not allowed to take this game step

commit 9770ba23d1a1dee45d3bbc475d3441c0de962439
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 14 17:06:29 2023 -0400

    formatting and version update

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 9b690b58..b0896925 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -44,7 +44,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-14.pt'
+DATA_FILE = 'dqn-agent-model-15.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -260,8 +260,8 @@ class DQNModel(nn.Module):
 
         # Convert the zipped values to numpy arrays
         # Library throws a repeated warning if we convert directly to tensors. E.g
-        # train_dqn_agent_v13.py:262: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. 
-        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. 
+        # train_dqn_agent_v13.py:262: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.
+        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.
         states_np = np.array(states, dtype=np.float32)
         actions_np = np.array(actions, dtype=np.int64)
         rewards_np = np.array(rewards, dtype=np.float32)
@@ -296,7 +296,6 @@ class DQNModel(nn.Module):
 
         self.optimizer.zero_grad()
 
-
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint
         save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
@@ -323,31 +322,30 @@ class DQNModel(nn.Module):
         normalized_reward = min(score / max_score, 1)
 
         return normalized_reward
-    
+
     # Provide the x/y coordinates of our command center(s)
     def get_command_center_coordinates(self, obs):
         # Get Command Centers using the get_units_by_type function
-        command_centers = self.get_units_by_type(obs, units.Terran.CommandCenter)
+        command_centers = self.get_units_by_type(
+            obs, units.Terran.CommandCenter)
 
         # print("command center is set to:", command_centers)
 
         # If there's a Command Center, return its coordinates
         if command_centers:
-            command_center = command_centers[0] # Grabbing the first for now (no expansion support for cameras)
+            # Grabbing the first for now (no expansion support for cameras)
+            command_center = command_centers[0]
             return command_center.x, command_center.y
         else:
             print("Command Center not found.")
             return None, None
-        
+
     # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
     def translate_coordinates(self, x, y, original_size=84, target_size=64):
         scale_factor = target_size / original_size
         return int(x * scale_factor), int(y * scale_factor)
 
 
-
-
-
 # Agent Implementation
 
 class DQNAgent(base_agent.BaseAgent):
@@ -428,8 +426,6 @@ class DQNAgent(base_agent.BaseAgent):
 
     # CUSTOM
 
-
-
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -528,15 +524,14 @@ class DQNAgent(base_agent.BaseAgent):
             if obs.first():
 
                 player_y, player_x = (obs.observation.feature_minimap.player_relative ==
-                                        features.PlayerRelative.SELF).nonzero()
+                                      features.PlayerRelative.SELF).nonzero()
                 xmean = player_x.mean()
                 ymean = player_y.mean()
 
                 if xmean <= 31 and ymean <= 31:
                     self.base_top_left = True
                 else:
-                    self.base_top_left = False             
-
+                    self.base_top_left = False
 
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
@@ -545,27 +540,31 @@ class DQNAgent(base_agent.BaseAgent):
 
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # supply_depot_count = int(round(len(depot_y) / 69))
-        all_supply_depots = self.dqn_model.get_units_by_type(obs, units.Terran.SupplyDepot)
-        supply_depot_count = len([unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
+        all_supply_depots = self.dqn_model.get_units_by_type(
+            obs, units.Terran.SupplyDepot)
+        supply_depot_count = len(
+            [unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
 
         barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # barracks_count = int(round(len(barracks_y) / 137))
-        all_barracks = self.dqn_model.get_units_by_type(obs, units.Terran.Barracks)
-        barracks_count = len([unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
-        
+        all_barracks = self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks)
+        barracks_count = len(
+            [unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+
         # Find our command centers:
         # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
         # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
 
         # If we haven't set our global CC yet, add it here
         if not self.command_center:
-            self.command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+            self.command_center = self.dqn_model.get_units_by_type(
+                obs, units.Terran.CommandCenter)
 
         # if self.command_center:
         #     print("Command centers are set to:", self.command_center)
 
-
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
         army_supply = obs.observation['player'][5]
@@ -750,7 +749,7 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Next step will build an SCV at: ", target)
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
-                
+
             # Custom camera reset code
             # AI can only act on units on its screen (unless using hotkeys/mappings...)
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
@@ -774,9 +773,9 @@ class DQNAgent(base_agent.BaseAgent):
                 #     # Handle the case where the coordinates are not found, e.g., return a default action
 
                 if self.base_top_left:
-                    return actions.FUNCTIONS.move_camera((25,25))
+                    return actions.FUNCTIONS.move_camera((25, 25))
                 else:
-                    return actions.FUNCTIONS.move_camera((36,46))
+                    return actions.FUNCTIONS.move_camera((36, 46))
 
         elif self.move_number == 1:
             self.move_number += 1

commit 12e056c53a6f2916a5bf7dc06fb906f0ce5c2b65
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 14 16:55:01 2023 -0400

    PyTorch optimization/fixed perf warning

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 6865b50e..9b690b58 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -258,12 +258,20 @@ class DQNModel(nn.Module):
         # Unzip the transitions into separate variables
         states, actions, rewards, next_states = zip(*transitions)
 
+        # Convert the zipped values to numpy arrays
+        # Library throws a repeated warning if we convert directly to tensors. E.g
+        # train_dqn_agent_v13.py:262: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. 
+        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. 
+        states_np = np.array(states, dtype=np.float32)
+        actions_np = np.array(actions, dtype=np.int64)
+        rewards_np = np.array(rewards, dtype=np.float32)
+        next_states_np = np.array(next_states, dtype=np.float32)
+
         # Convert to tensors
-        states = torch.tensor(states, dtype=torch.float32).to(self.device)
-        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
-        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
-        next_states = torch.tensor(
-            next_states, dtype=torch.float32).to(self.device)
+        states = torch.tensor(states_np).to(self.device)
+        actions = torch.tensor(actions_np).to(self.device)
+        rewards = torch.tensor(rewards_np).to(self.device)
+        next_states = torch.tensor(next_states_np).to(self.device)
 
         # Compute the Q-values for the current states
         q_values = self.model(states)
@@ -288,6 +296,7 @@ class DQNModel(nn.Module):
 
         self.optimizer.zero_grad()
 
+
     def save_model(self, file_path, episode_count, reward):
         # Save our checkpoint
         save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"

commit 4b2bf581ff455f5a0d2762ddd9f25cb0cffe4e4c
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 14 16:50:32 2023 -0400

    made camera reset location static for now

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index e372cf26..6865b50e 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -329,6 +329,12 @@ class DQNModel(nn.Module):
         else:
             print("Command Center not found.")
             return None, None
+        
+    # This function (tries...) to translate from Screen (84x84) -> Minimap (64x64)
+    def translate_coordinates(self, x, y, original_size=84, target_size=64):
+        scale_factor = target_size / original_size
+        return int(x * scale_factor), int(y * scale_factor)
+
 
 
 
@@ -496,15 +502,31 @@ class DQNAgent(base_agent.BaseAgent):
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
         if obs.first():
-            self.player_y, self.player_x = (
-                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            self.base_top_left = 1 if self.player_y.any() and self.player_y.mean() <= 31 else 0
 
-            # print("Player x and y are set to: ", self.player_x, self.player_y)
+            # Original logic, doesn't work properly...
+            # player_y, player_x = (
+            #     obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            # self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            # # print("Player x and y are set to: ", self.player_x, self.player_y)
 
             self.cc_y, self.cc_x = (
                 unit_type == _TERRAN_COMMANDCENTER).nonzero()
-            
+
+            # Using similar approach to Kane AI to Figure out where our home base is
+            # Original reference is here of course:
+            # https://raw.githubusercontent.com/skjb/pysc2-tutorial/master/Build%20a%20Zerg%20Bot/zerg_agent_step7.py
+            if obs.first():
+
+                player_y, player_x = (obs.observation.feature_minimap.player_relative ==
+                                        features.PlayerRelative.SELF).nonzero()
+                xmean = player_x.mean()
+                ymean = player_y.mean()
+
+                if xmean <= 31 and ymean <= 31:
+                    self.base_top_left = True
+                else:
+                    self.base_top_left = False             
 
 
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
@@ -723,18 +745,29 @@ class DQNAgent(base_agent.BaseAgent):
             # Custom camera reset code
             # AI can only act on units on its screen (unless using hotkeys/mappings...)
             # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            # Coordinate flipping isn't ideal for camera movement unfortunately...
+
             elif smart_action == ACTION_RESET_CAMERA:
                 self.camera_delay_timer = 0
 
-                if not obs.first():
-                    if self.command_center[0].x is not None and self.command_center[0].y is not None:
-                        # print("Resetting camera to: ", (self.command_center[0].x, self.command_center[0].y))
-                        return actions.FUNCTIONS.move_camera((self.command_center[0].x, self.command_center[0].y))
-                    else:
-                        print("Command Center coordinates not found. Unable to reset camera to home base.")
-                        # Handle the case where the coordinates are not found, e.g., return a default action
-
-
+                # # This logic...usually works, but often fails spectacularly.
+                # # Using hardcoded logic for now
+                # # Will revisit later if possible.
+                # if not obs.first() and self.command_center[0].x is not None and self.command_center[0].y is not None:
+                #      # Using unpacking *, do inline transformation
+                #     x, y = self.transformLocation(*self.dqn_model.translate_coordinates(self.command_center[0].x, self.command_center[0].y))
+                #     print("Original CC coordinates were: ", self.command_center[0].x, self.command_center[0].y)
+                #     print("Translated coordinates are: ", x, y)
+                #     return actions.FUNCTIONS.move_camera((x, y))
+
+                # else:
+                #     print("Command Center coordinates not found. Unable to reset camera to home base.")
+                #     # Handle the case where the coordinates are not found, e.g., return a default action
+
+                if self.base_top_left:
+                    return actions.FUNCTIONS.move_camera((25,25))
+                else:
+                    return actions.FUNCTIONS.move_camera((36,46))
 
         elif self.move_number == 1:
             self.move_number += 1

commit 699e4d669d1729f8f2310d92edabf06443413e21
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 14 13:16:40 2023 -0400

    new actions, camera reset, and cc location tracking

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 4afa753c..e372cf26 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -44,7 +44,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-13.pt'
+DATA_FILE = 'dqn-agent-model-14.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -88,6 +88,7 @@ ACTION_ATTACK = 'attack'
 # CUSTOM
 ACTION_BUILD_SCV = 'buildscv'
 ACTION_ATTACK_UNIT = 'attackunit'
+ACTION_RESET_CAMERA = 'resetcamera'
 # END OF CUSTOM
 
 smart_actions = [
@@ -97,6 +98,7 @@ smart_actions = [
     ACTION_BUILD_MARINE,
     ACTION_ATTACK_UNIT,
     ACTION_BUILD_SCV,
+    ACTION_RESET_CAMERA,
 ]
 
 # DQN State size
@@ -312,6 +314,23 @@ class DQNModel(nn.Module):
         normalized_reward = min(score / max_score, 1)
 
         return normalized_reward
+    
+    # Provide the x/y coordinates of our command center(s)
+    def get_command_center_coordinates(self, obs):
+        # Get Command Centers using the get_units_by_type function
+        command_centers = self.get_units_by_type(obs, units.Terran.CommandCenter)
+
+        # print("command center is set to:", command_centers)
+
+        # If there's a Command Center, return its coordinates
+        if command_centers:
+            command_center = command_centers[0] # Grabbing the first for now (no expansion support for cameras)
+            return command_center.x, command_center.y
+        else:
+            print("Command Center not found.")
+            return None, None
+
+
 
 
 # Agent Implementation
@@ -350,6 +369,9 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
         self.scv_delay_timer = 0
+        self.camera_delay_timer = 0
+
+        self.command_center = []
 
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
@@ -391,6 +413,8 @@ class DQNAgent(base_agent.BaseAgent):
 
     # CUSTOM
 
+
+
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
@@ -398,6 +422,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.unit_attack_delay_timer += 1
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
+        self.camera_delay_timer += 1
 
         # Check our current score, just for debugging
         # print("Current score is: ", self.get_normalized_reward(obs))
@@ -471,12 +496,16 @@ class DQNAgent(base_agent.BaseAgent):
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
         if obs.first():
-            player_y, player_x = (
+            self.player_y, self.player_x = (
                 obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
-            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+            self.base_top_left = 1 if self.player_y.any() and self.player_y.mean() <= 31 else 0
+
+            # print("Player x and y are set to: ", self.player_x, self.player_y)
 
             self.cc_y, self.cc_x = (
                 unit_type == _TERRAN_COMMANDCENTER).nonzero()
+            
+
 
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
@@ -485,14 +514,26 @@ class DQNAgent(base_agent.BaseAgent):
 
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # supply_depot_count = int(round(len(depot_y) / 69))
-        supply_depot_count = len(
-            self.dqn_model.get_units_by_type(obs, units.Terran.SupplyDepot))
+        all_supply_depots = self.dqn_model.get_units_by_type(obs, units.Terran.SupplyDepot)
+        supply_depot_count = len([unit for unit in all_supply_depots if unit.alliance == features.PlayerRelative.SELF])
 
         barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
         # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
         # barracks_count = int(round(len(barracks_y) / 137))
-        barracks_count = len(self.dqn_model.get_units_by_type(
-            obs, units.Terran.Barracks))
+        all_barracks = self.dqn_model.get_units_by_type(obs, units.Terran.Barracks)
+        barracks_count = len([unit for unit in all_barracks if unit.alliance == features.PlayerRelative.SELF])
+        
+        # Find our command centers:
+        # command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+        # friendly_command_centers = [unit for unit in command_centers if unit.alliance == features.PlayerRelative.SELF]
+
+        # If we haven't set our global CC yet, add it here
+        if not self.command_center:
+            self.command_center = self.dqn_model.get_units_by_type(obs, units.Terran.CommandCenter)
+
+        # if self.command_center:
+        #     print("Command centers are set to:", self.command_center)
+
 
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
@@ -593,13 +634,14 @@ class DQNAgent(base_agent.BaseAgent):
             # 3: 'buildmarine'
             # 4: 'attackunit'
             # 5: 'buildscv'
-            # 6: 'attack_15_15'
-            # 7: 'attack_15_47'
-            # 8: 'attack_47_15'
-            # 9: 'attack_47_47'
+            # 6: 'resetcamera'
+            # 7: 'attack_15_15'
+            # 8: 'attack_15_47'
+            # 9: 'attack_47_15'
+            # 10: 'attack_47_47'
 
             # Modified, self-generated code to scale supply depot creation
-            if supply_free > 7 or self.supply_delay_timer < 10:
+            if supply_free > 7 or self.supply_delay_timer < 15:
                 excluded_actions.append(1)
 
             if barracks_count > 4:
@@ -623,12 +665,16 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(5)
             # END OF CUSTOM
 
+            # Camera reset handling
+            if self.camera_delay_timer < 10:
+                excluded_actions.append(6)
+
             # modified original logic, waits for 8 marines before attacking
             if army_supply < 10 or self.attack_delay_timer < 60:
-                excluded_actions.append(6)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
                 excluded_actions.append(9)
+                excluded_actions.append(10)
 
             # print("Our exclusions are set to: ", excluded_actions)
 
@@ -673,6 +719,22 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Next step will build an SCV at: ", target)
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                
+            # Custom camera reset code
+            # AI can only act on units on its screen (unless using hotkeys/mappings...)
+            # This avoids annoying stalls after moving the camera to attack a specific unit on the Screen
+            elif smart_action == ACTION_RESET_CAMERA:
+                self.camera_delay_timer = 0
+
+                if not obs.first():
+                    if self.command_center[0].x is not None and self.command_center[0].y is not None:
+                        # print("Resetting camera to: ", (self.command_center[0].x, self.command_center[0].y))
+                        return actions.FUNCTIONS.move_camera((self.command_center[0].x, self.command_center[0].y))
+                    else:
+                        print("Command Center coordinates not found. Unable to reset camera to home base.")
+                        # Handle the case where the coordinates are not found, e.g., return a default action
+
+
 
         elif self.move_number == 1:
             self.move_number += 1

commit bc8effc432552d2c2618e5180a78963a91241f91
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 14 10:50:18 2023 -0400

    Added delay timer, reduced batch_size, and some formatting changes

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 7c9f858f..4afa753c 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -8,8 +8,9 @@
 
 # Train against Kane-AI with this string:
 # python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
-# ulimit -n 4096
-# required as after 1k episodes we run out of FD's
+# ulimit -n 8192
+# required as after 1k episodes we run out of FD's, will still crash after 8k episodes with:
+# FILE OPEN ERROR: for /.../StarCraft II/2023-08-14 02T45T30.000.test: Too many open files
 
 
 import random
@@ -115,7 +116,7 @@ print("smart_actions is set to: ", smart_actions)
 
 # Custom DQN Agent implementation with a replay buffer of 1M
 class DQNModel(nn.Module):
-    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.90, buffer_capacity=1000000, batch_size=8192):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.90, buffer_capacity=1000000, batch_size=512):
         super(DQNModel, self).__init__()
         self.actions = actions
         self.lr = learning_rate
@@ -197,8 +198,7 @@ class DQNModel(nn.Module):
             self.buffer[i] = (s, a, new_reward, s_next)
             # print("After backpropagation: ", self.buffer[i][2])
 
-        print("Replay buffer currently has: ", len(self.buffer), "entries") 
-
+        print("Replay buffer currently has: ", len(self.buffer), "entries")
 
     # # sample transitions from replay buffer queue for the last game
     # def sample(self, batch_size):
@@ -318,7 +318,6 @@ class DQNModel(nn.Module):
 
 class DQNAgent(base_agent.BaseAgent):
 
-
     def __init__(self):
         super(DQNAgent, self).__init__()
 
@@ -348,6 +347,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         # Custom Delay Timers
         self.attack_delay_timer = 0
+        self.unit_attack_delay_timer = 0
         self.supply_delay_timer = 0
         self.scv_delay_timer = 0
 
@@ -395,6 +395,7 @@ class DQNAgent(base_agent.BaseAgent):
         super(DQNAgent, self).step(obs)
         # Using delay timers to avoid duplicate commands being issued by the AI
         self.attack_delay_timer += 1
+        self.unit_attack_delay_timer += 1
         self.supply_delay_timer += 1
         self.scv_delay_timer += 1
 
@@ -414,41 +415,47 @@ class DQNAgent(base_agent.BaseAgent):
                 step_penalty = extra_steps // 1000 * 0.1
                 step_reward = 1.0 - step_penalty
                 combined_reward = base_reward + step_reward
-                combined_reward = max(combined_reward, 1.1)  # Ensure combined_reward never goes below 1.1 for a win
+                # Ensure combined_reward never goes below 1.1 for a win
+                combined_reward = max(combined_reward, 1.1)
                 final_reward_multiplier = 1
             elif base_reward == 0:  # 0 indicates a draw
                 combined_reward = base_reward
-                final_reward_multiplier = 0.95 # Slight decrease in score for a draw
+                final_reward_multiplier = 0.95  # Slight decrease in score for a draw
             else:  # -1 indicates a loss
                 combined_reward = base_reward
                 final_reward_multiplier = 0.5
 
             self.last_rewards.append(combined_reward)  # Add the latest reward
-            # Calculate the average reward
+            # Calculate the rolling average reward
             avg_reward = sum(self.last_rewards) / len(self.last_rewards)
 
+            print("------------------------------------------------------")
+
             # Optimal reward is '2' (perfect string of wins at 10K steps or less)
             if avg_reward > self.previous_avg_reward:
-                print("------------------------------------------------------")
                 print("Average reward was great enough to checkpoint the model!")
-                print("Previous average reward was: ", self.previous_avg_reward)
-                print("Our average reward is: ", avg_reward)
-                print("Latest game reward was: ", combined_reward)
-                print("Number of steps were: ", episode_steps)
-                print("Number of in-game model updates: ", self.in_game_training_iterations)
-                print("------------------------------------------------------")
-
-                self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
-
+                self.dqn_model.save_model(
+                    DATA_FILE, self.episode_count, avg_reward)
+
+            print("Previous average reward was: ",
+                  self.previous_avg_reward)
+            print("Our rolling-average reward is: ", avg_reward)
+            print("Latest game reward was: ", combined_reward)
+            print("Number of steps were: ", episode_steps)
+            print("Number of in-game model updates: ",
+                  self.in_game_training_iterations)
             # Backpropagate the final reward to previous actions
-            print("Backpropagating reward updates across", self.actual_root_level_steps_taken, "root level actions.")
-            self.dqn_model.backpropagate_final_reward(final_reward_multiplier, self.actual_root_level_steps_taken)
+            print("Backpropagating reward updates across",
+                  self.actual_root_level_steps_taken, "root level actions.")
+            self.dqn_model.backpropagate_final_reward(
+                final_reward_multiplier, self.actual_root_level_steps_taken)
 
             # Learn after every game, not just the successful ones:
-            print("Training the model after game completion")
+            print("Training the model after game completion...")
             self.dqn_model.train()
             print("Training complete")
             # Zero out the remaining, episode-specific counters
+            print("------------------------------------------------------")
             self.previous_avg_reward = avg_reward
             self.previous_action = None
             self.previous_state = None
@@ -458,7 +465,6 @@ class DQNAgent(base_agent.BaseAgent):
 
             return actions.FunctionCall(_NO_OP, [])
 
-
         # BOILER PLATE Action-Space Guardrails
         # Used as a way of limiting the potential action space at the beginning of the game for the agent
 
@@ -566,7 +572,6 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.store_transition(self.previous_state,
                                                 self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
 
-
             # Do in-game training of the model for every 100 root actions it takes:
             if self.actual_root_level_steps_taken % 100 == 0:
                 # print("Beginning in-game training for the model.")
@@ -609,7 +614,7 @@ class DQNAgent(base_agent.BaseAgent):
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
             # print("Length of enemy_units is: ",len(enemy_units))
             # print(enemy_units)
-            if len(enemy_units) == 0:
+            if len(enemy_units) == 0 or self.unit_attack_delay_timer < 4:
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
@@ -784,6 +789,10 @@ class DQNAgent(base_agent.BaseAgent):
                         # Move the camera to the clamped coordinates
                         print(
                             "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+
+                        # Reset our timer to 0
+                        self.unit_attack_delay_timer = 0
+
                         return actions.FUNCTIONS.move_camera((target_x, target_y))
 
         elif self.move_number == 2:

commit 9c8d92582be5b7dca16f9cbfdab20fb1135f0083
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 18:08:13 2023 -0400

    reduced in-game training console spam

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 84bd6e0f..7c9f858f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -344,6 +344,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.episode_count = 0
         self.previous_avg_reward = 0
         self.actual_root_level_steps_taken = 0
+        self.in_game_training_iterations = 0
 
         # Custom Delay Timers
         self.attack_delay_timer = 0
@@ -434,6 +435,7 @@ class DQNAgent(base_agent.BaseAgent):
                 print("Our average reward is: ", avg_reward)
                 print("Latest game reward was: ", combined_reward)
                 print("Number of steps were: ", episode_steps)
+                print("Number of in-game model updates: ", self.in_game_training_iterations)
                 print("------------------------------------------------------")
 
                 self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
@@ -452,6 +454,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.previous_state = None
             self.move_number = 0
             self.actual_root_level_steps_taken = 0
+            self.in_game_training_iterations = 0
 
             return actions.FunctionCall(_NO_OP, [])
 
@@ -566,9 +569,10 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Do in-game training of the model for every 100 root actions it takes:
             if self.actual_root_level_steps_taken % 100 == 0:
-                print("Beginning in-game training for the model.")
+                # print("Beginning in-game training for the model.")
+                self.in_game_training_iterations += 1
                 self.dqn_model.train()
-                print("Training complete.")
+                # print("Training complete.")
 
             # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []

commit a6aee434e4443805b11e29f18ea71bb7d64d6ce5
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 18:02:53 2023 -0400

    added in-game training and fixed backprop issues again

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index d98096b2..84bd6e0f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -177,9 +177,9 @@ class DQNModel(nn.Module):
     # This function goes back and tweaks the rewards associated with a given game
     # Based on the final tangible reward we get from PySC2
     # Win/loss/draw are multipliers
-    def backpropagate_final_reward(self, final_reward, episode_steps):
+    def backpropagate_final_reward(self, final_reward, root_actions_taken_last_game):
         # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
-        steps_to_iterate = min(episode_steps, len(self.buffer))
+        steps_to_iterate = min(root_actions_taken_last_game, len(self.buffer))
 
         # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
         for i in range(-steps_to_iterate, 0):
@@ -439,11 +439,13 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
 
             # Backpropagate the final reward to previous actions
-            print("Backpropagating reward updates across ", self.actual_root_level_steps_taken, "root level actions.")
+            print("Backpropagating reward updates across", self.actual_root_level_steps_taken, "root level actions.")
             self.dqn_model.backpropagate_final_reward(final_reward_multiplier, self.actual_root_level_steps_taken)
 
             # Learn after every game, not just the successful ones:
+            print("Training the model after game completion")
             self.dqn_model.train()
+            print("Training complete")
             # Zero out the remaining, episode-specific counters
             self.previous_avg_reward = avg_reward
             self.previous_action = None
@@ -561,6 +563,14 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.store_transition(self.previous_state,
                                                 self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
 
+
+            # Do in-game training of the model for every 100 root actions it takes:
+            if self.actual_root_level_steps_taken % 100 == 0:
+                print("Beginning in-game training for the model.")
+                self.dqn_model.train()
+                print("Training complete.")
+
+            # this is where we store arbitrary actions which the agent is not allowed to take this game step
             excluded_actions = []
             # print("excluded_actions at the start are set to: ", excluded_actions)
             # Original Code:

commit 4f7939009df0c5e28c50fca6c133b0bca1c40312
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 17:52:21 2023 -0400

    fixed backpropagation logic

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 3b5f96fd..d98096b2 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -324,7 +324,7 @@ class DQNAgent(base_agent.BaseAgent):
 
         initial_actions = list(range(len(smart_actions)))
         print("Creating the model with the following attributes:")
-        print("Actions are set to:", initial_actions)
+        print("Available actions are set to:", initial_actions)
         print("State Size:", STATE_SIZE)
 
         self.dqn_model = DQNModel(
@@ -343,6 +343,7 @@ class DQNAgent(base_agent.BaseAgent):
         self.last_rewards = deque([0] * 100, maxlen=100)
         self.episode_count = 0
         self.previous_avg_reward = 0
+        self.actual_root_level_steps_taken = 0
 
         # Custom Delay Timers
         self.attack_delay_timer = 0
@@ -438,7 +439,8 @@ class DQNAgent(base_agent.BaseAgent):
                 self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
 
             # Backpropagate the final reward to previous actions
-            self.dqn_model.backpropagate_final_reward(final_reward_multiplier, episode_steps)
+            print("Backpropagating reward updates across ", self.actual_root_level_steps_taken, "root level actions.")
+            self.dqn_model.backpropagate_final_reward(final_reward_multiplier, self.actual_root_level_steps_taken)
 
             # Learn after every game, not just the successful ones:
             self.dqn_model.train()
@@ -447,6 +449,7 @@ class DQNAgent(base_agent.BaseAgent):
             self.previous_action = None
             self.previous_state = None
             self.move_number = 0
+            self.actual_root_level_steps_taken = 0
 
             return actions.FunctionCall(_NO_OP, [])
 
@@ -552,6 +555,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Push s/a/r/s_next our replay buffer
             if self.previous_action is not None:
+                self.actual_root_level_steps_taken += 1
                 # Debugs
                 # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
                 self.dqn_model.store_transition(self.previous_state,

commit c29055526c25aa3df45fa22ed55637544960e895
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 17:48:34 2023 -0400

    modified win multiplier slightly

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
index 9c3a862e..3b5f96fd 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -191,9 +191,13 @@ class DQNModel(nn.Module):
 
             # Modify the reward to include the final reward
             new_reward = r * final_reward
+            # print("Before backpropagation: ", r)
 
             # Replace the transition in the replay buffer
             self.buffer[i] = (s, a, new_reward, s_next)
+            # print("After backpropagation: ", self.buffer[i][2])
+
+        print("Replay buffer currently has: ", len(self.buffer), "entries") 
 
 
     # # sample transitions from replay buffer queue for the last game
@@ -408,10 +412,11 @@ class DQNAgent(base_agent.BaseAgent):
                 step_penalty = extra_steps // 1000 * 0.1
                 step_reward = 1.0 - step_penalty
                 combined_reward = base_reward + step_reward
+                combined_reward = max(combined_reward, 1.1)  # Ensure combined_reward never goes below 1.1 for a win
                 final_reward_multiplier = 1
             elif base_reward == 0:  # 0 indicates a draw
                 combined_reward = base_reward
-                final_reward_multiplier = 1
+                final_reward_multiplier = 0.95 # Slight decrease in score for a draw
             else:  # -1 indicates a loss
                 combined_reward = base_reward
                 final_reward_multiplier = 0.5
@@ -435,7 +440,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Backpropagate the final reward to previous actions
             self.dqn_model.backpropagate_final_reward(final_reward_multiplier, episode_steps)
 
-            # Learn on every game, not just the successful ones:
+            # Learn after every game, not just the successful ones:
             self.dqn_model.train()
             # Zero out the remaining, episode-specific counters
             self.previous_avg_reward = avg_reward

commit d228beadeb60492dbc657b72a0d1d0696513c05a
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 17:39:41 2023 -0400

    Added V13 with a (finally!) working replay buffer

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
new file mode 100644
index 00000000..9c3a862e
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v13.py
@@ -0,0 +1,788 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Train the agent against a very_easy Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+
+# Train against Kane-AI with this string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v13.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
+# ulimit -n 4096
+# required as after 1k episodes we run out of FD's
+
+
+import random
+from random import sample as random_sample
+import math
+import os
+import torch
+import torch.nn as nn
+import torch.optim as optim
+# from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...not excellent.
+# from torch.optim.lr_scheduler import CosineAnnealingLR
+import numpy as np
+import pandas as pd
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+from pysc2.lib import units
+# queue used for replay buffer
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-agent-model-13.pt'
+
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+# CUSTOM
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+# END CUSTOM
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+# CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+# END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+]
+
+# DQN State size
+STATE_SIZE = 13
+
+# Steven Brown's implementation for spawn-location-agnostic quadrant attacks
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code #####################################################
+
+
+# Custom DQN Agent implementation with a replay buffer of 1M
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.90, buffer_capacity=1000000, batch_size=8192):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.10
+        # We decay over 10M actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (
+            self.epsilon - self.final_epsilon) / 10000000
+        self.action_counter = 0
+        #
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque(maxlen=buffer_capacity)
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 2048),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(2048, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    # This is where we store items for our replay buffer
+    # s: The current state of the environment.
+    # a: The action taken by the agent in state s.
+    # r: The reward received after taking action a in state s.
+    # s_next: The resulting state after taking
+    def store_transition(self, s, a, r, s_next):
+        # Transition is a tuple (s, a, r, s_next)
+        transition = (s, a, r, s_next)
+        # Append the transition to the replay buffer
+        self.buffer.append(transition)
+
+    # This function goes back and tweaks the rewards associated with a given game
+    # Based on the final tangible reward we get from PySC2
+    # Win/loss/draw are multipliers
+    def backpropagate_final_reward(self, final_reward, episode_steps):
+        # Calculate the number of steps to iterate over, limited by the length of the buffer to ensure safety
+        steps_to_iterate = min(episode_steps, len(self.buffer))
+
+        # Iterate over the last steps_to_iterate in the replay buffer/queue...in reverse order (newest to oldest)
+        for i in range(-steps_to_iterate, 0):
+            # Skip if the buffer index result is None
+            if self.buffer[i] is None:
+                continue
+            # Fetch the transition
+            s, a, r, s_next = self.buffer[i]
+
+            # Modify the reward to include the final reward
+            new_reward = r * final_reward
+
+            # Replace the transition in the replay buffer
+            self.buffer[i] = (s, a, new_reward, s_next)
+
+
+    # # sample transitions from replay buffer queue for the last game
+    # def sample(self, batch_size):
+    #     # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+    #     return list(self.buffer)[-batch_size:]
+
+    # Sample random transitions from replay buffer
+    # Hopefully in a stochastic manner...
+
+    def random_sample(self, batch_size):
+        return random_sample(self.buffer, batch_size)
+
+    def forward(self, x):
+        return self.model(x)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Algorithm that Steven Brown (PySC2 Dev) created
+    # It's visible here
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
+    def choose_action(self, observation, excluded_actions=[]):
+        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+            # This is where we keep the logic for epsilon decay (linear)
+            self.action_counter += 1
+            self.epsilon -= self.epsilon_decay_rate
+            self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+            q_values = self.model(observation_tensor)
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+        # Add the action to the replay buffer
+        # deprecated - using full s/a/r/s_next instead now
+        # self.buffer.append(action)
+        # print("Action Selected This Step:", action)
+
+        return action
+
+    # This is where we train the model
+    # It samples randomly from the replay buffer - relatively simplistic but seemingly effective!
+    def train(self):
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        transitions = self.random_sample(self.batch_size)
+        # Unzip the transitions into separate variables
+        states, actions, rewards, next_states = zip(*transitions)
+
+        # Convert to tensors
+        states = torch.tensor(states, dtype=torch.float32).to(self.device)
+        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
+        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
+        next_states = torch.tensor(
+            next_states, dtype=torch.float32).to(self.device)
+
+        # Compute the Q-values for the current states
+        q_values = self.model(states)
+        q_predict = q_values.gather(1, actions.unsqueeze(1)).squeeze()
+
+        # Compute the target Q-values
+        next_q_values = self.model(next_states)
+        max_next_q_values = next_q_values.max(1)[0]
+        q_target = rewards + self.gamma * max_next_q_values
+
+        # Compute the loss and update the model's weights
+        loss = self.loss_fn(q_predict, q_target)
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
+    # We identify the current per-step reward based on in-game score and normalize it
+    def get_normalized_reward(self, obs):
+        # Extract the cumulative score from the observation
+        score = obs.observation.score_cumulative.score
+        # Anything about 10k score results in a full score being provided to the model
+        max_score = 10000
+        # Normalize the score to be between 0 and 1
+        normalized_reward = min(score / max_score, 1)
+
+        return normalized_reward
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+
+
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.supply_delay_timer = 0
+        self.scv_delay_timer = 0
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.supply_delay_timer += 1
+        self.scv_delay_timer += 1
+
+        # Check our current score, just for debugging
+        # print("Current score is: ", self.get_normalized_reward(obs))
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+                final_reward_multiplier = 1
+            elif base_reward == 0:  # 0 indicates a draw
+                combined_reward = base_reward
+                final_reward_multiplier = 1
+            else:  # -1 indicates a loss
+                combined_reward = base_reward
+                final_reward_multiplier = 0.5
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to checkpoint the model!")
+                print("Previous average reward was: ", self.previous_avg_reward)
+                print("Our average reward is: ", avg_reward)
+                print("Latest game reward was: ", combined_reward)
+                print("Number of steps were: ", episode_steps)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
+
+            # Backpropagate the final reward to previous actions
+            self.dqn_model.backpropagate_final_reward(final_reward_multiplier, episode_steps)
+
+            # Learn on every game, not just the successful ones:
+            self.dqn_model.train()
+            # Zero out the remaining, episode-specific counters
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # supply_depot_count = int(round(len(depot_y) / 69))
+        supply_depot_count = len(
+            self.dqn_model.get_units_by_type(obs, units.Terran.SupplyDepot))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        barracks_count = len(self.dqn_model.get_units_by_type(
+            obs, units.Terran.Barracks))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        # BEGIN CUSTOM CODE
+        enemy_units = [
+            unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+        # END CUSTOM CODE
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(13)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = army_supply
+            # Custom State Add-on
+            current_state[4] = len(enemy_units)
+
+            # print("Current state is set to:", current_state)
+            # print("Player data is set to:", obs.observation['player'])
+            # print("Enemy units are: ",  [unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY])
+            # for unit in enemy_units:
+            #     x = unit.x
+            #     y = unit.y
+            #     print(f"Enemy unit of type {unit.unit_type} is at coordinates ({x}, {y})")
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                # print("Hot Squares in current_state",
+                #       i+5, "is set to: ", hot_squares[i])
+                current_state[i + 5] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 9] = green_squares[i]
+
+            # print("State at the end of the step is set to: ", current_state)
+
+            # Push s/a/r/s_next our replay buffer
+            if self.previous_action is not None:
+                # Debugs
+                # print("Pushing to the replay buffer: ", self.previous_state,self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
+                self.dqn_model.store_transition(self.previous_state,
+                                                self.previous_action, self.dqn_model.get_normalized_reward(obs), current_state)
+
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            # Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
+            # 6: 'attack_15_15'
+            # 7: 'attack_15_47'
+            # 8: 'attack_47_15'
+            # 9: 'attack_47_47'
+
+            # Modified, self-generated code to scale supply depot creation
+            if supply_free > 7 or self.supply_delay_timer < 10:
+                excluded_actions.append(1)
+
+            if barracks_count > 4:
+                excluded_actions.append(2)
+
+            # Exclude marinies from the build queue
+            if supply_free == 0 or barracks_count == 0:
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                excluded_actions.append(3)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Length of enemy_units is: ",len(enemy_units))
+            # print(enemy_units)
+            if len(enemy_units) == 0:
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            # SCV Checks
+            if worker_supply > 15 or self.scv_delay_timer < 7:
+                excluded_actions.append(5)
+            # END OF CUSTOM
+
+            # modified original logic, waits for 8 marines before attacking
+            if army_supply < 10 or self.attack_delay_timer < 60:
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN - let the model  select the action
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # print("Our excluded actions for this step are: ", excluded_actions)
+
+            # boiler plate again
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK or smart_action == ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                if self.cc_y.any():
+                    i = random.randint(0, len(self.cc_y) - 1)
+                    target = [self.cc_x[i], self.cc_y[i]]
+                    # print("Next step will build an SCV at: ", target)
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                # print("SCV Smart Action Set")
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    # print("Trying to train an SCV")
+                    # Zero out our build timer
+                    self.scv_delay_timer = 0
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+
+            # start of boiler plate code (small modifications like delay timers, that's it)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        print(
+                            "Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit 9523613fe269bdd1cf676677dfa3cb56f0759535
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 16:29:16 2023 -0400

    increased state size to 13 to fix bug

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
index 7b8b7765..c569084a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
@@ -30,6 +30,7 @@ import pandas as pd
 from pysc2.agents import base_agent
 from pysc2.lib import actions
 from pysc2.lib import features
+from pysc2.lib import units
 # queue used for replay buffer
 from collections import deque
 
@@ -41,7 +42,7 @@ from collections import deque
 
 ####### CUSTOM CODE ######
 _TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
-DATA_FILE = 'dqn-agent-model-12.pt'
+DATA_FILE = 'dqn-agent-model-13.pt'
 
 ###### END OF GLOBAL CUSTOM CODE #####
 
@@ -97,7 +98,7 @@ smart_actions = [
 ]
 
 # DQN State size
-STATE_SIZE = 12
+STATE_SIZE = 13
 
 # Steven Brown's implementation for spawn-location-agnostic quadrant attacks
 for mm_x in range(0, 64):
@@ -162,7 +163,7 @@ class DQNModel(nn.Module):
 
     # sample transitions from replay buffer queue for the last game
     def sample(self, batch_size):
-        print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+        # print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
         return list(self.buffer)[-batch_size:]
 
 
@@ -194,6 +195,7 @@ class DQNModel(nn.Module):
 
         # Add the action to the replay buffer
         self.buffer.append(action)
+        # print("Action Selected This Step:", action)
 
         return action
 
@@ -254,9 +256,14 @@ class DQNModel(nn.Module):
         self.model.load_state_dict(torch.load(file_path))
 
 
+
 # Agent Implementation
 
 class DQNAgent(base_agent.BaseAgent):
+    # This function identifies all units of a specific desired type
+    def get_units_by_type(self, obs, unit_type):
+        return [unit for unit in obs.observation.feature_units if unit.unit_type == unit_type]
+
     def __init__(self):
         super(DQNAgent, self).__init__()
 
@@ -395,12 +402,16 @@ class DQNAgent(base_agent.BaseAgent):
         cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
         cc_count = 1 if cc_y.any() else 0
 
-        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
-        supply_depot_count = int(round(len(depot_y) / 69))
+        # depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
 
-        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
-        barracks_count = int(round(len(barracks_y) / 137))
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # supply_depot_count = int(round(len(depot_y) / 69))
+        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))
 
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        # Original barracks_count and supply_depot_count from reference code had flaws when y was shared
+        # barracks_count = int(round(len(barracks_y) / 137))
+        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))
 
         supply_used = obs.observation['player'][3]
         supply_limit = obs.observation['player'][4]
@@ -415,7 +426,7 @@ class DQNAgent(base_agent.BaseAgent):
         if self.move_number == 0:
             self.move_number += 1
 
-            current_state = np.zeros(12)
+            current_state = np.zeros(13)
             current_state[0] = cc_count
             current_state[1] = supply_depot_count
             current_state[2] = barracks_count
@@ -447,7 +458,8 @@ class DQNAgent(base_agent.BaseAgent):
                 hot_squares = hot_squares[::-1]
 
             for i in range(0, 4):
-                current_state[i + 4] = hot_squares[i]
+                print("Hot Squares in current_state",i+5, "is set to: ", hot_squares[i])
+                current_state[i + 5] = hot_squares[i]
 
             green_squares = np.zeros(4)
             friendly_y, friendly_x = (
@@ -465,7 +477,9 @@ class DQNAgent(base_agent.BaseAgent):
                 green_squares = green_squares[::-1]
 
             for i in range(0, 4):
-                current_state[i + 8] = green_squares[i]
+                current_state[i + 9] = green_squares[i]
+
+            # print("State at the end of the step is set to: ", current_state)
 
             # Updated to DQN implementation
             # if self.previous_action is not None:
@@ -491,7 +505,7 @@ class DQNAgent(base_agent.BaseAgent):
             # 9: 'attack_47_47'
 
             # Modified, self-generated code to scale supply depot creation
-            if supply_free > 6 or self.supply_delay_timer < 10:
+            if supply_free > 7 or self.supply_delay_timer < 10:
                 excluded_actions.append(1)
 
             if barracks_count > 4:
@@ -499,7 +513,7 @@ class DQNAgent(base_agent.BaseAgent):
 
             # Exclude marinies from the build queue
             if supply_free == 0 or barracks_count == 0:
-                # print("Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
+                # print("Marine Build Excluded. Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
 
             # CUSTOM
@@ -516,7 +530,7 @@ class DQNAgent(base_agent.BaseAgent):
             # END OF CUSTOM
 
             # modified original logic, waits for 8 marines before attacking
-            if army_supply < 8 or self.attack_delay_timer < 60:
+            if army_supply < 10 or self.attack_delay_timer < 60:
                 excluded_actions.append(6)
                 excluded_actions.append(7)
                 excluded_actions.append(8)

commit 57cb4f5d0ef395cdab3abb12d14f4343f1848695
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 15:31:28 2023 -0400

    increased barracks, added SCV training timer

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
index cf397e08..7b8b7765 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
@@ -285,6 +285,7 @@ class DQNAgent(base_agent.BaseAgent):
         # Custom Delay Timers
         self.attack_delay_timer = 0
         self.supply_delay_timer = 0
+        self.scv_delay_timer = 0
 
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
@@ -331,6 +332,7 @@ class DQNAgent(base_agent.BaseAgent):
         # Using delay timers to avoid duplicate commands being issued by the AI
         self.attack_delay_timer += 1
         self.supply_delay_timer += 1
+        self.scv_delay_timer += 1
 
         # If this is our last step
         if obs.last():
@@ -471,7 +473,7 @@ class DQNAgent(base_agent.BaseAgent):
             #                          self.previous_action, 0, current_state, 0)
 
             excluded_actions = []
-            print("excluded_actions at the start are set to: ", excluded_actions)
+            # print("excluded_actions at the start are set to: ", excluded_actions)
             # Original Code:
             # # if supply_depot_count == 2 or worker_supply == 0:
             #     excluded_actions.append(1)
@@ -489,16 +491,16 @@ class DQNAgent(base_agent.BaseAgent):
             # 9: 'attack_47_47'
 
             # Modified, self-generated code to scale supply depot creation
-            if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
+            if supply_free > 6 or self.supply_delay_timer < 10:
                 excluded_actions.append(1)
 
-            if barracks_count > 3:
+            if barracks_count > 4:
                 excluded_actions.append(2)
 
+            # Exclude marinies from the build queue
             if supply_free == 0 or barracks_count == 0:
-                # print("Before exclusion: ", excluded_actions)
+                # print("Supply is set to: ", supply_free," and barracks_count is: ", barracks_count)
                 excluded_actions.append(3)
-                # print("We are excluding something here:", excluded_actions)
 
             # CUSTOM
             # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
@@ -508,8 +510,8 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("excluding attack units")
                 excluded_actions.append(4)
 
-            #SCV Code
-            if worker_supply > 15:
+            #SCV Checks
+            if worker_supply > 15 or self.scv_delay_timer < 7:
                 excluded_actions.append(5)
             # END OF CUSTOM
 
@@ -529,6 +531,8 @@ class DQNAgent(base_agent.BaseAgent):
             self.previous_state = current_state
             self.previous_action = rl_action
 
+            # print("Our excluded actions for this step are: ", excluded_actions)
+
             # boiler plate again
 
             smart_action, x, y = self.splitAction(self.previous_action)
@@ -558,7 +562,7 @@ class DQNAgent(base_agent.BaseAgent):
                 if self.cc_y.any():
                     i = random.randint(0, len(self.cc_y) - 1)
                     target = [self.cc_x[i], self.cc_y[i]]
-                    print("Next step will build an SCV at: ", target)
+                    # print("Next step will build an SCV at: ", target)
 
                     return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
 
@@ -620,9 +624,11 @@ class DQNAgent(base_agent.BaseAgent):
             
             # CUSTOM SCV Build Logiic
             elif smart_action == ACTION_BUILD_SCV:
-                print("SCV Smart Action Set")
+                # print("SCV Smart Action Set")
                 if _TRAIN_SCV in obs.observation['available_actions']:
-                    print("Trying to train an SCV")
+                    # print("Trying to train an SCV")
+                    # Zero out our build timer 
+                    self.scv_delay_timer = 0
                     return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
             
             

commit 551880c2752037731ca1cdee7857dd51ec0f5b45
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Sun Aug 13 14:43:22 2023 -0400

    Added support for building SCVs

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
index f028e10a..cf397e08 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
@@ -4,11 +4,12 @@
 # All of the RL algorithms were implemented by me
 
 # Train the agent against a very_easy Terran bot with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v11.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
 
 # Train against Kane-AI with this string:
-# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v10.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
-# ulimit -n 4096 required as after 1k episodes we run out of FD's
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v11.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
+# ulimit -n 4096 
+# required as after 1k episodes we run out of FD's
 
 
 import random
@@ -17,7 +18,7 @@ import os
 import torch
 import torch.nn as nn
 import torch.optim as optim
-from torchsummary import summary
+# from torchsummary import summary
 # Using amp for mixed-precision (FP16) to improve RTX 4090 performance
 from torch.cuda.amp import autocast, GradScaler
 # Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
@@ -105,7 +106,7 @@ for mm_x in range(0, 64):
             smart_actions.append(ACTION_ATTACK + '_' +
                                  str(mm_x - 16) + '_' + str(mm_y - 16))
 
-# print("smart_actions is set to: ", smart_actions)
+print("smart_actions is set to: ", smart_actions)
 
 ################################## End of BoilerPlate Code #####################################################
 
@@ -470,7 +471,7 @@ class DQNAgent(base_agent.BaseAgent):
             #                          self.previous_action, 0, current_state, 0)
 
             excluded_actions = []
-            # print("excluded_actions at the start are set to: ", excluded_actions)
+            print("excluded_actions at the start are set to: ", excluded_actions)
             # Original Code:
             # # if supply_depot_count == 2 or worker_supply == 0:
             #     excluded_actions.append(1)
@@ -480,8 +481,8 @@ class DQNAgent(base_agent.BaseAgent):
             # 1: 'buildsupplydepot'
             # 2: 'buildbarracks'
             # 3: 'buildmarine'
-            # 4: 'buildscv' NOT IMPLEMENTED YET
-            # 5: 'attackunit'
+            # 4: 'attackunit'
+            # 5: 'buildscv'
             # 6: 'attack_15_15'
             # 7: 'attack_15_47'
             # 8: 'attack_47_15'
@@ -508,11 +509,12 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(4)
 
             #SCV Code
-            excluded_actions.append(5)
+            if worker_supply > 15:
+                excluded_actions.append(5)
             # END OF CUSTOM
 
             # modified original logic, waits for 8 marines before attacking
-            if army_supply < 8 or self.attack_delay_timer < 100:
+            if army_supply < 8 or self.attack_delay_timer < 60:
                 excluded_actions.append(6)
                 excluded_actions.append(7)
                 excluded_actions.append(8)
@@ -550,6 +552,15 @@ class DQNAgent(base_agent.BaseAgent):
             elif smart_action == ACTION_ATTACK or smart_action==ACTION_ATTACK_UNIT:
                 if _SELECT_ARMY in obs.observation['available_actions']:
                     return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+                
+            # CUSTOM SCV Build Code
+            elif smart_action == ACTION_BUILD_SCV:
+                if self.cc_y.any():
+                    i = random.randint(0, len(self.cc_y) - 1)
+                    target = [self.cc_x[i], self.cc_y[i]]
+                    print("Next step will build an SCV at: ", target)
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
 
         elif self.move_number == 1:
             self.move_number += 1
@@ -605,6 +616,16 @@ class DQNAgent(base_agent.BaseAgent):
 
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
+            
+            
+            # CUSTOM SCV Build Logiic
+            elif smart_action == ACTION_BUILD_SCV:
+                print("SCV Smart Action Set")
+                if _TRAIN_SCV in obs.observation['available_actions']:
+                    print("Trying to train an SCV")
+                    return actions.FunctionCall(_TRAIN_SCV, [_QUEUED])
+            
+            
             # start of boiler plate code (small modifications like delay timers, that's it)
             elif smart_action == ACTION_BUILD_MARINE:
                 if _TRAIN_MARINE in obs.observation['available_actions']:

commit 9109caefc52410e55004107e85037b41d1909895
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 11 17:46:13 2023 -0400

    modified barracks build to be more intelligent

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
index f4f08ec5..f028e10a 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
@@ -491,7 +491,7 @@ class DQNAgent(base_agent.BaseAgent):
             if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
                 excluded_actions.append(1)
 
-            if barracks_count > 2:
+            if barracks_count > 3:
                 excluded_actions.append(2)
 
             if supply_free == 0 or barracks_count == 0:
@@ -518,11 +518,9 @@ class DQNAgent(base_agent.BaseAgent):
                 excluded_actions.append(8)
                 excluded_actions.append(9)
 
-
-
             # print("Our exclusions are set to: ", excluded_actions)
 
-            # Updated for DQN
+            # Updated for DQN - let the model  select the action
             rl_action = self.dqn_model.choose_action(
                 current_state, excluded_actions)
 
@@ -591,8 +589,8 @@ class DQNAgent(base_agent.BaseAgent):
                     if self.cc_y.any():
                         x_padding = random.randint(-30, 30)
                         y_padding = random.randint(-30, 30)
-                        target_x = round(self.cc_x.mean()) + 15 + x_padding
-                        target_y = round(self.cc_y.mean()) - 9 + y_padding
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
 
                         target = self.transformLocation(target_x, target_y)
 

commit 53a883a75554ff17610a04d3d007f069b83178d6
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Fri Aug 11 17:15:20 2023 -0400

    Smart attack, new actions, camera movement, etc

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
new file mode 100644
index 00000000..f4f08ec5
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v11.py
@@ -0,0 +1,686 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Train the agent against a very_easy Terran bot with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+
+# Train against Kane-AI with this string:
+# python -m pysc2.bin.agent --map Simple64 --agent train_dqn_agent_v10.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --agent2 Kane-AI.KaneAI --agent2_race terran
+# ulimit -n 4096 required as after 1k episodes we run out of FD's
+
+
+import random
+import math
+import os
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...not excellent.
+# from torch.optim.lr_scheduler import CosineAnnealingLR
+import numpy as np
+import pandas as pd
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+# queue used for replay buffer
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-agent-model-12.pt'
+
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+#CUSTOM
+_ATTACK_SCREEN = actions.FUNCTIONS.Attack_screen.id
+# END CUSTOM
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+# _PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+#CUSTOM
+ACTION_BUILD_SCV = 'buildscv'
+ACTION_ATTACK_UNIT = 'attackunit'
+#END OF CUSTOM
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+    ACTION_ATTACK_UNIT,
+    ACTION_BUILD_SCV,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+# Steven Brown's implementation for spawn-location-agnostic quadrant attacks
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+# print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code #####################################################
+
+
+# Custom DQN Agent implementation with a replay buffer of 50k
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.90, buffer_capacity=50000, batch_size=8192):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 10% of the time (down from 90%)
+        self.final_epsilon = 0.10
+        # We decay over 10M actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (self.epsilon - self.final_epsilon) / 10000000
+        self.action_counter = 0
+        #
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque([0]* buffer_capacity, maxlen=buffer_capacity)
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 2048),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(2048, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    # sample transitions from replay buffer queue for the last game
+    def sample(self, batch_size):
+        print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+        return list(self.buffer)[-batch_size:]
+
+
+    def forward(self, x):
+        return self.model(x)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Algorithm that Steven Brown (PySC2 Dev) created
+    # It's visible here
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
+    def choose_action(self, observation, excluded_actions=[]):
+        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+            # This is where we keep the logic for epsilon decay (linear)
+            self.action_counter += 1
+            self.epsilon -= self.epsilon_decay_rate
+            self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+            q_values = self.model(observation_tensor)
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+        # Add the action to the replay buffer
+        self.buffer.append(action)
+
+        return action
+
+    # a in our case is the total number of actions from the episode (we pull just those from the replay buffer)
+    # s_ is not used as actions are grouped together with a single reward (and no next state is really available)
+    def train(self, s, a, r):
+        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(self.device)
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < a:
+            return
+
+        # Sample a mini-batch of actions from the buffer
+        actions = self.sample(a)
+        # Debugs - print the last 10 actions in our buffer
+        # print("Printing Actions:")
+        # print("Size of replay buffer: ", len(self.buffer) )
+        # for action in actions[-10:]:
+        #     print("Action: ", action)
+
+        # Convert the actions to a tensor
+        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
+
+        # Compute the Q-values for the current states
+        q_values = self.model(s)
+        q_predict = q_values[0, actions]
+
+        # Compute the target Q-values
+        q_target = q_values.clone().detach()
+        q_target[0, actions] = r + self.gamma * torch.max(q_values)
+
+        # Compute the loss and update the model's weights
+        loss = self.loss_fn(q_predict, q_target[0, actions])
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.supply_delay_timer = 0
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    # Custom code for transforming Screen instead of Minimap (references transformLocation of course)
+    def transformLocationScreen(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 84 - x, 84 - y)
+            return [84 - x, 84 - y]
+
+        return [x, y]
+    
+    # Return of boiler plate
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.supply_delay_timer += 1
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+            else:
+                # For a loss or draw, ignore the step-based reward and use the ternary system
+                combined_reward = base_reward
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to checkpoint the model!")
+                print("Previous average reward was: ",
+                      self.previous_avg_reward)
+                print("Our average reward is: ", avg_reward)
+                print("Latest game reward was: ", combined_reward)
+                print("Number of steps were: ",  episode_steps)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(
+                    DATA_FILE, self.episode_count, avg_reward)
+            # Learn on every game, not just the successful ones:
+            self.dqn_model.train(
+                self.previous_state, episode_steps, combined_reward)
+            # Zero out the remaining, episode-specific counters
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        #BEGIN CUSTOM CODE
+        enemy_units = [unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY]
+        # END CUSTOM CODE
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = army_supply
+            # Custom State Add-on
+            current_state[4] = len(enemy_units)
+
+            # print("Current state is set to:", current_state)
+            # print("Player data is set to:", obs.observation['player'])
+            #print("Enemy units are: ",  [unit for unit in obs.observation.feature_units if unit.alliance == features.PlayerRelative.ENEMY])
+            # for unit in enemy_units:
+            #     x = unit.x
+            #     y = unit.y
+            #     print(f"Enemy unit of type {unit.unit_type} is at coordinates ({x}, {y})")
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            # if self.previous_action is not None:
+            #     self.dqn_model.train(self.previous_state,
+            #                          self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            # Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'buildscv' NOT IMPLEMENTED YET
+            # 5: 'attackunit'
+            # 6: 'attack_15_15'
+            # 7: 'attack_15_47'
+            # 8: 'attack_47_15'
+            # 9: 'attack_47_47'
+
+            # Modified, self-generated code to scale supply depot creation
+            if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
+                excluded_actions.append(1)
+
+            if barracks_count > 2:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                # print("Before exclusion: ", excluded_actions)
+                excluded_actions.append(3)
+                # print("We are excluding something here:", excluded_actions)
+
+            # CUSTOM
+            # If we don't see an enemy or if we've issued an attack order 2 steps ago, skip...
+            # print("Length of enemy_units is: ",len(enemy_units))
+            # print(enemy_units)
+            if len(enemy_units) == 0:
+                # print("excluding attack units")
+                excluded_actions.append(4)
+
+            #SCV Code
+            excluded_actions.append(5)
+            # END OF CUSTOM
+
+            # modified original logic, waits for 8 marines before attacking
+            if army_supply < 8 or self.attack_delay_timer < 100:
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+                excluded_actions.append(8)
+                excluded_actions.append(9)
+
+
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # boiler plate again
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK or smart_action==ACTION_ATTACK_UNIT:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) + 15 + x_padding
+                        target_y = round(self.cc_y.mean()) - 9 + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # start of boiler plate code (small modifications like delay timers, that's it)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+            # Custom implementation to attack random enemy unit
+            elif smart_action == ACTION_ATTACK_UNIT:                
+                if enemy_units and actions.FUNCTIONS.Attack_screen.id in obs.observation["available_actions"]:
+                    # Select a random enemy unit
+                    target_unit = random.choice(enemy_units)
+
+                    # Check if the target unit is within the current view
+                    if 0 <= target_unit.x < 84 and 0 <= target_unit.y < 84:
+                        # Issue the attack command using screen coordinates
+                        # print("Attacking enemy unit: ", target_unit, " at: ", self.transformLocationScreen(target_unit.x, target_unit.y), " with original coordinates: ", target_unit.x, target_unit.y)
+                        return actions.FunctionCall(_ATTACK_SCREEN, [_NOT_QUEUED, self.transformLocationScreen(target_unit.x, target_unit.y)])
+                    else:
+                        # Clamp the target camera coordinates to a valid range
+                        target_x = max(0, min(target_unit.x, 83))
+                        target_y = max(0, min(target_unit.y, 83))
+
+                        # Move the camera to the clamped coordinates
+                        print("Moving camera to fix out-of-bounds issue for attack_screen: ", target_x, target_y)
+                        return actions.FUNCTIONS.move_camera((target_x, target_y))
+
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit 7a39a64782432a9a8b5b589042cc9420301af8b6
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Thu Aug 10 17:25:38 2023 -0400

    Adding v10, fixed replay buffer (was not being used!)

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v10.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v10.py
new file mode 100644
index 00000000..7f717704
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/train_dqn_agent_v10.py
@@ -0,0 +1,620 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Run the agent with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+import random
+import math
+import os
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...not excellent.
+# from torch.optim.lr_scheduler import CosineAnnealingLR
+import numpy as np
+import pandas as pd
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+# used for reward tracking
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-agent-model-10.pt'
+
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+# print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code #####################################################
+
+
+# Custom DQN Agent implementation with a replay buffer of 50k
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.95, buffer_capacity=50000, batch_size=8192):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 5% of the time (down from 95%)
+        self.final_epsilon = 0.05
+        # We decay over 5M actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (self.epsilon - self.final_epsilon) / 5000000
+        self.action_counter = 0
+        #
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = deque([0]* buffer_capacity, maxlen=buffer_capacity)
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 2048),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(2048, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    # def push(self, state, action, reward, next_state, done):
+    #     state = torch.tensor(state, dtype=torch.float32).unsqueeze(
+    #         0).to(self.device)
+    #     if next_state is not None:
+    #         next_state = torch.tensor(
+    #             next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+    #     else:
+    #         next_state = torch.zeros_like(state).to(self.device)
+    #     action = torch.tensor([action], dtype=torch.int64).to(self.device)
+    #     reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
+    #     done = torch.tensor([done == 'last'],
+    #                         dtype=torch.bool).to(self.device)
+    #     self.buffer.append((state, action, reward, next_state, done))
+
+    # sample transitions from replay buffer queue for the last game
+    def sample(self, batch_size):
+        print("Replay buffer is returning actions: ", len(list(self.buffer)[-batch_size:]))
+        return list(self.buffer)[-batch_size:]
+
+
+    def forward(self, x):
+        return self.model(x)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Algorithm that Steven Brown (PySC2 Dev) created
+    # It's visible here
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
+    def choose_action(self, observation, excluded_actions=[]):
+        observation_tensor = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+            # This is where we keep the logic for epsilon decay (linear)
+            self.action_counter += 1
+            self.epsilon -= self.epsilon_decay_rate
+            self.epsilon = max(self.final_epsilon, self.epsilon)
+        else:
+            q_values = self.model(observation_tensor)
+            for action in excluded_actions:
+                q_values[0][action] = float('-inf')
+            action = torch.argmax(q_values).item()
+
+        # Add the action to the replay buffer
+        self.buffer.append(action)
+
+        return action
+
+    # a in our case is the total number of actions from the episode (we pull just those from the replay buffer)
+    # s_ is not used as actions are grouped together with a single reward (and no next state is really available)
+    def train(self, s, a, r):
+        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0).to(self.device)
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < a:
+            return
+
+        # Sample a mini-batch of actions from the buffer
+        actions = self.sample(a)
+        # Debugs - print the last 10 actions in our buffer
+        # print("Printing Actions:")
+        # print("Size of replay buffer: ", len(self.buffer) )
+        # for action in actions[-10:]:
+        #     print("Action: ", action)
+
+        # Convert the actions to a tensor
+        actions = torch.tensor(actions, dtype=torch.int64).to(self.device)
+
+        # Compute the Q-values for the current states
+        q_values = self.model(s)
+        q_predict = q_values[0, actions]
+
+        # Compute the target Q-values
+        q_target = q_values.clone().detach()
+        q_target[0, actions] = r + self.gamma * torch.max(q_values)
+
+        # Compute the loss and update the model's weights
+        loss = self.loss_fn(q_predict, q_target[0, actions])
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.supply_delay_timer = 0
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.supply_delay_timer += 1
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+            else:
+                # For a loss or draw, ignore the step-based reward and use the ternary system
+                combined_reward = base_reward
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to checkpoint the model!")
+                print("Previous average reward was: ",
+                      self.previous_avg_reward)
+                print("Our average reward is: ", avg_reward)
+                print("Latest game reward was: ", combined_reward)
+                print("Number of steps were: ",  episode_steps)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(
+                    DATA_FILE, self.episode_count, avg_reward)
+            # Learn on every game, not just the successful ones:
+            self.dqn_model.train(
+                self.previous_state, episode_steps, combined_reward)
+            # Zero out the remaining, episode-specific counters
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            # if self.previous_action is not None:
+            #     self.dqn_model.train(self.previous_state,
+            #                          self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            # Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attack_15_15'
+            # 5: 'attack_15_47'
+            # 6: 'attack_47_15'
+            # 7: 'attack_47_47'
+
+            # Modified, self-generated code to scale supply depot creation
+            if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
+                excluded_actions.append(1)
+
+            if barracks_count > 2:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                # print("Before exclusion: ", excluded_actions)
+                excluded_actions.append(3)
+                # print("We are excluding something here:", excluded_actions)
+
+            # modified original logic, waits for 8 marines before attacking
+            if army_supply < 8 or self.attack_delay_timer < 100:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # boiler plate again
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) + 15 + x_padding
+                        target_y = round(self.cc_y.mean()) - 9 + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # start of boiler plate code (small modifications like delay timers, that's it)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit c59e287a9492cd254473b5439918b2b8d714db9b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Wed Aug 9 19:54:01 2023 -0400

    v7 model, has linear decay for epsilon, larger model,

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v7.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v7.py
new file mode 100644
index 00000000..f64f4ccc
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v7.py
@@ -0,0 +1,643 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Run the agent with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+import random
+import math
+import os
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+# Using a learning Rate Scheduler to help the model converge faster / avoid getting stuck
+# The static learning rate of 0.01 was...not excellent.
+# from torch.optim.lr_scheduler import CosineAnnealingLR
+import numpy as np
+import pandas as pd
+
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+# used for reward tracking
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+####### CUSTOM CODE ######
+_TRAIN_SCV = actions.FUNCTIONS.Train_SCV_quick.id
+DATA_FILE = 'dqn-agent-model-8.pt'
+
+###### END OF GLOBAL CUSTOM CODE #####
+
+################################## Start of BoilerPlate Code #####################################################
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+# print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code #####################################################
+
+
+# Custom DQN Agent implementation with a replay buffer of 1M
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.95, buffer_capacity=1000000, batch_size=128):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        # Using linear epsilon decay to reduce random action probability over time
+        self.epsilon = e_greedy
+        # Final decayed epsilon will result in a random action being taken 5% of the time (down from 95%)
+        self.final_epsilon = 0.05
+        # We decay over 5M actions taken (no real way to track per-episode easily with PySC2 from the model's perspective)
+        self.epsilon_decay_rate = (self.epsilon - self.final_epsilon) / 5000000
+        self.action_counter = 0
+        #
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = []
+        # self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 2048),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(2048, 1024),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(1024, 512),
+            nn.ReLU(),
+            nn.Linear(512, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    def push(self, state, action, reward, next_state, done):
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(
+            0).to(self.device)
+        if next_state is not None:
+            next_state = torch.tensor(
+                next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        else:
+            next_state = torch.zeros_like(state).to(self.device)
+        action = torch.tensor([action], dtype=torch.int64).to(self.device)
+        reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
+        done = torch.tensor([done == 'last'],
+                            dtype=torch.bool).to(self.device)
+        self.buffer.append((state, action, reward, next_state, done))
+
+    # sample transitions from replay buffer
+
+    def sample(self, batch_size):
+        batch = random.sample(self.buffer, batch_size)
+        state, action, reward, next_state, done = map(torch.stack, zip(*batch))
+        return state, action, reward, next_state, done
+
+    def forward(self, x):
+        return self.model(x)
+
+    # The basic framework of the function `choose_action`` was leveraged from the initial Q-Learning
+    # Algorithm that Steven Brown (PySC2 Dev) created
+    # It's visible here
+    # https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+    # I've of course modified it to use linear decay, a DQN (via torch) instead of Q-Learning, etc but...the initial work is his
+    def choose_action(self, observation, excluded_actions=[]):
+        # Debug
+        # print("Our epsilon greedy is currently set to:", self.epsilon)
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        # np.random.uniform generates a (mostly random...) 0->1 which we compare to our current self.epsilon with linear decay
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            action = np.random.choice(available_actions)
+
+            # This is where we keep the logic for epsilon decay (linear)
+            self.action_counter += 1
+            self.epsilon -= self.epsilon_decay_rate
+            self.epsilon = max(self.final_epsilon, self.epsilon)
+
+            return action
+
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def train(self, s, a, r, s_, done):
+        s = torch.tensor(s, dtype=torch.float32).to(self.device)
+        if s_ != 'last':
+            s_ = torch.tensor(s_, dtype=torch.float32).to(self.device)
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        states, actions, rewards, next_states, dones = self.sample(
+            self.batch_size)
+
+        # Squeeze the actions tensor to remove the extra dimension
+        actions = actions.squeeze(1)
+
+        # Compute the Q-values for the current states with mixed precision/autocast
+        with autocast():
+            q_predict = self.model(states).squeeze(1)
+            q_target = q_predict.clone().detach().squeeze(1)
+
+            # Compute the target Q-values within autocast
+            for i in range(self.batch_size):
+                action = actions[i].item()
+                reward = rewards[i]
+                next_state = next_states[i].unsqueeze(
+                    0) if next_states[i] is not None else None
+                if dones[i]:
+                    q_target[i][action] = reward
+                else:
+                    if next_state is not None:
+                        # print("q_target shape:", q_target.shape)
+                        # print("i:", i)
+                        # print("action:", action)
+                        # print("reward:", reward)
+                        # print("next_state shape:", next_state.shape if next_state is not None else None)
+                        # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
+
+                        q_target[i][action] = reward + self.gamma * \
+                            torch.max(self.model(next_state))
+                    else:
+                        q_target[i][action] = reward
+
+            # Compute the loss and update the model's weights (FP16/mixed precision)
+            loss = self.loss_fn(q_predict, q_target)
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        # Keep the last 100 rewards, initialize it to 0's to avoid overweighting early successes
+        self.last_rewards = deque([0] * 100, maxlen=100)
+        self.episode_count = 0
+        self.previous_avg_reward = 0
+
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.supply_delay_timer = 0
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+
+        return [x + x_distance, y + y_distance]
+
+    def transformLocation(self, x, y):
+        # Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            # Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        # Using delay timers to avoid duplicate commands being issued by the AI
+        self.attack_delay_timer += 1
+        self.supply_delay_timer += 1
+
+        # If this is our last step
+        if obs.last():
+            self.episode_count += 1
+            base_reward = obs.reward  # This is a ternary system - -1, 0, 1
+            episode_steps = obs.observation.game_loop[0]
+
+            # Apply step-based reward only if the agent won, encouraging the agent to find efficient victories
+            # Reward decreases the longer it takes after 10,000 game steps, becoming 0 after 20,000 steps
+            if base_reward == 1:  # 1 indicates a win
+                extra_steps = max(0, episode_steps - 10000)
+                step_penalty = extra_steps // 1000 * 0.1
+                step_reward = 1.0 - step_penalty
+                combined_reward = base_reward + step_reward
+            else:
+                # For a loss or draw, ignore the step-based reward and use the ternary system
+                combined_reward = base_reward
+
+            self.last_rewards.append(combined_reward)  # Add the latest reward
+            # Calculate the average reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards)
+
+            # Optimal reward is '2' (perfect string of wins at 10K steps or less)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to checkpoint the model!")
+                print("Previous average reward was: ",
+                      self.previous_avg_reward)
+                print("Our average reward is: ", avg_reward)
+                print("Latest game reward was: ", combined_reward)
+                print("Number of steps were: ",  episode_steps)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(
+                    DATA_FILE, self.episode_count, avg_reward)
+            # Learn on every step, not just the successful ones:
+            self.dqn_model.train(
+                self.previous_state, self.previous_action, combined_reward, 'last', 'last')
+            # Zero out the remaining, episode-specific counters
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        # BOILER PLATE Action-Space Guardrails
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.train(self.previous_state,
+                                     self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            # Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot'
+            # 2: 'buildbarracks'
+            # 3: 'buildmarine'
+            # 4: 'attack_15_15'
+            # 5: 'attack_15_47'
+            # 6: 'attack_47_15'
+            # 7: 'attack_47_47'
+
+            # Modified, self-generated code to scale supply depot creation
+            if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
+                excluded_actions.append(1)
+
+            if barracks_count > 2:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                # print("Before exclusion: ", excluded_actions)
+                excluded_actions.append(3)
+                # print("We are excluding something here:", excluded_actions)
+
+            # modified original logic, waits for 8 marines before attacking
+            if army_supply < 8 or self.attack_delay_timer < 100:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            # boiler plate again
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            # end of boiler plate
+
+            # Custom code / initial design similar to boiler plate
+
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) + 15 + x_padding
+                        target_y = round(self.cc_y.mean()) - 9 + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        # Assuming screen size is 84x84
+                        target[0] = max(BORDER_PADDING, min(
+                            target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(
+                            target[1], 83 - BORDER_PADDING))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            # start of boiler plate code (small modifications like delay timers, that's it)
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.attack_delay_timer = 0
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
+
+    # end of boiler plate code

commit c3b84642f220cd8cf50671049b4c71dace0ee81f
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Aug 8 14:56:14 2023 -0400

    Adding delay timers to limit repeated actions
    
    Win rate is approaching 100% now against very_easy terran

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
index d8dc572f..69367999 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
@@ -280,8 +280,10 @@ class DQNAgent(base_agent.BaseAgent):
         self.episode_count = 0
         self.previous_avg_reward = -1
 
-        # Custom Action Delay Timer
-        self.action_delay_timer = 0
+        # Custom Delay Timers
+        self.attack_delay_timer = 0
+        self.supply_delay_timer = 0
+
 
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
@@ -321,8 +323,9 @@ class DQNAgent(base_agent.BaseAgent):
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         #Using delay timers to avoid duplicate commands being issued by the AI
-        # print("ACTION_DELAY_TIMER", self.action_delay_timer)
-        self.action_delay_timer += 1
+        self.attack_delay_timer += 1
+        self.supply_delay_timer += 1
+
         if obs.last():
             self.episode_count += 1
             reward = obs.reward # This is a ternary system - -1, 0, 1
@@ -444,18 +447,9 @@ class DQNAgent(base_agent.BaseAgent):
             # 7: 'attack_47_47' 
 
             #Modified, self-generated code to scale supply depot creation
-            if supply_free > 6 or worker_supply == 0:
+            if supply_free > 6 or worker_supply == 0 or self.supply_delay_timer < 40:
                 excluded_actions.append(1)  
 
-            # Custom Code to exclude home base location from attack (coordinates are inverted for modeling...)
-            # excluded_actions.append(7)
-            # if self.base_top_left: # If spawn location is top left
-            #     excluded_actions.append(7) # Exclude the action that maps to the local (top left) quadrant
-            # else: # If spawn location is bottom right
-            #     excluded_actions.append(7) # Exclude the action that maps to the local (bottom right) quadrant
-
-            # Reference Code:
-
             if barracks_count > 2:
                 excluded_actions.append(2)
 
@@ -465,7 +459,7 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("We are excluding something here:", excluded_actions)
 
             #modified original logic, waits for 8 marines before attacking
-            if army_supply < 8 or self.action_delay_timer < 100:
+            if army_supply < 8 or self.attack_delay_timer < 100:
                 excluded_actions.append(4)
                 excluded_actions.append(5)
                 excluded_actions.append(6)
@@ -525,6 +519,8 @@ class DQNAgent(base_agent.BaseAgent):
 
                         # print("Trying to build a supply depot at:", target)
 
+                        # Sleep to avoid duplicate actions
+                        self.supply_delay_timer = 0
                         return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
             elif smart_action == ACTION_BUILD_BARRACKS:
@@ -568,7 +564,7 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
 
                     # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
-                    self.action_delay_timer = 0
+                    self.attack_delay_timer = 0
 
                     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
                 # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:

commit beeec25bcbc1e7d0880d4f5dc0c48e8f7cfd1e0b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Aug 8 14:14:29 2023 -0400

    Introduced action delay
    
    Success increased considerably, although the agent still hunts around in odd ways

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
index 10cb6979..d8dc572f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
@@ -280,6 +280,9 @@ class DQNAgent(base_agent.BaseAgent):
         self.episode_count = 0
         self.previous_avg_reward = -1
 
+        # Custom Action Delay Timer
+        self.action_delay_timer = 0
+
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
             self.dqn_model.load_model(DATA_FILE)
@@ -317,6 +320,9 @@ class DQNAgent(base_agent.BaseAgent):
 
     def step(self, obs):
         super(DQNAgent, self).step(obs)
+        #Using delay timers to avoid duplicate commands being issued by the AI
+        # print("ACTION_DELAY_TIMER", self.action_delay_timer)
+        self.action_delay_timer += 1
         if obs.last():
             self.episode_count += 1
             reward = obs.reward # This is a ternary system - -1, 0, 1
@@ -327,7 +333,7 @@ class DQNAgent(base_agent.BaseAgent):
             # Optimal reward is of course '1' (perfect string of wins)
             if avg_reward > self.previous_avg_reward:
                 print("------------------------------------------------------")
-                print("Average reward was great enough to update the model!")
+                print("Average reward was great enough to checkpoint the model!")
                 print("Previous average reward was: ", self.previous_avg_reward)
                 print("Our average reward was: ", avg_reward)
                 print("------------------------------------------------------")
@@ -459,7 +465,7 @@ class DQNAgent(base_agent.BaseAgent):
                 # print("We are excluding something here:", excluded_actions)
 
             #modified original logic, waits for 8 marines before attacking
-            if army_supply < 8:
+            if army_supply < 8 or self.action_delay_timer < 100:
                 excluded_actions.append(4)
                 excluded_actions.append(5)
                 excluded_actions.append(6)
@@ -501,19 +507,8 @@ class DQNAgent(base_agent.BaseAgent):
 
             smart_action, x, y = self.splitAction(self.previous_action)
 
-
-            # Modified supply & barracks creation code to be more intelligent/flexible for the AI
-            # if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
-            #     # Modified check
-            #     if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
-            #         if self.cc_y.any():
-            #             x_padding = random.randint(-3, 3)
-            #             y_padding = random.randint(-3, 3)
-            #             target = self.transformDistance(round(self.cc_x.mean()), -35 + x_padding, round(self.cc_y.mean()), y_padding)
-            #             print("Trying to build a supply depot at:", target)
-
-            #             return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
-
+            # Used to ensure buildings are placed on the border, resulting in trapped units
+            BORDER_PADDING = 6
             if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
                 if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
                     if self.cc_y.any():
@@ -525,14 +520,13 @@ class DQNAgent(base_agent.BaseAgent):
                         target = self.transformLocation(target_x, target_y)
 
                         # Ensure the coordinates are within valid bounds after transformation
-                        target[0] = max(0, min(target[0], 83)) 
-                        target[1] = max(0, min(target[1], 83))
+                        target[0] = max(BORDER_PADDING, min(target[0], 83 - BORDER_PADDING))
+                        target[1] = max(BORDER_PADDING, min(target[1], 83 - BORDER_PADDING))
 
                         # print("Trying to build a supply depot at:", target)
 
                         return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
 
-            # Modified supply & barracks creation code to be more intelligent/flexible for the AI
             elif smart_action == ACTION_BUILD_BARRACKS:
                 if _BUILD_BARRACKS in obs.observation['available_actions']:
                     if self.cc_y.any():
@@ -544,13 +538,14 @@ class DQNAgent(base_agent.BaseAgent):
                         target = self.transformLocation(target_x, target_y)
 
                         # Ensure the coordinates are within valid bounds after transformation
-                        target[0] = max(0, min(target[0], 83))  # Assuming screen size is 84x84
-                        target[1] = max(0, min(target[1], 83))
+                        target[0] = max(BORDER_PADDING, min(target[0], 83 - BORDER_PADDING))  # Assuming screen size is 84x84
+                        target[1] = max(BORDER_PADDING, min(target[1], 83 - BORDER_PADDING))
 
                         # print("Trying to build a barracks at:", target)
 
                         return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
 
+
             elif smart_action == ACTION_BUILD_MARINE:
                 if _TRAIN_MARINE in obs.observation['available_actions']:
                     return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
@@ -572,6 +567,9 @@ class DQNAgent(base_agent.BaseAgent):
                     # print("Our base is top left: ", self.base_top_left)
                     # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
 
+                    # Zero out our attack delay timer so we don't issue this repetitively (will wait 100 steps)
+                    self.action_delay_timer = 0
+
                     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
                 # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
                 #     # Debugs

commit bfa9e89b63587110a4e7e379a04bd55b8cb79d6b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Tue Aug 8 11:53:24 2023 -0400

    adding v4 and v5 of the DQN agent
    
    DQNv5 was the first AI to win a game (a few!)

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v4.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v4.py
new file mode 100644
index 00000000..551dd6bd
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v4.py
@@ -0,0 +1,546 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Run the agent with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+import numpy as np
+import pandas as pd
+import random
+import math
+import os
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+# used for reward tracking
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+################################## Start of BoilerPlate Code ##################################################### 
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+DATA_FILE = 'dqn-agent-model-v4.pt'
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+            
+# print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code ##################################################### 
+
+
+# Custom DQN Agent implementation with replay buffer
+# Using fullyConv implementation in V4 to improve performance
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, buffer_capacity=250000, batch_size=64):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        self.epsilon = e_greedy
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = []
+        self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        # self.model = nn.Sequential(
+        #     nn.Linear(self.state_size, 24),
+        #     nn.ReLU(),
+        #     nn.Linear(24, 24),
+        #     nn.ReLU(),
+        #     nn.Linear(24, len(self.actions))
+        # ).to(self.device)  # Move model to GPU if available
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 128),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(128, 256),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(256, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 256),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(256, 128),
+            nn.ReLU(),
+            nn.Linear(128, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+
+
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+        # print("Model Details: ", self.model.summary())
+
+    def push(self, state, action, reward, next_state, done):
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(
+            0).to(self.device)
+        if next_state is not None:
+            next_state = torch.tensor(
+                next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        else:
+            next_state = torch.zeros_like(state).to(self.device)
+        action = torch.tensor([action], dtype=torch.int64).to(self.device)
+        reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
+        done = torch.tensor([done == 'terminal'],
+                            dtype=torch.bool).to(self.device)
+        self.buffer.append((state, action, reward, next_state, done))
+
+    # sample transitions from replay buffer
+
+    def sample(self, batch_size):
+        batch = random.sample(self.buffer, batch_size)
+        state, action, reward, next_state, done = map(torch.stack, zip(*batch))
+        return state, action, reward, next_state, done
+
+    def forward(self, x):
+        return self.model(x)
+
+    def choose_action(self, observation, excluded_actions=[]):
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            return np.random.choice(available_actions)
+
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def learn(self, s, a, r, s_, done):
+        s = torch.tensor(s, dtype=torch.float32).to(self.device)
+        if s_ != 'terminal':
+            s_ = torch.tensor(s_, dtype=torch.float32).to(self.device)
+        # # Store the current transition in the replay buffer
+        # self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
+        #     s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+        # self.push(s, a, r, s_ if s_ != 'terminal' else None, done)
+
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        states, actions, rewards, next_states, dones = self.sample(
+            self.batch_size)
+
+        # Squeeze the actions tensor to remove the extra dimension
+        actions = actions.squeeze(1)
+
+        # Compute the Q-values for the current states with mixed precision/autocast
+        with autocast():
+            q_predict = self.model(states).squeeze(1)
+            q_target = q_predict.clone().detach().squeeze(1)
+
+            # Compute the target Q-values within autocast
+            for i in range(self.batch_size):
+                action = actions[i].item()
+                reward = rewards[i]
+                next_state = next_states[i].unsqueeze(
+                    0) if next_states[i] is not None else None
+                if dones[i]:
+                    q_target[i][action] = reward
+                else:
+                    if next_state is not None:
+                        # print("q_target shape:", q_target.shape)
+                        # print("i:", i)
+                        # print("action:", action)
+                        # print("reward:", reward)
+                        # print("next_state shape:", next_state.shape if next_state is not None else None)
+                        # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
+
+                        q_target[i][action] = reward + self.gamma * \
+                            torch.max(self.model(next_state))
+                    else:
+                        q_target[i][action] = reward
+
+            # Compute the loss and update the model's weights (FP16/mixed precision)
+            loss = self.loss_fn(q_predict, q_target)
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        self.last_rewards = deque(maxlen=100) # Keep the last 100 rewards
+        self.episode_count = 0
+        self.previous_avg_reward = -1
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+
+        return [x + x_distance, y + y_distance]
+
+    def transformLocation(self, x, y):
+        #Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            #Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)            
+            return [64 - x, 64 - y]
+        
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+    
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        if obs.last():
+            self.episode_count += 1
+            reward = obs.reward # This is a ternary system - -1, 0, 1
+            # Concious decision was made to avoid using blizzard score based on DeepMind's findings in their original 2017 paper
+            self.last_rewards.append(reward) # Add the latest reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards) # Calculate the average reward
+
+            # Optimal reward is of course '1' (perfect string of wins)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to update the model!")
+                print("Previous average reward was: ", self.previous_avg_reward)
+                print("Our average reward was: ", avg_reward)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
+            # Learn on every step, not just the successful ones:
+            self.dqn_model.learn(self.previous_state, self.previous_action, reward, 'terminal', 'terminal')
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+        
+        # BOILER PLATE Action-Space Guardrails 
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.learn(self.previous_state,
+                                     self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            if supply_depot_count == 2 or worker_supply == 0:
+                excluded_actions.append(1)
+
+            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                excluded_actions.append(3)
+
+            if army_supply == 0:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if supply_depot_count < 2 and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if supply_depot_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -35, round(self.cc_y.mean()), 0)
+                        elif supply_depot_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -25, round(self.cc_y.mean()), -25)
+
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if barracks_count < 2 and _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if barracks_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), -9)
+                        elif barracks_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), 12)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     x_offset = random.randint(-1, 1)
+                #     y_offset = random.randint(-1, 1)
+
+                #     # Debugs
+                #     print("Our base is top left: ", self.base_top_left)
+                #     print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
new file mode 100644
index 00000000..10cb6979
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v5.py
@@ -0,0 +1,604 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Run the agent with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torchsummary import summary
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+import numpy as np
+import pandas as pd
+import random
+import math
+import os
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+# used for reward tracking
+from collections import deque
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+################################## Start of BoilerPlate Code ##################################################### 
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+DATA_FILE = 'dqn-agent-model-v6.pt'
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+            
+# print("smart_actions is set to: ", smart_actions)
+
+################################## End of BoilerPlate Code ##################################################### 
+
+
+# Custom DQN Agent implementation with replay buffer
+# Using fullyConv implementation in V4 to improve performance
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, buffer_capacity=250000, batch_size=64):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        self.epsilon = e_greedy
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = []
+        self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+        # Attempt GPU acceleration
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
+
+        # self.model = nn.Sequential(
+        #     nn.Linear(self.state_size, 24),
+        #     nn.ReLU(),
+        #     nn.Linear(24, 24),
+        #     nn.ReLU(),
+        #     nn.Linear(24, len(self.actions))
+        # ).to(self.device)  # Move model to GPU if available
+        self.model = nn.Sequential(
+            nn.Linear(state_size, 128),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(128, 256),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(256, 512),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(512, 256),
+            nn.ReLU(),
+            nn.Dropout(0.2),
+            nn.Linear(256, 128),
+            nn.ReLU(),
+            nn.Linear(128, len(self.actions))
+        ).to(self.device)  # Move model to GPU if available
+
+
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+        # print("Model Details: ", self.model.summary())
+
+    def push(self, state, action, reward, next_state, done):
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(
+            0).to(self.device)
+        if next_state is not None:
+            next_state = torch.tensor(
+                next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        else:
+            next_state = torch.zeros_like(state).to(self.device)
+        action = torch.tensor([action], dtype=torch.int64).to(self.device)
+        reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
+        done = torch.tensor([done == 'terminal'],
+                            dtype=torch.bool).to(self.device)
+        self.buffer.append((state, action, reward, next_state, done))
+
+    # sample transitions from replay buffer
+
+    def sample(self, batch_size):
+        batch = random.sample(self.buffer, batch_size)
+        state, action, reward, next_state, done = map(torch.stack, zip(*batch))
+        return state, action, reward, next_state, done
+
+    def forward(self, x):
+        return self.model(x)
+
+    def choose_action(self, observation, excluded_actions=[]):
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            #Debug
+            # print("Available actions are: ", self.actions)
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            return np.random.choice(available_actions)
+
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def learn(self, s, a, r, s_, done):
+        s = torch.tensor(s, dtype=torch.float32).to(self.device)
+        if s_ != 'terminal':
+            s_ = torch.tensor(s_, dtype=torch.float32).to(self.device)
+        # # Store the current transition in the replay buffer
+        # self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
+        #     s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+        # self.push(s, a, r, s_ if s_ != 'terminal' else None, done)
+
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        states, actions, rewards, next_states, dones = self.sample(
+            self.batch_size)
+
+        # Squeeze the actions tensor to remove the extra dimension
+        actions = actions.squeeze(1)
+
+        # Compute the Q-values for the current states with mixed precision/autocast
+        with autocast():
+            q_predict = self.model(states).squeeze(1)
+            q_target = q_predict.clone().detach().squeeze(1)
+
+            # Compute the target Q-values within autocast
+            for i in range(self.batch_size):
+                action = actions[i].item()
+                reward = rewards[i]
+                next_state = next_states[i].unsqueeze(
+                    0) if next_states[i] is not None else None
+                if dones[i]:
+                    q_target[i][action] = reward
+                else:
+                    if next_state is not None:
+                        # print("q_target shape:", q_target.shape)
+                        # print("i:", i)
+                        # print("action:", action)
+                        # print("reward:", reward)
+                        # print("next_state shape:", next_state.shape if next_state is not None else None)
+                        # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
+
+                        q_target[i][action] = reward + self.gamma * \
+                            torch.max(self.model(next_state))
+                    else:
+                        q_target[i][action] = reward
+
+            # Compute the loss and update the model's weights (FP16/mixed precision)
+            loss = self.loss_fn(q_predict, q_target)
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        print("Loading last checkpoint: ", file_path)
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        # Used for tracking rewards for use in model saving/checkpointing
+        self.last_rewards = deque([-1] * 100, maxlen=100) # Keep the last 100 rewards, initialize it to -1's to avoid overweighting early successes
+        self.episode_count = 0
+        self.previous_avg_reward = -1
+
+        if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+
+        return [x + x_distance, y + y_distance]
+
+    def transformLocation(self, x, y):
+        #Debug
+        # print("Before transforming, x and y are set to: ", x , y)
+        if not self.base_top_left:
+            #Debug
+            # print("After transformation, x and y are set tO: ", 64 - x, 64 - y)            
+            return [64 - x, 64 - y]
+        
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+    
+    # CUSTOM
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        if obs.last():
+            self.episode_count += 1
+            reward = obs.reward # This is a ternary system - -1, 0, 1
+            # Concious decision was made to avoid using blizzard score based on DeepMind's findings in their original 2017 paper
+            self.last_rewards.append(reward) # Add the latest reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards) # Calculate the average reward
+
+            # Optimal reward is of course '1' (perfect string of wins)
+            if avg_reward > self.previous_avg_reward:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to update the model!")
+                print("Previous average reward was: ", self.previous_avg_reward)
+                print("Our average reward was: ", avg_reward)
+                print("------------------------------------------------------")
+
+                self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
+            # Learn on every step, not just the successful ones:
+            self.dqn_model.learn(self.previous_state, self.previous_action, reward, 'terminal', 'terminal')
+            self.previous_avg_reward = avg_reward
+            self.previous_action = None
+            self.previous_state = None
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+        
+        # BOILER PLATE Action-Space Guardrails 
+        # Used as a way of limiting the potential action space at the beginning of the game for the agent
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
+
+                index = ((y - 1) * 2) + (x - 1)
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.learn(self.previous_state,
+                                     self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            # print("excluded_actions at the start are set to: ", excluded_actions)
+            # Original Code:
+            # # if supply_depot_count == 2 or worker_supply == 0:
+            #     excluded_actions.append(1)
+
+            ### Action Mapping
+            # 0: 'donothing'
+            # 1: 'buildsupplydepot' 
+            # 2: 'buildbarracks' 
+            # 3: 'buildmarine' 
+            # 4: 'attack_15_15' 
+            # 5: 'attack_15_47' 
+            # 6: 'attack_47_15' 
+            # 7: 'attack_47_47' 
+
+            #Modified, self-generated code to scale supply depot creation
+            if supply_free > 6 or worker_supply == 0:
+                excluded_actions.append(1)  
+
+            # Custom Code to exclude home base location from attack (coordinates are inverted for modeling...)
+            # excluded_actions.append(7)
+            # if self.base_top_left: # If spawn location is top left
+            #     excluded_actions.append(7) # Exclude the action that maps to the local (top left) quadrant
+            # else: # If spawn location is bottom right
+            #     excluded_actions.append(7) # Exclude the action that maps to the local (bottom right) quadrant
+
+            # Reference Code:
+
+            if barracks_count > 2:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                # print("Before exclusion: ", excluded_actions)
+                excluded_actions.append(3)
+                # print("We are excluding something here:", excluded_actions)
+
+            #modified original logic, waits for 8 marines before attacking
+            if army_supply < 8:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # print("Our exclusions are set to: ", excluded_actions)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+
+            # Modified supply & barracks creation code to be more intelligent/flexible for the AI
+            # if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+            #     # Modified check
+            #     if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+            #         if self.cc_y.any():
+            #             x_padding = random.randint(-3, 3)
+            #             y_padding = random.randint(-3, 3)
+            #             target = self.transformDistance(round(self.cc_x.mean()), -35 + x_padding, round(self.cc_y.mean()), y_padding)
+            #             print("Trying to build a supply depot at:", target)
+
+            #             return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) - 35 + x_padding
+                        target_y = round(self.cc_y.mean()) + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(0, min(target[0], 83)) 
+                        target[1] = max(0, min(target[1], 83))
+
+                        # print("Trying to build a supply depot at:", target)
+
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            # Modified supply & barracks creation code to be more intelligent/flexible for the AI
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        x_padding = random.randint(-30, 30)
+                        y_padding = random.randint(-30, 30)
+                        target_x = round(self.cc_x.mean()) + 15 + x_padding
+                        target_y = round(self.cc_y.mean()) - 9 + y_padding
+
+                        target = self.transformLocation(target_x, target_y)
+
+                        # Ensure the coordinates are within valid bounds after transformation
+                        target[0] = max(0, min(target[0], 83))  # Assuming screen size is 84x84
+                        target[1] = max(0, min(target[1], 83))
+
+                        # print("Trying to build a barracks at:", target)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    # Debugs
+                    # print("Our base is top left: ", self.base_top_left)
+                    # print("Our attack minimap location is: ", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                # if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                #     # Debugs
+                #     # print("Our base is top left: ", self.base_top_left)
+                #     # print("Our attack minimap location is: ", self.transformLocation(int(x), int(y)))
+
+                #     return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x), int(y))])
+
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])

commit fb87809c6df97bdec0a7fee8808eaeaa92584c9d
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 7 22:04:46 2023 -0400

    Fixed Map Off-by-One and updated Reward Tracking

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
index 35740b2f..5c85b0c0 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
@@ -4,7 +4,7 @@
 # All of the RL algorithms were implemented by me
 
 # Run the agent with the string:
-# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran 
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran
 import torch
 import torch.nn as nn
 import torch.optim as optim
@@ -18,7 +18,8 @@ import os
 from pysc2.agents import base_agent
 from pysc2.lib import actions
 from pysc2.lib import features
-
+# used for reward tracking
+from collections import deque
 
 # BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
 # https://github.com/skjb/pysc2-tutorial/tree/master
@@ -101,7 +102,8 @@ class DQNModel(nn.Module):
         self.scaler = GradScaler()
 
         # Attempt GPU acceleration
-        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+        self.device = torch.device(
+            "cuda" if torch.cuda.is_available() else "cpu")
 
         self.model = nn.Sequential(
             nn.Linear(self.state_size, 24),
@@ -109,25 +111,27 @@ class DQNModel(nn.Module):
             nn.Linear(24, 24),
             nn.ReLU(),
             nn.Linear(24, len(self.actions))
-        ).to(self.device) # Move model to GPU if available
+        ).to(self.device)  # Move model to GPU if available
 
         self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
         self.loss_fn = nn.MSELoss()
 
     def push(self, state, action, reward, next_state, done):
-        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(
+            0).to(self.device)
         if next_state is not None:
-            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+            next_state = torch.tensor(
+                next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
         else:
             next_state = torch.zeros_like(state).to(self.device)
         action = torch.tensor([action], dtype=torch.int64).to(self.device)
         reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
-        done = torch.tensor([done == 'terminal'], dtype=torch.bool).to(self.device)
+        done = torch.tensor([done == 'terminal'],
+                            dtype=torch.bool).to(self.device)
         self.buffer.append((state, action, reward, next_state, done))
 
-
-
     # sample transitions from replay buffer
+
     def sample(self, batch_size):
         batch = random.sample(self.buffer, batch_size)
         state, action, reward, next_state, done = map(torch.stack, zip(*batch))
@@ -137,12 +141,13 @@ class DQNModel(nn.Module):
         return self.model(x)
 
     def choose_action(self, observation, excluded_actions=[]):
-        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(self.device) # Move observation to GPU
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(
+            0).to(self.device)  # Move observation to GPU
         if np.random.uniform() < self.epsilon:
             available_actions = [
                 a for a in self.actions if a not in excluded_actions]
             return np.random.choice(available_actions)
-        
+
         q_values = self.model(observation)
         for action in excluded_actions:
             q_values[0][action] = float('-inf')
@@ -153,9 +158,11 @@ class DQNModel(nn.Module):
         s = torch.tensor(s, dtype=torch.float32).to(self.device)
         if s_ != 'terminal':
             s_ = torch.tensor(s_, dtype=torch.float32).to(self.device)
-        # Store the current transition in the replay buffer
-        self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
-            s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+        # # Store the current transition in the replay buffer
+        # self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
+        #     s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+        # self.push(s, a, r, s_ if s_ != 'terminal' else None, done)
+
 
         # Check if the replay buffer has enough samples
         if len(self.buffer) < self.batch_size:
@@ -177,7 +184,8 @@ class DQNModel(nn.Module):
             for i in range(self.batch_size):
                 action = actions[i].item()
                 reward = rewards[i]
-                next_state = next_states[i].unsqueeze(0) if next_states[i] is not None else None
+                next_state = next_states[i].unsqueeze(
+                    0) if next_states[i] is not None else None
                 if dones[i]:
                     q_target[i][action] = reward
                 else:
@@ -189,7 +197,8 @@ class DQNModel(nn.Module):
                         # print("next_state shape:", next_state.shape if next_state is not None else None)
                         # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
 
-                        q_target[i][action] = reward + self.gamma * torch.max(self.model(next_state))
+                        q_target[i][action] = reward + self.gamma * \
+                            torch.max(self.model(next_state))
                     else:
                         q_target[i][action] = reward
 
@@ -215,13 +224,10 @@ class DQNModel(nn.Module):
         # Also overwrite the top of tree so that we always have the latest to load if necessary:
         torch.save(self.model.state_dict(), file_path)
 
-
     def load_model(self, file_path):
         self.model.load_state_dict(torch.load(file_path))
 
 
-
-
 # Agent Implementation
 
 class DQNAgent(base_agent.BaseAgent):
@@ -245,21 +251,14 @@ class DQNAgent(base_agent.BaseAgent):
         self.move_number = 0
 
         # Used for tracking rewards for use in model saving/checkpointing
-        # -1's for 10 losses in a row
-        self.last_rewards = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]
+        self.last_rewards = deque(maxlen=100) # Keep the last 100 rewards
         self.episode_count = 0
         self.previous_avg_reward = -1
 
-
         if os.path.isfile(DATA_FILE):
             print("Loading previous model: ", DATA_FILE)
             self.dqn_model.load_model(DATA_FILE)
 
-    #used for reward tracking
-    def calculate_average_reward(self):
-        return sum(self.last_rewards) / len(self.last_rewards)
-
-
     # BOILER PLATE CODE AGAIN
 
     def transformDistance(self, x, x_distance, y, y_distance):
@@ -283,35 +282,36 @@ class DQNAgent(base_agent.BaseAgent):
             smart_action, x, y = smart_action.split('_')
 
         return (smart_action, x, y)
+    
+    # CUSTOM
 
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         if obs.last():
             self.episode_count += 1
             reward = obs.reward
-            self.last_rewards.pop(0)  # Remove the oldest reward
-            self.last_rewards.append(reward)  # Add the latest reward
-            avg_reward = self.calculate_average_reward()
+            self.last_rewards.append(reward) # Add the latest reward
+            avg_reward = sum(self.last_rewards) / len(self.last_rewards) # Calculate the average reward
 
             # Optimal reward is of course '1' (perfect string of wins)
-            if avg_reward > self.previous_avg_reward or avg_reward == 1:
+            if avg_reward > self.previous_avg_reward:
                 print("------------------------------------------------------")
                 print("Average reward was great enough to update the model!")
                 print("Previous average reward was: ", self.previous_avg_reward)
                 print("Our average reward was: ", avg_reward)
                 print("------------------------------------------------------")
 
-                self.dqn_model.learn(self.previous_state,
-                                    self.previous_action, reward, 'terminal', 'terminal')
                 self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
-
+            # Learn on every step, not just the successful ones:
+            self.dqn_model.learn(self.previous_state, self.previous_action, reward, 'terminal', 'terminal')
             self.previous_avg_reward = avg_reward
             self.previous_action = None
             self.previous_state = None
             self.move_number = 0
 
             return actions.FunctionCall(_NO_OP, [])
-
+        
+        # BOILER PLATE
 
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
@@ -352,12 +352,16 @@ class DQNAgent(base_agent.BaseAgent):
             enemy_y, enemy_x = (
                 obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
             for i in range(0, len(enemy_y)):
-                y = int(math.ceil((enemy_y[i] + 1) / 32))
-                x = int(math.ceil((enemy_x[i] + 1) / 32))
+                # Had to adjust due to out of range errors - screen is actually 84x84
+                y = int(math.ceil((enemy_y[i] + 1) / 42))
+                x = int(math.ceil((enemy_x[i] + 1) / 42))
 
                 index = ((y - 1) * 2) + (x - 1)
-                if index > 3:
-                    index = 3
+                # if index > 3:
+                #     print("INDEX OUT OF RANGE: ", index)
+                #     print("x is set to: ", x)
+                #     print("y is set to: ", y)
+                #     index = 3
 
                 hot_squares[index] = 1
 
@@ -371,12 +375,13 @@ class DQNAgent(base_agent.BaseAgent):
             friendly_y, friendly_x = (
                 obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
             for i in range(0, len(friendly_y)):
-                y = int(math.ceil((friendly_y[i] + 1) / 32))
-                x = int(math.ceil((friendly_x[i] + 1) / 32))
+                # Screen is actually 84x84, not 32x or 64x
+                y = int(math.ceil((friendly_y[i] + 1) / 42))
+                x = int(math.ceil((friendly_x[i] + 1) / 42))
 
                 index = ((y - 1) * 2) + (x - 1)
-                if index > 3:
-                    index = 3
+                # if index > 3:
+                #     index = 3
 
                 green_squares[index] = 1
 

commit 21afc2a2f8f35076ca856fc26306315d57fce91b
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 7 17:23:19 2023 -0400

    added intelligent checkpointing and model saves

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
index d2fac87f..35740b2f 100644
--- a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
@@ -53,7 +53,7 @@ _NOT_QUEUED = [0]
 _QUEUED = [1]
 _SELECT_ALL = [2]
 
-DATA_FILE = 'dqn-agent-model-v2.pt'
+DATA_FILE = 'dqn-agent-model-v3.pt'
 
 ACTION_DO_NOTHING = 'donothing'
 ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
@@ -100,7 +100,6 @@ class DQNModel(nn.Module):
         # Mixed Precision
         self.scaler = GradScaler()
 
-
         # Attempt GPU acceleration
         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
@@ -208,9 +207,15 @@ class DQNModel(nn.Module):
 
         self.optimizer.zero_grad()
 
-    def save_model(self, file_path):
+    def save_model(self, file_path, episode_count, reward):
+        # Save our checkpoint
+        save_path = f"{file_path}_episode_{episode_count}_reward_{reward:.2f}.pt"
+        torch.save(self.model.state_dict(), save_path)
+
+        # Also overwrite the top of tree so that we always have the latest to load if necessary:
         torch.save(self.model.state_dict(), file_path)
 
+
     def load_model(self, file_path):
         self.model.load_state_dict(torch.load(file_path))
 
@@ -239,9 +244,22 @@ class DQNAgent(base_agent.BaseAgent):
 
         self.move_number = 0
 
+        # Used for tracking rewards for use in model saving/checkpointing
+        # -1's for 10 losses in a row
+        self.last_rewards = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]
+        self.episode_count = 0
+        self.previous_avg_reward = -1
+
+
         if os.path.isfile(DATA_FILE):
+            print("Loading previous model: ", DATA_FILE)
             self.dqn_model.load_model(DATA_FILE)
 
+    #used for reward tracking
+    def calculate_average_reward(self):
+        return sum(self.last_rewards) / len(self.last_rewards)
+
+
     # BOILER PLATE CODE AGAIN
 
     def transformDistance(self, x, x_distance, y, y_distance):
@@ -269,18 +287,32 @@ class DQNAgent(base_agent.BaseAgent):
     def step(self, obs):
         super(DQNAgent, self).step(obs)
         if obs.last():
+            self.episode_count += 1
             reward = obs.reward
-            self.dqn_model.learn(self.previous_state,
-                                 self.previous_action, reward, 'terminal', 'terminal')
-            self.dqn_model.save_model(DATA_FILE)
+            self.last_rewards.pop(0)  # Remove the oldest reward
+            self.last_rewards.append(reward)  # Add the latest reward
+            avg_reward = self.calculate_average_reward()
+
+            # Optimal reward is of course '1' (perfect string of wins)
+            if avg_reward > self.previous_avg_reward or avg_reward == 1:
+                print("------------------------------------------------------")
+                print("Average reward was great enough to update the model!")
+                print("Previous average reward was: ", self.previous_avg_reward)
+                print("Our average reward was: ", avg_reward)
+                print("------------------------------------------------------")
+
+                self.dqn_model.learn(self.previous_state,
+                                    self.previous_action, reward, 'terminal', 'terminal')
+                self.dqn_model.save_model(DATA_FILE, self.episode_count, avg_reward)
 
+            self.previous_avg_reward = avg_reward
             self.previous_action = None
             self.previous_state = None
-
             self.move_number = 0
 
             return actions.FunctionCall(_NO_OP, [])
 
+
         unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
 
         if obs.first():

commit c2bf48c794ee3c23a6f6e19750de49b3f5bad8f9
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 7 16:10:02 2023 -0400

    adding v3 with GPU acceleration and mixed-precision FP16

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
new file mode 100644
index 00000000..d2fac87f
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v3.py
@@ -0,0 +1,475 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# Run the agent with the string:
+# python -m pysc2.bin.agent --map Simple64 --agent dqn_agent_v3.DQNAgent --agent_race terran --max_agent_steps 0 --norender --use_feature_units --difficulty very_easy --agent2_race terran 
+import torch
+import torch.nn as nn
+import torch.optim as optim
+# Using amp for mixed-precision (FP16) to improve RTX 4090 performance
+from torch.cuda.amp import autocast, GradScaler
+import numpy as np
+import pandas as pd
+import random
+import math
+import os
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+DATA_FILE = 'dqn-agent-model-v2.pt'
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+# End of BoilerPlate Code
+
+
+# Custom DQN Agent implementation with replay buffer
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, buffer_capacity=250000, batch_size=64):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        self.epsilon = e_greedy
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = []
+        self.position = 0
+        self.batch_size = batch_size
+        # Mixed Precision
+        self.scaler = GradScaler()
+
+
+        # Attempt GPU acceleration
+        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
+
+        self.model = nn.Sequential(
+            nn.Linear(self.state_size, 24),
+            nn.ReLU(),
+            nn.Linear(24, 24),
+            nn.ReLU(),
+            nn.Linear(24, len(self.actions))
+        ).to(self.device) # Move model to GPU if available
+
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    def push(self, state, action, reward, next_state, done):
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        if next_state is not None:
+            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(self.device)
+        else:
+            next_state = torch.zeros_like(state).to(self.device)
+        action = torch.tensor([action], dtype=torch.int64).to(self.device)
+        reward = torch.tensor([reward], dtype=torch.float32).to(self.device)
+        done = torch.tensor([done == 'terminal'], dtype=torch.bool).to(self.device)
+        self.buffer.append((state, action, reward, next_state, done))
+
+
+
+    # sample transitions from replay buffer
+    def sample(self, batch_size):
+        batch = random.sample(self.buffer, batch_size)
+        state, action, reward, next_state, done = map(torch.stack, zip(*batch))
+        return state, action, reward, next_state, done
+
+    def forward(self, x):
+        return self.model(x)
+
+    def choose_action(self, observation, excluded_actions=[]):
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(self.device) # Move observation to GPU
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            return np.random.choice(available_actions)
+        
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def learn(self, s, a, r, s_, done):
+        s = torch.tensor(s, dtype=torch.float32).to(self.device)
+        if s_ != 'terminal':
+            s_ = torch.tensor(s_, dtype=torch.float32).to(self.device)
+        # Store the current transition in the replay buffer
+        self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
+            s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        states, actions, rewards, next_states, dones = self.sample(
+            self.batch_size)
+
+        # Squeeze the actions tensor to remove the extra dimension
+        actions = actions.squeeze(1)
+
+        # Compute the Q-values for the current states with mixed precision/autocast
+        with autocast():
+            q_predict = self.model(states).squeeze(1)
+            q_target = q_predict.clone().detach().squeeze(1)
+
+            # Compute the target Q-values within autocast
+            for i in range(self.batch_size):
+                action = actions[i].item()
+                reward = rewards[i]
+                next_state = next_states[i].unsqueeze(0) if next_states[i] is not None else None
+                if dones[i]:
+                    q_target[i][action] = reward
+                else:
+                    if next_state is not None:
+                        # print("q_target shape:", q_target.shape)
+                        # print("i:", i)
+                        # print("action:", action)
+                        # print("reward:", reward)
+                        # print("next_state shape:", next_state.shape if next_state is not None else None)
+                        # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
+
+                        q_target[i][action] = reward + self.gamma * torch.max(self.model(next_state))
+                    else:
+                        q_target[i][action] = reward
+
+            # Compute the loss and update the model's weights (FP16/mixed precision)
+            loss = self.loss_fn(q_predict, q_target)
+
+        # Backward pass with scaling
+        self.scaler.scale(loss).backward()
+
+        # Update the model's weights with scaling
+        self.scaler.step(self.optimizer)
+
+        # Update the scale for next iteration
+        self.scaler.update()
+
+        self.optimizer.zero_grad()
+
+    def save_model(self, file_path):
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        self.model.load_state_dict(torch.load(file_path))
+
+
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        if os.path.isfile(DATA_FILE):
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+
+        return [x + x_distance, y + y_distance]
+
+    def transformLocation(self, x, y):
+        if not self.base_top_left:
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        if obs.last():
+            reward = obs.reward
+            self.dqn_model.learn(self.previous_state,
+                                 self.previous_action, reward, 'terminal', 'terminal')
+            self.dqn_model.save_model(DATA_FILE)
+
+            self.previous_action = None
+            self.previous_state = None
+
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                y = int(math.ceil((enemy_y[i] + 1) / 32))
+                x = int(math.ceil((enemy_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                y = int(math.ceil((friendly_y[i] + 1) / 32))
+                x = int(math.ceil((friendly_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.learn(self.previous_state,
+                                     self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            if supply_depot_count == 2 or worker_supply == 0:
+                excluded_actions.append(1)
+
+            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                excluded_actions.append(3)
+
+            if army_supply == 0:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if supply_depot_count < 2 and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if supply_depot_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -35, round(self.cc_y.mean()), 0)
+                        elif supply_depot_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -25, round(self.cc_y.mean()), -25)
+
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if barracks_count < 2 and _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if barracks_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), -9)
+                        elif barracks_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), 12)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])

commit 0d666b951671c9eec312094e06f0192e6b9bd4ce
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Aug 7 15:34:39 2023 -0400

    adding DQN models - git cli is broken b/c of large file

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn-agent-agent-performance-data.csv b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn-agent-agent-performance-data.csv
new file mode 100644
index 00000000..1f09f92f
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn-agent-agent-performance-data.csv
@@ -0,0 +1,164 @@
+agent_name,agent_version,episode_count,game_steps,outcome,reward,score,opponent_race,bot_difficulty
+dqn-agent,0.01,56,15872,-1,-1,7505,terran,very_easy
+dqn-agent,0.01,57,11032,-1,-1,4365,terran,very_easy
+dqn-agent,0.01,58,16616,-1,-1,7940,terran,very_easy
+dqn-agent,0.01,59,24496,-1,-1,7165,terran,very_easy
+dqn-agent,0.01,60,10488,-1,-1,4140,terran,very_easy
+dqn-agent,0.01,61,19576,-1,-1,4660,terran,very_easy
+dqn-agent,0.01,62,27760,-1,-1,6950,terran,very_easy
+dqn-agent,0.01,63,11016,-1,-1,4075,terran,very_easy
+dqn-agent,0.01,64,28800,0,0,11380,terran,very_easy
+dqn-agent,0.01,65,23024,-1,-1,6320,terran,very_easy
+dqn-agent,0.01,66,11312,-1,-1,4655,terran,very_easy
+dqn-agent,0.01,67,19288,-1,-1,4820,terran,very_easy
+dqn-agent,0.01,68,10792,-1,-1,3930,terran,very_easy
+dqn-agent,0.01,69,27984,-1,-1,6205,terran,very_easy
+dqn-agent,0.01,70,18656,-1,-1,3915,terran,very_easy
+dqn-agent,0.01,71,11320,-1,-1,4355,terran,very_easy
+dqn-agent,0.01,72,23136,-1,-1,5420,terran,very_easy
+dqn-agent,0.01,73,10568,-1,-1,3970,terran,very_easy
+dqn-agent,0.01,74,11304,-1,-1,4500,terran,very_easy
+dqn-agent,0.01,75,24640,-1,-1,6560,terran,very_easy
+dqn-agent,0.01,76,24296,-1,-1,6155,terran,very_easy
+dqn-agent,0.01,77,28800,0,0,11300,terran,very_easy
+dqn-agent,0.01,78,10400,-1,-1,3960,terran,very_easy
+dqn-agent,0.01,79,24184,-1,-1,6805,terran,very_easy
+dqn-agent,0.01,80,21144,-1,-1,5040,terran,very_easy
+dqn-agent,0.01,81,10808,-1,-1,4050,terran,very_easy
+dqn-agent,0.01,82,10840,-1,-1,4075,terran,very_easy
+dqn-agent,0.01,83,26040,-1,-1,6325,terran,very_easy
+dqn-agent,0.01,84,11384,-1,-1,4540,terran,very_easy
+dqn-agent,0.01,85,18048,-1,-1,4240,terran,very_easy
+dqn-agent,0.01,86,10960,-1,-1,4245,terran,very_easy
+dqn-agent,0.01,87,22784,-1,-1,5425,terran,very_easy
+dqn-agent,0.01,88,10504,-1,-1,3815,terran,very_easy
+dqn-agent,0.01,89,11328,-1,-1,4395,terran,very_easy
+dqn-agent,0.01,90,10952,-1,-1,4460,terran,very_easy
+dqn-agent,0.01,91,10616,-1,-1,3730,terran,very_easy
+dqn-agent,0.01,92,10944,-1,-1,4140,terran,very_easy
+dqn-agent,0.01,93,20608,-1,-1,3565,terran,very_easy
+dqn-agent,0.01,94,19624,-1,-1,3710,terran,very_easy
+dqn-agent,0.01,95,28800,0,0,7895,terran,very_easy
+dqn-agent,0.01,96,20320,-1,-1,4405,terran,very_easy
+dqn-agent,0.01,97,10880,-1,-1,3815,terran,very_easy
+dqn-agent,0.01,98,10432,-1,-1,3825,terran,very_easy
+dqn-agent,0.01,99,10752,-1,-1,4225,terran,very_easy
+dqn-agent,0.01,100,22928,-1,-1,6810,terran,very_easy
+dqn-agent,0.01,101,28800,0,0,10150,terran,very_easy
+dqn-agent,0.01,102,16184,-1,-1,7550,terran,very_easy
+dqn-agent,0.01,103,10544,-1,-1,4060,terran,very_easy
+dqn-agent,0.01,104,11144,-1,-1,4010,terran,very_easy
+dqn-agent,0.01,105,11520,-1,-1,4295,terran,very_easy
+dqn-agent,0.01,106,10624,-1,-1,3700,terran,very_easy
+dqn-agent,0.01,107,24160,-1,-1,4510,terran,very_easy
+dqn-agent,0.01,108,11504,-1,-1,4285,terran,very_easy
+dqn-agent,0.01,109,10920,-1,-1,4245,terran,very_easy
+dqn-agent,0.01,110,28800,0,0,10350,terran,very_easy
+dqn-agent,0.01,111,25240,-1,-1,7480,terran,very_easy
+dqn-agent,0.01,112,23432,-1,-1,4685,terran,very_easy
+dqn-agent,0.01,113,10960,-1,-1,3910,terran,very_easy
+dqn-agent,0.01,114,26488,-1,-1,5180,terran,very_easy
+dqn-agent,0.01,115,16272,-1,-1,7550,terran,very_easy
+dqn-agent,0.01,116,22952,-1,-1,5405,terran,very_easy
+dqn-agent,0.01,117,11320,-1,-1,4465,terran,very_easy
+dqn-agent,0.01,118,11280,-1,-1,4310,terran,very_easy
+dqn-agent,0.01,119,25784,-1,-1,7305,terran,very_easy
+dqn-agent,0.01,120,28800,0,0,9250,terran,very_easy
+dqn-agent,0.01,121,11400,-1,-1,4670,terran,very_easy
+dqn-agent,0.01,122,11480,-1,-1,4465,terran,very_easy
+dqn-agent,0.01,123,17272,-1,-1,4605,terran,very_easy
+dqn-agent,0.01,124,28800,0,0,7750,terran,very_easy
+dqn-agent,0.01,125,10648,-1,-1,3905,terran,very_easy
+dqn-agent,0.01,126,16160,-1,-1,7665,terran,very_easy
+dqn-agent,0.01,127,23152,-1,-1,5080,terran,very_easy
+dqn-agent,0.01,128,20264,-1,-1,4590,terran,very_easy
+dqn-agent,0.01,129,10880,-1,-1,4205,terran,very_easy
+dqn-agent,0.01,130,24320,-1,-1,6360,terran,very_easy
+dqn-agent,0.01,131,23752,-1,-1,7285,terran,very_easy
+dqn-agent,0.01,132,28640,-1,-1,6490,terran,very_easy
+dqn-agent,0.01,133,22936,-1,-1,6865,terran,very_easy
+dqn-agent,0.01,134,11280,-1,-1,4315,terran,very_easy
+dqn-agent,0.01,135,22328,-1,-1,6795,terran,very_easy
+dqn-agent,0.01,136,10992,-1,-1,4235,terran,very_easy
+dqn-agent,0.01,137,27568,-1,-1,6000,terran,very_easy
+dqn-agent,0.01,138,10736,-1,-1,3930,terran,very_easy
+dqn-agent,0.01,139,24472,-1,-1,7035,terran,very_easy
+dqn-agent,0.01,140,11184,-1,-1,4475,terran,very_easy
+dqn-agent,0.01,141,28800,0,0,10295,terran,very_easy
+dqn-agent,0.01,142,25000,-1,-1,5170,terran,very_easy
+dqn-agent,0.01,143,23024,-1,-1,4730,terran,very_easy
+dqn-agent,0.01,144,10872,-1,-1,4085,terran,very_easy
+dqn-agent,0.01,145,24256,-1,-1,6290,terran,very_easy
+dqn-agent,0.01,146,11600,-1,-1,4420,terran,very_easy
+dqn-agent,0.01,147,24464,-1,-1,7250,terran,very_easy
+dqn-agent,0.01,148,19368,-1,-1,5290,terran,very_easy
+dqn-agent,0.01,149,11024,-1,-1,3905,terran,very_easy
+dqn-agent,0.01,150,20312,-1,-1,6590,terran,very_easy
+dqn-agent,0.01,151,25152,-1,-1,5790,terran,very_easy
+dqn-agent,0.01,152,11336,-1,-1,4610,terran,very_easy
+dqn-agent,0.01,153,28800,0,0,11345,terran,very_easy
+dqn-agent,0.01,154,11472,-1,-1,4605,terran,very_easy
+dqn-agent,0.01,155,26512,-1,-1,6300,terran,very_easy
+dqn-agent,0.01,156,10632,-1,-1,4060,terran,very_easy
+dqn-agent,0.01,157,28800,0,0,9400,terran,very_easy
+dqn-agent,0.01,158,23056,-1,-1,5805,terran,very_easy
+dqn-agent,0.01,159,15992,-1,-1,7840,terran,very_easy
+dqn-agent,0.01,160,19112,-1,-1,4705,terran,very_easy
+dqn-agent,0.01,161,10528,-1,-1,3800,terran,very_easy
+dqn-agent,0.01,162,28800,0,0,8150,terran,very_easy
+dqn-agent,0.01,163,10616,-1,-1,3645,terran,very_easy
+dqn-agent,0.01,164,24896,-1,-1,6040,terran,very_easy
+dqn-agent,0.01,165,11432,-1,-1,4500,terran,very_easy
+dqn-agent,0.01,166,23664,-1,-1,7415,terran,very_easy
+dqn-agent,0.01,167,10952,-1,-1,4235,terran,very_easy
+dqn-agent,0.01,168,28800,0,0,10350,terran,very_easy
+dqn-agent,0.01,169,10760,-1,-1,4065,terran,very_easy
+dqn-agent,0.01,170,11232,-1,-1,4315,terran,very_easy
+dqn-agent,0.01,171,10952,-1,-1,4245,terran,very_easy
+dqn-agent,0.01,172,10992,-1,-1,4160,terran,very_easy
+dqn-agent,0.01,173,11456,-1,-1,4465,terran,very_easy
+dqn-agent,0.01,174,10432,-1,-1,3835,terran,very_easy
+dqn-agent,0.01,175,10856,-1,-1,4160,terran,very_easy
+dqn-agent,0.01,176,10576,-1,-1,3900,terran,very_easy
+dqn-agent,0.01,177,11160,-1,-1,4385,terran,very_easy
+dqn-agent,0.01,178,24000,-1,-1,6335,terran,very_easy
+dqn-agent,0.01,179,11112,-1,-1,4215,terran,very_easy
+dqn-agent,0.01,180,10632,-1,-1,3960,terran,very_easy
+dqn-agent,0.01,181,11328,-1,-1,4430,terran,very_easy
+dqn-agent,0.01,182,25480,-1,-1,6880,terran,very_easy
+dqn-agent,0.01,183,10752,-1,-1,3945,terran,very_easy
+dqn-agent,0.01,184,28560,-1,-1,6450,terran,very_easy
+dqn-agent,0.01,185,11064,-1,-1,4100,terran,very_easy
+dqn-agent,0.01,186,24448,-1,-1,6595,terran,very_easy
+dqn-agent,0.01,187,28400,-1,-1,6570,terran,very_easy
+dqn-agent,0.01,188,27728,-1,-1,6810,terran,very_easy
+dqn-agent,0.01,189,23048,-1,-1,6680,terran,very_easy
+dqn-agent,0.01,190,10528,-1,-1,3845,terran,very_easy
+dqn-agent,0.01,191,19024,-1,-1,5190,terran,very_easy
+dqn-agent,0.01,192,10984,-1,-1,4250,terran,very_easy
+dqn-agent,0.01,193,27168,-1,-1,5500,terran,very_easy
+dqn-agent,0.01,194,24808,-1,-1,4495,terran,very_easy
+dqn-agent,0.01,195,24144,-1,-1,5780,terran,very_easy
+dqn-agent,0.01,196,11624,-1,-1,4590,terran,very_easy
+dqn-agent,0.01,197,10912,-1,-1,3755,terran,very_easy
+dqn-agent,0.01,198,11128,-1,-1,4410,terran,very_easy
+dqn-agent,0.01,199,11448,-1,-1,4395,terran,very_easy
+dqn-agent,0.01,200,10752,-1,-1,3860,terran,very_easy
+dqn-agent,0.01,201,10656,-1,-1,3955,terran,very_easy
+dqn-agent,0.01,202,28800,0,0,7845,terran,very_easy
+dqn-agent,0.01,203,28800,0,0,11200,terran,very_easy
+dqn-agent,0.01,204,11216,-1,-1,4275,terran,very_easy
+dqn-agent,0.01,205,11520,-1,-1,4495,terran,very_easy
+dqn-agent,0.01,206,25400,-1,-1,6180,terran,very_easy
+dqn-agent,0.01,207,24000,-1,-1,5910,terran,very_easy
+dqn-agent,0.01,208,10728,-1,-1,4015,terran,very_easy
+dqn-agent,0.01,209,11544,-1,-1,4590,terran,very_easy
+dqn-agent,0.01,210,11344,-1,-1,4455,terran,very_easy
+dqn-agent,0.01,211,10960,-1,-1,4285,terran,very_easy
+dqn-agent,0.01,212,11376,-1,-1,4290,terran,very_easy
+dqn-agent,0.01,213,24208,-1,-1,5825,terran,very_easy
+dqn-agent,0.01,214,22600,-1,-1,6460,terran,very_easy
+dqn-agent,0.01,215,23432,-1,-1,5270,terran,very_easy
+dqn-agent,0.01,216,10976,-1,-1,4130,terran,very_easy
+dqn-agent,0.01,217,10232,-1,-1,3075,terran,very_easy
+dqn-agent,0.01,218,23016,-1,-1,6135,terran,very_easy
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent.py
new file mode 100644
index 00000000..93fcb9eb
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent.py
@@ -0,0 +1,460 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# import tensorflow as tf
+# For debugging
+# tf.debugging.set_log_device_placement(True)
+# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)
+
+import torch
+import torch.nn as nn 
+import torch.optim as optim
+import numpy as np
+import pandas as pd
+import random, math, os 
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+
+
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot: 
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45 
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+DATA_FILE = 'dqn-agent-model.pt'
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+### DQN State size
+STATE_SIZE = 12
+
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))
+
+#### End of BoilerPlate Code
+
+
+# Custom DQN Agent implementation
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        self.epsilon = e_greedy
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        
+        self.model = nn.Sequential(
+            nn.Linear(self.state_size, 24),
+            nn.ReLU(),
+            nn.Linear(24, 24),
+            nn.ReLU(),
+            nn.Linear(24, len(self.actions))
+        )
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    def forward(self, x):
+        return self.model(x)
+
+    def choose_action(self, observation, excluded_actions=[]):
+        if np.random.uniform() < self.epsilon:
+            available_actions = [a for a in self.actions if a not in excluded_actions]
+            return np.random.choice(available_actions)
+
+        observation = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def learn(self, s, a, r, s_):
+        s = torch.tensor(s, dtype=torch.float32).unsqueeze(0)
+        q_predict = self.model(s)[0]
+        q_target = q_predict.clone().detach()
+
+        if s_ == 'terminal':
+            q_target[a] = r
+        else:
+            s_ = torch.tensor(s_, dtype=torch.float32).unsqueeze(0)
+            if s_ in self.disallowed_actions:
+                for excluded_action in self.disallowed_actions[s_]:
+                    q_target[excluded_action] = float('-inf')
+
+            q_target[a] = r + self.gamma * torch.max(self.model(s_))
+
+        loss = self.loss_fn(q_predict, q_target)
+        self.optimizer.zero_grad()
+        loss.backward()
+        self.optimizer.step()
+
+    def save_model(self, file_path):
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        self.model.load_state_dict(torch.load(file_path))
+
+
+# Using Tensorflow...had many bugs
+# class DQNModel:
+#     def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):
+#         self.actions = actions
+#         self.lr = learning_rate
+#         self.gamma = reward_decay
+#         self.epsilon = e_greedy
+#         self.state_size = state_size
+#         self.model = self.build_model()
+#         self.disallowed_actions = {}
+
+#     def build_model(self):
+#         print("self.state_size is set to: ", self.state_size)
+#         print("Self actions are set to: ", self.actions)
+#         model = tf.keras.Sequential([
+#             tf.keras.layers.Dense(24, activation='relu', input_shape=(self.state_size,)),
+#             tf.keras.layers.Dense(24, activation='relu'),
+#             tf.keras.layers.Dense(len(self.actions))
+#         ])
+
+
+#     def choose_action(self, observation, excluded_actions=[]):
+#         if np.random.uniform() < self.epsilon:
+#             available_actions = [a for a in self.actions if a not in excluded_actions]
+#             return np.random.choice(available_actions)
+
+#         q_values = self.model.predict(np.array([observation]))
+#         for action in excluded_actions:
+#             q_values[0][action] = -np.inf
+
+#         return np.argmax(q_values)
+
+#     def learn(self, s, a, r, s_):
+#         q_predict = self.model.predict(np.array([s]))[0]
+#         q_target = q_predict.copy()
+
+#         if s_ == 'terminal':
+#             q_target[a] = r
+#         else:
+#             if s_ in self.disallowed_actions:
+#                 for excluded_action in self.disallowed_actions[s_]:
+#                     q_target[excluded_action] = -np.inf
+
+#             q_target[a] = r + self.gamma * np.max(self.model.predict(np.array([s_])))
+
+#         self.model.fit(np.array([s]), np.array([q_target]), epochs=1, verbose=0)
+
+
+#     def save_model(self, file_path):
+#         self.model.save(file_path)
+
+#     def load_model(self, file_path):
+#         self.model = tf.keras.models.load_model(file_path)
+
+
+
+#### Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        if os.path.isfile(DATA_FILE):
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+        
+        return [x + x_distance, y + y_distance]
+    
+    def transformLocation(self, x, y):
+        if not self.base_top_left:
+            return [64 - x, 64 - y]
+        
+        return [x, y]
+    
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+            
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+        
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        
+        # if obs.last():
+        #     reward = obs.reward
+        
+        #     self.qlearn.learn(str(self.previous_state), self.previous_action, reward, 'terminal')
+            
+        #     self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')
+            
+        #     self.previous_action = None
+        #     self.previous_state = None
+            
+        #     self.move_number = 0
+            
+        #     return actions.FunctionCall(_NO_OP, [])
+        if obs.last():
+            reward = obs.reward
+            self.dqn_model.learn(self.previous_state, self.previous_action, reward, 'terminal')
+            self.dqn_model.save_model(DATA_FILE)
+
+            self.previous_action = None
+            self.previous_state = None
+
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+        
+            self.cc_y, self.cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+        
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+            
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+        
+        supply_free = supply_limit - supply_used
+        
+        if self.move_number == 0:
+            self.move_number += 1
+            
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+    
+            hot_squares = np.zeros(4)        
+            enemy_y, enemy_x = (obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                y = int(math.ceil((enemy_y[i] + 1) / 32))
+                x = int(math.ceil((enemy_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+                
+                hot_squares[index] = 1
+            
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+            
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+            
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                y = int(math.ceil((friendly_y[i] + 1) / 32))
+                x = int(math.ceil((friendly_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+
+                green_squares[index] = 1
+            
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+            
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+    
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.learn(self.previous_state, self.previous_action, 0, current_state)
+
+            excluded_actions = []
+            if supply_depot_count == 2 or worker_supply == 0:
+                excluded_actions.append(1)
+                
+            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                excluded_actions.append(3)
+                
+            if army_supply == 0:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+        
+            smart_action, x, y = self.splitAction(self.previous_action)
+            
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+                    
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+                    
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+                
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+            
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+                
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+        
+        elif self.move_number == 1:
+            self.move_number += 1
+            
+            smart_action, x, y = self.splitAction(self.previous_action)
+                
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if supply_depot_count < 2 and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if supply_depot_count == 0:
+                            target = self.transformDistance(round(self.cc_x.mean()), -35, round(self.cc_y.mean()), 0)
+                        elif supply_depot_count == 1:
+                            target = self.transformDistance(round(self.cc_x.mean()), -25, round(self.cc_y.mean()), -25)
+    
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+            
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if barracks_count < 2 and _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if  barracks_count == 0:
+                            target = self.transformDistance(round(self.cc_x.mean()), 15, round(self.cc_y.mean()), -9)
+                        elif  barracks_count == 1:
+                            target = self.transformDistance(round(self.cc_x.mean()), 15, round(self.cc_y.mean()), 12)
+    
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+    
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+        
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+                
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+                
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+                
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+                    
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+                
+        elif self.move_number == 2:
+            self.move_number = 0
+            
+            smart_action, x, y = self.splitAction(self.previous_action)
+                
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+                    
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+                        
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+                        
+                        target = [int(m_x), int(m_y)]
+                        
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+        
+        return actions.FunctionCall(_NO_OP, [])
\ No newline at end of file
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v2.py b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v2.py
new file mode 100644
index 00000000..9ef7cb35
--- /dev/null
+++ b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/dqn_agent_v2.py
@@ -0,0 +1,458 @@
+# This is my reinforcement-learning agent, inspired by the 2017 efforts of Steven Brown (PySC2 Dev) who used Q-Learning in his blog series
+# The PySC2 test harness that he built for his agents is leveraged (somewhat...) so that
+# The RL Algorithm can properly interact with StarCraft II / PySC2
+# All of the RL algorithms were implemented by me
+
+# import tensorflow as tf
+# For debugging
+# tf.debugging.set_log_device_placement(True)
+# tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)
+
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import numpy as np
+import pandas as pd
+import random
+import math
+import os
+from pysc2.agents import base_agent
+from pysc2.lib import actions
+from pysc2.lib import features
+
+
+# BoilerPlate code from Steven Brown's Q-Learning Implementation found here:
+# https://github.com/skjb/pysc2-tutorial/tree/master
+# Specifically, the Q-Learning bot:
+# https://github.com/skjb/pysc2-tutorial/blob/master/Refining%20the%20Sparse%20Reward%20Agent/refined_agent.py
+# His boilerplate code was written in 2017 and needed some updating (e.g many pandas functions have been deprecated)
+
+_NO_OP = actions.FUNCTIONS.no_op.id
+_SELECT_POINT = actions.FUNCTIONS.select_point.id
+_BUILD_SUPPLY_DEPOT = actions.FUNCTIONS.Build_SupplyDepot_screen.id
+_BUILD_BARRACKS = actions.FUNCTIONS.Build_Barracks_screen.id
+_TRAIN_MARINE = actions.FUNCTIONS.Train_Marine_quick.id
+_SELECT_ARMY = actions.FUNCTIONS.select_army.id
+_ATTACK_MINIMAP = actions.FUNCTIONS.Attack_minimap.id
+_HARVEST_GATHER = actions.FUNCTIONS.Harvest_Gather_screen.id
+
+_PLAYER_RELATIVE = features.SCREEN_FEATURES.player_relative.index
+_UNIT_TYPE = features.SCREEN_FEATURES.unit_type.index
+_PLAYER_ID = features.SCREEN_FEATURES.player_id.index
+
+_PLAYER_SELF = 1
+_PLAYER_HOSTILE = 4
+_ARMY_SUPPLY = 5
+
+_TERRAN_COMMANDCENTER = 18
+_TERRAN_SCV = 45
+_TERRAN_SUPPLY_DEPOT = 19
+_TERRAN_BARRACKS = 21
+_NEUTRAL_MINERAL_FIELD = 341
+
+_NOT_QUEUED = [0]
+_QUEUED = [1]
+_SELECT_ALL = [2]
+
+DATA_FILE = 'dqn-agent-model-v2.pt'
+
+ACTION_DO_NOTHING = 'donothing'
+ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'
+ACTION_BUILD_BARRACKS = 'buildbarracks'
+ACTION_BUILD_MARINE = 'buildmarine'
+ACTION_ATTACK = 'attack'
+
+smart_actions = [
+    ACTION_DO_NOTHING,
+    ACTION_BUILD_SUPPLY_DEPOT,
+    ACTION_BUILD_BARRACKS,
+    ACTION_BUILD_MARINE,
+]
+
+# DQN State size
+STATE_SIZE = 12
+
+
+for mm_x in range(0, 64):
+    for mm_y in range(0, 64):
+        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:
+            smart_actions.append(ACTION_ATTACK + '_' +
+                                 str(mm_x - 16) + '_' + str(mm_y - 16))
+
+# End of BoilerPlate Code
+
+
+# Custom DQN Agent implementation with replay buffer
+
+class DQNModel(nn.Module):
+    def __init__(self, actions, state_size, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9, buffer_capacity=250000, batch_size=64):
+        super(DQNModel, self).__init__()
+        self.actions = actions
+        self.lr = learning_rate
+        self.gamma = reward_decay
+        self.epsilon = e_greedy
+        self.state_size = state_size
+        self.disallowed_actions = {}
+        # Replay buffer
+        self.buffer_capacity = buffer_capacity
+        self.buffer = []
+        self.position = 0
+        self.batch_size = batch_size
+
+        self.model = nn.Sequential(
+            nn.Linear(self.state_size, 24),
+            nn.ReLU(),
+            nn.Linear(24, 24),
+            nn.ReLU(),
+            nn.Linear(24, len(self.actions))
+        )
+        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
+        self.loss_fn = nn.MSELoss()
+
+    def push(self, state, action, reward, next_state, done):
+        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert state to tensor
+        if next_state is not None:
+            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)  # Convert next_state to tensor
+        else:
+            next_state = torch.zeros_like(state)  # Use a tensor of zeros for the terminal state
+        action = torch.tensor([action], dtype=torch.int64)  # Convert action to tensor
+        reward = torch.tensor([reward], dtype=torch.float32)  # Convert reward to tensor
+        done = torch.tensor([done == 'terminal'], dtype=torch.bool)  # Convert done to tensor
+        self.buffer.append((state, action, reward, next_state, done))
+
+
+    # sample transitions from replay buffer
+    def sample(self, batch_size):
+        batch = random.sample(self.buffer, batch_size)
+        state, action, reward, next_state, done = map(torch.stack, zip(*batch))
+        return state, action, reward, next_state, done
+
+    def forward(self, x):
+        return self.model(x)
+
+    def choose_action(self, observation, excluded_actions=[]):
+        if np.random.uniform() < self.epsilon:
+            available_actions = [
+                a for a in self.actions if a not in excluded_actions]
+            return np.random.choice(available_actions)
+
+        observation = torch.tensor(
+            observation, dtype=torch.float32).unsqueeze(0)
+        q_values = self.model(observation)
+        for action in excluded_actions:
+            q_values[0][action] = float('-inf')
+
+        return torch.argmax(q_values).item()
+
+    def learn(self, s, a, r, s_, done):
+        # Store the current transition in the replay buffer
+        self.push(torch.tensor(s, dtype=torch.float32), a, r, torch.tensor(
+            s_, dtype=torch.float32) if s_ != 'terminal' else None, done)
+
+        # Check if the replay buffer has enough samples
+        if len(self.buffer) < self.batch_size:
+            return
+
+        # Sample a mini-batch of transitions from the buffer
+        states, actions, rewards, next_states, dones = self.sample(
+            self.batch_size)
+
+        # Squeeze the actions tensor to remove the extra dimension
+        actions = actions.squeeze(1)
+
+        # Compute the Q-values for the current states
+        q_predict = self.model(states).squeeze(1)
+        q_target = q_predict.clone().detach().squeeze(1)
+
+
+        # Compute the target Q-values
+        for i in range(self.batch_size):
+            action = actions[i].item()
+            reward = rewards[i]
+            next_state = next_states[i].unsqueeze(0) if next_states[i] is not None else None
+            if dones[i]:
+                q_target[i][action] = reward
+            else:
+                if next_state is not None:
+                    # print("q_target shape:", q_target.shape)
+                    # print("i:", i)
+                    # print("action:", action)
+                    # print("reward:", reward)
+                    # print("next_state shape:", next_state.shape if next_state is not None else None)
+                    # print("self.model(next_state) shape:", self.model(next_state).shape if next_state is not None else None)
+
+                    q_target[i][action] = reward + self.gamma * torch.max(self.model(next_state))
+                else:
+                    q_target[i][action] = reward
+
+        # Compute the loss and update the model's weights
+        loss = self.loss_fn(q_predict, q_target)
+        self.optimizer.zero_grad()
+        loss.backward()
+        self.optimizer.step()
+
+    def save_model(self, file_path):
+        torch.save(self.model.state_dict(), file_path)
+
+    def load_model(self, file_path):
+        self.model.load_state_dict(torch.load(file_path))
+
+
+
+
+# Agent Implementation
+
+class DQNAgent(base_agent.BaseAgent):
+    def __init__(self):
+        super(DQNAgent, self).__init__()
+
+        initial_actions = list(range(len(smart_actions)))
+        print("Creating the model with the following attributes:")
+        print("Actions are set to:", initial_actions)
+        print("State Size:", STATE_SIZE)
+
+        self.dqn_model = DQNModel(
+            actions=initial_actions, state_size=STATE_SIZE)
+
+        self.previous_action = None
+        self.previous_state = None
+
+        self.cc_y = None
+        self.cc_x = None
+
+        self.move_number = 0
+
+        if os.path.isfile(DATA_FILE):
+            self.dqn_model.load_model(DATA_FILE)
+
+    # BOILER PLATE CODE AGAIN
+
+    def transformDistance(self, x, x_distance, y, y_distance):
+        if not self.base_top_left:
+            return [x - x_distance, y - y_distance]
+
+        return [x + x_distance, y + y_distance]
+
+    def transformLocation(self, x, y):
+        if not self.base_top_left:
+            return [64 - x, 64 - y]
+
+        return [x, y]
+
+    def splitAction(self, action_id):
+        smart_action = smart_actions[action_id]
+
+        x = 0
+        y = 0
+        if '_' in smart_action:
+            smart_action, x, y = smart_action.split('_')
+
+        return (smart_action, x, y)
+
+    def step(self, obs):
+        super(DQNAgent, self).step(obs)
+        if obs.last():
+            reward = obs.reward
+            self.dqn_model.learn(self.previous_state,
+                                 self.previous_action, reward, 'terminal', 'terminal')
+            self.dqn_model.save_model(DATA_FILE)
+
+            self.previous_action = None
+            self.previous_state = None
+
+            self.move_number = 0
+
+            return actions.FunctionCall(_NO_OP, [])
+
+        unit_type = obs.observation["feature_screen"][_UNIT_TYPE]
+
+        if obs.first():
+            player_y, player_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0
+
+            self.cc_y, self.cc_x = (
+                unit_type == _TERRAN_COMMANDCENTER).nonzero()
+
+        cc_y, cc_x = (unit_type == _TERRAN_COMMANDCENTER).nonzero()
+        cc_count = 1 if cc_y.any() else 0
+
+        depot_y, depot_x = (unit_type == _TERRAN_SUPPLY_DEPOT).nonzero()
+        supply_depot_count = int(round(len(depot_y) / 69))
+
+        barracks_y, barracks_x = (unit_type == _TERRAN_BARRACKS).nonzero()
+        barracks_count = int(round(len(barracks_y) / 137))
+
+        supply_used = obs.observation['player'][3]
+        supply_limit = obs.observation['player'][4]
+        army_supply = obs.observation['player'][5]
+        worker_supply = obs.observation['player'][6]
+
+        supply_free = supply_limit - supply_used
+
+        if self.move_number == 0:
+            self.move_number += 1
+
+            current_state = np.zeros(12)
+            current_state[0] = cc_count
+            current_state[1] = supply_depot_count
+            current_state[2] = barracks_count
+            current_state[3] = obs.observation['player'][_ARMY_SUPPLY]
+
+            hot_squares = np.zeros(4)
+            enemy_y, enemy_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_HOSTILE).nonzero()
+            for i in range(0, len(enemy_y)):
+                y = int(math.ceil((enemy_y[i] + 1) / 32))
+                x = int(math.ceil((enemy_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+
+                hot_squares[index] = 1
+
+            if not self.base_top_left:
+                hot_squares = hot_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 4] = hot_squares[i]
+
+            green_squares = np.zeros(4)
+            friendly_y, friendly_x = (
+                obs.observation["feature_screen"][_PLAYER_RELATIVE] == _PLAYER_SELF).nonzero()
+            for i in range(0, len(friendly_y)):
+                y = int(math.ceil((friendly_y[i] + 1) / 32))
+                x = int(math.ceil((friendly_x[i] + 1) / 32))
+
+                index = ((y - 1) * 2) + (x - 1)
+                if index > 3:
+                    index = 3
+
+                green_squares[index] = 1
+
+            if not self.base_top_left:
+                green_squares = green_squares[::-1]
+
+            for i in range(0, 4):
+                current_state[i + 8] = green_squares[i]
+
+            # Updated to DQN implementation
+            if self.previous_action is not None:
+                self.dqn_model.learn(self.previous_state,
+                                     self.previous_action, 0, current_state, 0)
+
+            excluded_actions = []
+            if supply_depot_count == 2 or worker_supply == 0:
+                excluded_actions.append(1)
+
+            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:
+                excluded_actions.append(2)
+
+            if supply_free == 0 or barracks_count == 0:
+                excluded_actions.append(3)
+
+            if army_supply == 0:
+                excluded_actions.append(4)
+                excluded_actions.append(5)
+                excluded_actions.append(6)
+                excluded_actions.append(7)
+
+            # Updated for DQN
+            rl_action = self.dqn_model.choose_action(
+                current_state, excluded_actions)
+
+            self.previous_state = current_state
+            self.previous_action = rl_action
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                unit_y, unit_x = (unit_type == _TERRAN_SCV).nonzero()
+
+                if unit_y.any():
+                    i = random.randint(0, len(unit_y) - 1)
+                    target = [unit_x[i], unit_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if barracks_y.any():
+                    i = random.randint(0, len(barracks_y) - 1)
+                    target = [barracks_x[i], barracks_y[i]]
+
+                    return actions.FunctionCall(_SELECT_POINT, [_SELECT_ALL, target])
+
+            elif smart_action == ACTION_ATTACK:
+                if _SELECT_ARMY in obs.observation['available_actions']:
+                    return actions.FunctionCall(_SELECT_ARMY, [_NOT_QUEUED])
+
+        elif self.move_number == 1:
+            self.move_number += 1
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if supply_depot_count < 2 and _BUILD_SUPPLY_DEPOT in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if supply_depot_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -35, round(self.cc_y.mean()), 0)
+                        elif supply_depot_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), -25, round(self.cc_y.mean()), -25)
+
+                        return actions.FunctionCall(_BUILD_SUPPLY_DEPOT, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_BARRACKS:
+                if barracks_count < 2 and _BUILD_BARRACKS in obs.observation['available_actions']:
+                    if self.cc_y.any():
+                        if barracks_count == 0:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), -9)
+                        elif barracks_count == 1:
+                            target = self.transformDistance(
+                                round(self.cc_x.mean()), 15, round(self.cc_y.mean()), 12)
+
+                        return actions.FunctionCall(_BUILD_BARRACKS, [_NOT_QUEUED, target])
+
+            elif smart_action == ACTION_BUILD_MARINE:
+                if _TRAIN_MARINE in obs.observation['available_actions']:
+                    return actions.FunctionCall(_TRAIN_MARINE, [_QUEUED])
+
+            elif smart_action == ACTION_ATTACK:
+                do_it = True
+
+                if len(obs.observation['single_select']) > 0 and obs.observation['single_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if len(obs.observation['multi_select']) > 0 and obs.observation['multi_select'][0][0] == _TERRAN_SCV:
+                    do_it = False
+
+                if do_it and _ATTACK_MINIMAP in obs.observation["available_actions"]:
+                    x_offset = random.randint(-1, 1)
+                    y_offset = random.randint(-1, 1)
+
+                    return actions.FunctionCall(_ATTACK_MINIMAP, [_NOT_QUEUED, self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8))])
+
+        elif self.move_number == 2:
+            self.move_number = 0
+
+            smart_action, x, y = self.splitAction(self.previous_action)
+
+            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:
+                if _HARVEST_GATHER in obs.observation['available_actions']:
+                    unit_y, unit_x = (
+                        unit_type == _NEUTRAL_MINERAL_FIELD).nonzero()
+
+                    if unit_y.any():
+                        i = random.randint(0, len(unit_y) - 1)
+
+                        m_x = unit_x[i]
+                        m_y = unit_y[i]
+
+                        target = [int(m_x), int(m_y)]
+
+                        return actions.FunctionCall(_HARVEST_GATHER, [_QUEUED, target])
+
+        return actions.FunctionCall(_NO_OP, [])

commit efd6c9ff14d490e337904e8f20646762266e75d4
Author: Craig Dods <email.redacted@redacted_domain.com>
Date:   Mon Jul 17 10:30:41 2023 -0400

    we now have replay files for 4.10.0!!!!

diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-01.SC2Replay
new file mode 100644
index 00000000..2068777f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-17.SC2Replay
new file mode 100644
index 00000000..bb009052
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-32.SC2Replay
new file mode 100644
index 00000000..a8c91d94
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-51.SC2Replay
new file mode 100644
index 00000000..3347854b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-23-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-07.SC2Replay
new file mode 100644
index 00000000..a2bac3e8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-35.SC2Replay
new file mode 100644
index 00000000..caa66c6d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-24-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-04.SC2Replay
new file mode 100644
index 00000000..71783bea
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-23.SC2Replay
new file mode 100644
index 00000000..9c5f0e7f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-38.SC2Replay
new file mode 100644
index 00000000..31082f15
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-53.SC2Replay
new file mode 100644
index 00000000..7da7ca22
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-25-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-16.SC2Replay
new file mode 100644
index 00000000..f79205a5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-39.SC2Replay
new file mode 100644
index 00000000..6bd46bcf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-26-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-03.SC2Replay
new file mode 100644
index 00000000..85aa9af6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-15.SC2Replay
new file mode 100644
index 00000000..d6dc8ede
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-42.SC2Replay
new file mode 100644
index 00000000..ec7b011d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-56.SC2Replay
new file mode 100644
index 00000000..44856823
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-27-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-21.SC2Replay
new file mode 100644
index 00000000..7244bcb0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-33.SC2Replay
new file mode 100644
index 00000000..e3ba3379
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-53.SC2Replay
new file mode 100644
index 00000000..e0e81af2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-28-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-04.SC2Replay
new file mode 100644
index 00000000..ee09c0d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-23.SC2Replay
new file mode 100644
index 00000000..a8281ec7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-35.SC2Replay
new file mode 100644
index 00000000..a27ca969
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-29-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-03.SC2Replay
new file mode 100644
index 00000000..5e887e51
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-29.SC2Replay
new file mode 100644
index 00000000..08f605d0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-48.SC2Replay
new file mode 100644
index 00000000..01336b0d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-30-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-04.SC2Replay
new file mode 100644
index 00000000..6b826569
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-32.SC2Replay
new file mode 100644
index 00000000..00254d4c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-47.SC2Replay
new file mode 100644
index 00000000..4390fda2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-59.SC2Replay
new file mode 100644
index 00000000..78e6c7f0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-31-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-17.SC2Replay
new file mode 100644
index 00000000..21e752ac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-33.SC2Replay
new file mode 100644
index 00000000..24080f71
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-32-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-00.SC2Replay
new file mode 100644
index 00000000..a0bed1d4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-14.SC2Replay
new file mode 100644
index 00000000..db0acd78
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-28.SC2Replay
new file mode 100644
index 00000000..63518a06
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-40.SC2Replay
new file mode 100644
index 00000000..5e32cb62
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-33-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-00.SC2Replay
new file mode 100644
index 00000000..01035015
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-26.SC2Replay
new file mode 100644
index 00000000..13249a8f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-43.SC2Replay
new file mode 100644
index 00000000..5a08f7c4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-55.SC2Replay
new file mode 100644
index 00000000..7d5da922
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-34-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-09.SC2Replay
new file mode 100644
index 00000000..5f6b07f5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-27.SC2Replay
new file mode 100644
index 00000000..bd36e452
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-43.SC2Replay
new file mode 100644
index 00000000..3cfb2fa9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-58.SC2Replay
new file mode 100644
index 00000000..341b73bb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-35-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-14.SC2Replay
new file mode 100644
index 00000000..00cc8fa4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-41.SC2Replay
new file mode 100644
index 00000000..2576cbba
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-36-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-08.SC2Replay
new file mode 100644
index 00000000..9d30c6c7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-36.SC2Replay
new file mode 100644
index 00000000..cabe01f8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-56.SC2Replay
new file mode 100644
index 00000000..873fac07
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-37-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-13.SC2Replay
new file mode 100644
index 00000000..17384c24
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-25.SC2Replay
new file mode 100644
index 00000000..c8a30eb0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-44.SC2Replay
new file mode 100644
index 00000000..9fcb6d54
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-56.SC2Replay
new file mode 100644
index 00000000..af0a0a57
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-38-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-12.SC2Replay
new file mode 100644
index 00000000..fe56191f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-26.SC2Replay
new file mode 100644
index 00000000..cf44a325
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-40.SC2Replay
new file mode 100644
index 00000000..ca7819a6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-39-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-07.SC2Replay
new file mode 100644
index 00000000..ab6a8c6c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-33.SC2Replay
new file mode 100644
index 00000000..dca5f3ae
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-49.SC2Replay
new file mode 100644
index 00000000..9dcdeb66
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-40-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-08.SC2Replay
new file mode 100644
index 00000000..463d3677
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-40.SC2Replay
new file mode 100644
index 00000000..34acab2c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-56.SC2Replay
new file mode 100644
index 00000000..d3ea63f4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-41-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-07.SC2Replay
new file mode 100644
index 00000000..5d68007e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-34.SC2Replay
new file mode 100644
index 00000000..644a6f71
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-55.SC2Replay
new file mode 100644
index 00000000..d0329f67
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-42-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-06.SC2Replay
new file mode 100644
index 00000000..df3520f5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-26.SC2Replay
new file mode 100644
index 00000000..5a5ff9ad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-38.SC2Replay
new file mode 100644
index 00000000..435cc666
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-43-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-00.SC2Replay
new file mode 100644
index 00000000..2ea12281
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-21.SC2Replay
new file mode 100644
index 00000000..683ca686
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-33.SC2Replay
new file mode 100644
index 00000000..b2e6f7d0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-47.SC2Replay
new file mode 100644
index 00000000..c52fdc4a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-44-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-00.SC2Replay
new file mode 100644
index 00000000..97249cad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-23.SC2Replay
new file mode 100644
index 00000000..c4a702f4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-53.SC2Replay
new file mode 100644
index 00000000..2090f81e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-45-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-04.SC2Replay
new file mode 100644
index 00000000..64dfd142
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-23.SC2Replay
new file mode 100644
index 00000000..9996a932
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-36.SC2Replay
new file mode 100644
index 00000000..bbb3dd22
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-48.SC2Replay
new file mode 100644
index 00000000..d3abb4f7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-59.SC2Replay
new file mode 100644
index 00000000..858477b9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-46-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-13.SC2Replay
new file mode 100644
index 00000000..cf48c226
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-33.SC2Replay
new file mode 100644
index 00000000..9f2f43fe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-53.SC2Replay
new file mode 100644
index 00000000..a0e8b2e6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-47-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-15.SC2Replay
new file mode 100644
index 00000000..9ddc4928
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-37.SC2Replay
new file mode 100644
index 00000000..34a59e7f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-48-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-06.SC2Replay
new file mode 100644
index 00000000..d1c61a1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-43.SC2Replay
new file mode 100644
index 00000000..a7f6e50e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-49-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-09.SC2Replay
new file mode 100644
index 00000000..2b110c16
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-21.SC2Replay
new file mode 100644
index 00000000..136a1c20
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-47.SC2Replay
new file mode 100644
index 00000000..4d7e2560
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-50-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-10.SC2Replay
new file mode 100644
index 00000000..b452ed95
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-37.SC2Replay
new file mode 100644
index 00000000..030a285c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-50.SC2Replay
new file mode 100644
index 00000000..e3c09ee8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-51-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-01.SC2Replay
new file mode 100644
index 00000000..fbba5397
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-30.SC2Replay
new file mode 100644
index 00000000..c5337624
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-43.SC2Replay
new file mode 100644
index 00000000..63723ed2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-52-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-04.SC2Replay
new file mode 100644
index 00000000..b0f9fe06
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-14.SC2Replay
new file mode 100644
index 00000000..35144adf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-32.SC2Replay
new file mode 100644
index 00000000..ab5ce72a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-48.SC2Replay
new file mode 100644
index 00000000..f5ee02cd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-53-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-14.SC2Replay
new file mode 100644
index 00000000..de078bbf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-36.SC2Replay
new file mode 100644
index 00000000..26d330ee
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-59.SC2Replay
new file mode 100644
index 00000000..901db0e0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-54-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-55-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-55-25.SC2Replay
new file mode 100644
index 00000000..63a4740f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-13-55-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-03.SC2Replay
new file mode 100644
index 00000000..fafa0825
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-23.SC2Replay
new file mode 100644
index 00000000..debed977
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-50.SC2Replay
new file mode 100644
index 00000000..9213d8a6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-01-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-01.SC2Replay
new file mode 100644
index 00000000..bb683a5e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-28.SC2Replay
new file mode 100644
index 00000000..581c83a9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-40.SC2Replay
new file mode 100644
index 00000000..c645d3b4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-59.SC2Replay
new file mode 100644
index 00000000..aaae3a39
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-02-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-23.SC2Replay
new file mode 100644
index 00000000..2b0f0991
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-51.SC2Replay
new file mode 100644
index 00000000..ec8be64b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-03-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-19.SC2Replay
new file mode 100644
index 00000000..a2d6005c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-30.SC2Replay
new file mode 100644
index 00000000..18204620
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-04-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-00.SC2Replay
new file mode 100644
index 00000000..3e3ce3cd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-23.SC2Replay
new file mode 100644
index 00000000..b8ec1c37
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-47.SC2Replay
new file mode 100644
index 00000000..56d445e7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-05-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-13.SC2Replay
new file mode 100644
index 00000000..8263ee80
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-32.SC2Replay
new file mode 100644
index 00000000..05d79a10
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-42.SC2Replay
new file mode 100644
index 00000000..a8a21d8f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-56.SC2Replay
new file mode 100644
index 00000000..bcb88d13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-06-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-06.SC2Replay
new file mode 100644
index 00000000..4ca4f13d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-18.SC2Replay
new file mode 100644
index 00000000..a93715e1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-32.SC2Replay
new file mode 100644
index 00000000..6c839c04
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-07-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-02.SC2Replay
new file mode 100644
index 00000000..ef842e58
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-15.SC2Replay
new file mode 100644
index 00000000..a5054e49
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-27.SC2Replay
new file mode 100644
index 00000000..680d5b2f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-50.SC2Replay
new file mode 100644
index 00000000..ad776301
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-08-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-02.SC2Replay
new file mode 100644
index 00000000..da85a041
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-13.SC2Replay
new file mode 100644
index 00000000..d3b08f4f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-40.SC2Replay
new file mode 100644
index 00000000..499328ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-09-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-00.SC2Replay
new file mode 100644
index 00000000..7bda372c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-17.SC2Replay
new file mode 100644
index 00000000..d1e116bb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-40.SC2Replay
new file mode 100644
index 00000000..c0608004
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-52.SC2Replay
new file mode 100644
index 00000000..73db5e0c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-10-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-04.SC2Replay
new file mode 100644
index 00000000..6efa76a2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-15.SC2Replay
new file mode 100644
index 00000000..1f8088a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-41.SC2Replay
new file mode 100644
index 00000000..4f266091
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-53.SC2Replay
new file mode 100644
index 00000000..13574de3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-11-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-14.SC2Replay
new file mode 100644
index 00000000..7ed512b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-38.SC2Replay
new file mode 100644
index 00000000..70024619
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-56.SC2Replay
new file mode 100644
index 00000000..2e01225a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-12-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-26.SC2Replay
new file mode 100644
index 00000000..c49d0943
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-41.SC2Replay
new file mode 100644
index 00000000..15e18a26
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-13-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-03.SC2Replay
new file mode 100644
index 00000000..e37a3b5c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-28.SC2Replay
new file mode 100644
index 00000000..144edff6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-54.SC2Replay
new file mode 100644
index 00000000..46382856
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-14-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-07.SC2Replay
new file mode 100644
index 00000000..cfc7e9b8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-28.SC2Replay
new file mode 100644
index 00000000..8c46db9e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-43.SC2Replay
new file mode 100644
index 00000000..9231ad23
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-15-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-12.SC2Replay
new file mode 100644
index 00000000..0612033e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-27.SC2Replay
new file mode 100644
index 00000000..98e862f1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-39.SC2Replay
new file mode 100644
index 00000000..60e3dd36
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-16-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-06.SC2Replay
new file mode 100644
index 00000000..db126179
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-32.SC2Replay
new file mode 100644
index 00000000..88b06784
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-58.SC2Replay
new file mode 100644
index 00000000..d52c723d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-17-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-11.SC2Replay
new file mode 100644
index 00000000..71bd1a6e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-32.SC2Replay
new file mode 100644
index 00000000..5b64099c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-51.SC2Replay
new file mode 100644
index 00000000..3c5974a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-18-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-10.SC2Replay
new file mode 100644
index 00000000..459281e9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-31.SC2Replay
new file mode 100644
index 00000000..97e3311d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-19-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-00.SC2Replay
new file mode 100644
index 00000000..9f42a508
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-21.SC2Replay
new file mode 100644
index 00000000..6f97e6c8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-32.SC2Replay
new file mode 100644
index 00000000..70a9983c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-43.SC2Replay
new file mode 100644
index 00000000..be2dd596
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-20-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-12.SC2Replay
new file mode 100644
index 00000000..980c75cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-22.SC2Replay
new file mode 100644
index 00000000..00e583a9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-37.SC2Replay
new file mode 100644
index 00000000..89b68727
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-56.SC2Replay
new file mode 100644
index 00000000..966102f1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-21-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-07.SC2Replay
new file mode 100644
index 00000000..565c48f4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-31.SC2Replay
new file mode 100644
index 00000000..e343311a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-42.SC2Replay
new file mode 100644
index 00000000..fbf50496
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-57.SC2Replay
new file mode 100644
index 00000000..af6f0e13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-22-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-19.SC2Replay
new file mode 100644
index 00000000..3f2c9a6d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-39.SC2Replay
new file mode 100644
index 00000000..acfcf898
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-23-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-04.SC2Replay
new file mode 100644
index 00000000..576841ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-24.SC2Replay
new file mode 100644
index 00000000..f8df5ec7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-50.SC2Replay
new file mode 100644
index 00000000..9efab17d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-24-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-16.SC2Replay
new file mode 100644
index 00000000..dc0f6813
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-42.SC2Replay
new file mode 100644
index 00000000..d89ce3e4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-53.SC2Replay
new file mode 100644
index 00000000..041b0442
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-25-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-19.SC2Replay
new file mode 100644
index 00000000..badcd1fd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-45.SC2Replay
new file mode 100644
index 00000000..87a399e4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-26-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-16.SC2Replay
new file mode 100644
index 00000000..18fbaeea
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-26.SC2Replay
new file mode 100644
index 00000000..15f90e36
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-45.SC2Replay
new file mode 100644
index 00000000..3ff08ee7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-27-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-11.SC2Replay
new file mode 100644
index 00000000..dc8bfda0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-32.SC2Replay
new file mode 100644
index 00000000..a03d5fa2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-43.SC2Replay
new file mode 100644
index 00000000..2bbd9489
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-28-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-09.SC2Replay
new file mode 100644
index 00000000..05071a54
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-29.SC2Replay
new file mode 100644
index 00000000..b535e246
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-54.SC2Replay
new file mode 100644
index 00000000..3cf3efb3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-29-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-28.SC2Replay
new file mode 100644
index 00000000..d165a663
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-40.SC2Replay
new file mode 100644
index 00000000..7e57c8fe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-30-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-04.SC2Replay
new file mode 100644
index 00000000..0ef9856c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-20.SC2Replay
new file mode 100644
index 00000000..85adc3f5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-32.SC2Replay
new file mode 100644
index 00000000..b4663dd0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-47.SC2Replay
new file mode 100644
index 00000000..2852b82d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-58.SC2Replay
new file mode 100644
index 00000000..d285cf0b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-31-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-11.SC2Replay
new file mode 100644
index 00000000..612fe2cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-22.SC2Replay
new file mode 100644
index 00000000..53f4c802
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-46.SC2Replay
new file mode 100644
index 00000000..65a2c63f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-58.SC2Replay
new file mode 100644
index 00000000..628e9a96
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-32-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-20.SC2Replay
new file mode 100644
index 00000000..34dcb419
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-35.SC2Replay
new file mode 100644
index 00000000..a4ca5d53
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-50.SC2Replay
new file mode 100644
index 00000000..1ee0d909
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-33-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-02.SC2Replay
new file mode 100644
index 00000000..14f7b61d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-15.SC2Replay
new file mode 100644
index 00000000..caed21fb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-33.SC2Replay
new file mode 100644
index 00000000..65a5bb90
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-54.SC2Replay
new file mode 100644
index 00000000..48c9222a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-34-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-06.SC2Replay
new file mode 100644
index 00000000..39e26d32
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-28.SC2Replay
new file mode 100644
index 00000000..e14d0da2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-42.SC2Replay
new file mode 100644
index 00000000..3ca5ff3b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-55.SC2Replay
new file mode 100644
index 00000000..c854d538
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-35-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-09.SC2Replay
new file mode 100644
index 00000000..e05175db
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-25.SC2Replay
new file mode 100644
index 00000000..d7ac70c5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-38.SC2Replay
new file mode 100644
index 00000000..5b74f935
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-50.SC2Replay
new file mode 100644
index 00000000..b211620d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-36-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-05.SC2Replay
new file mode 100644
index 00000000..cd85cc04
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-18.SC2Replay
new file mode 100644
index 00000000..4c4ef94e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-31.SC2Replay
new file mode 100644
index 00000000..fc97617a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-52.SC2Replay
new file mode 100644
index 00000000..afc40594
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-37-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-06.SC2Replay
new file mode 100644
index 00000000..36dbbe4c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-27.SC2Replay
new file mode 100644
index 00000000..c777f3ca
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-41.SC2Replay
new file mode 100644
index 00000000..05139a48
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-38-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-09.SC2Replay
new file mode 100644
index 00000000..59140069
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-21.SC2Replay
new file mode 100644
index 00000000..e78ba83e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-34.SC2Replay
new file mode 100644
index 00000000..6ff53d36
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-48.SC2Replay
new file mode 100644
index 00000000..69a63520
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-59.SC2Replay
new file mode 100644
index 00000000..3c6fe05b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-39-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-19.SC2Replay
new file mode 100644
index 00000000..cdff3bd2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-33.SC2Replay
new file mode 100644
index 00000000..de24aea7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-44.SC2Replay
new file mode 100644
index 00000000..50999bf9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-57.SC2Replay
new file mode 100644
index 00000000..eafb1226
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-40-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-17.SC2Replay
new file mode 100644
index 00000000..6f121012
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-32.SC2Replay
new file mode 100644
index 00000000..ed0f86ea
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-56.SC2Replay
new file mode 100644
index 00000000..e84eeab7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-41-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-10.SC2Replay
new file mode 100644
index 00000000..193a6f35
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-22.SC2Replay
new file mode 100644
index 00000000..789b6284
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-44.SC2Replay
new file mode 100644
index 00000000..b57b92a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-57.SC2Replay
new file mode 100644
index 00000000..00b869bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-42-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-10.SC2Replay
new file mode 100644
index 00000000..259a711b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-31.SC2Replay
new file mode 100644
index 00000000..e533a190
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-38.SC2Replay
new file mode 100644
index 00000000..5325c9fb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-50.SC2Replay
new file mode 100644
index 00000000..0e62b6a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-43-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-03.SC2Replay
new file mode 100644
index 00000000..7974869c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-22.SC2Replay
new file mode 100644
index 00000000..88efff85
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-34.SC2Replay
new file mode 100644
index 00000000..7d4dc4aa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-37.SC2Replay
new file mode 100644
index 00000000..9e77a57d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-58.SC2Replay
new file mode 100644
index 00000000..98fda4d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-44-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-10.SC2Replay
new file mode 100644
index 00000000..abb9670d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-22.SC2Replay
new file mode 100644
index 00000000..48c776be
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-45-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-03.SC2Replay
new file mode 100644
index 00000000..a6236efd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-16.SC2Replay
new file mode 100644
index 00000000..a828bd03
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-28.SC2Replay
new file mode 100644
index 00000000..5cd93bdd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-42.SC2Replay
new file mode 100644
index 00000000..dc916c45
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-46-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-15.SC2Replay
new file mode 100644
index 00000000..b948b8e4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-18.SC2Replay
new file mode 100644
index 00000000..c70b5cde
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-19.SC2Replay
new file mode 100644
index 00000000..d4140364
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-21.SC2Replay
new file mode 100644
index 00000000..9c1a9a77
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-24.SC2Replay
new file mode 100644
index 00000000..eefca13b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-28.SC2Replay
new file mode 100644
index 00000000..f01af057
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-30.SC2Replay
new file mode 100644
index 00000000..a79d021e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-31.SC2Replay
new file mode 100644
index 00000000..e94fecf2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-42.SC2Replay
new file mode 100644
index 00000000..426da48f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-44.SC2Replay
new file mode 100644
index 00000000..dc02518c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-46.SC2Replay
new file mode 100644
index 00000000..34e6989d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-47.SC2Replay
new file mode 100644
index 00000000..cb1aa754
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-49.SC2Replay
new file mode 100644
index 00000000..00db0ccf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-50.SC2Replay
new file mode 100644
index 00000000..fb5562ff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-58.SC2Replay
new file mode 100644
index 00000000..f2f1aa22
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-47-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-48-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-48-19.SC2Replay
new file mode 100644
index 00000000..17c434ab
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-48-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-02.SC2Replay
new file mode 100644
index 00000000..39816958
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-11.SC2Replay
new file mode 100644
index 00000000..165a21fe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-13.SC2Replay
new file mode 100644
index 00000000..4c47a199
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-14.SC2Replay
new file mode 100644
index 00000000..bb3d7112
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-16.SC2Replay
new file mode 100644
index 00000000..d4f5983c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-19.SC2Replay
new file mode 100644
index 00000000..89f8a2ca
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-21.SC2Replay
new file mode 100644
index 00000000..0b07ef16
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-23.SC2Replay
new file mode 100644
index 00000000..a3c30c5f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-34.SC2Replay
new file mode 100644
index 00000000..07418f78
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-39.SC2Replay
new file mode 100644
index 00000000..9eab6df1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-56.SC2Replay
new file mode 100644
index 00000000..f298d2dc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-59.SC2Replay
new file mode 100644
index 00000000..0d6b6317
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-49-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-01.SC2Replay
new file mode 100644
index 00000000..85f1789e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-03.SC2Replay
new file mode 100644
index 00000000..543ade3b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-50-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-52.SC2Replay
new file mode 100644
index 00000000..f19c8a12
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-54.SC2Replay
new file mode 100644
index 00000000..c68aff1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-55.SC2Replay
new file mode 100644
index 00000000..c0011f20
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-57.SC2Replay
new file mode 100644
index 00000000..b911ce74
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-51-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-00.SC2Replay
new file mode 100644
index 00000000..55a9310c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-04.SC2Replay
new file mode 100644
index 00000000..6c167960
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-05.SC2Replay
new file mode 100644
index 00000000..b651da9c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-07.SC2Replay
new file mode 100644
index 00000000..c167699d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-09.SC2Replay
new file mode 100644
index 00000000..a2026bce
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-13.SC2Replay
new file mode 100644
index 00000000..891eb3fb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-20.SC2Replay
new file mode 100644
index 00000000..7ea3c271
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-22.SC2Replay
new file mode 100644
index 00000000..a4e191c4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-29.SC2Replay
new file mode 100644
index 00000000..b9380e04
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-30.SC2Replay
new file mode 100644
index 00000000..67c237cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-31.SC2Replay
new file mode 100644
index 00000000..4ed28d29
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-39.SC2Replay
new file mode 100644
index 00000000..ef76d16a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-40.SC2Replay
new file mode 100644
index 00000000..50bdecc7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-43.SC2Replay
new file mode 100644
index 00000000..f30443e4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-52-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-10.SC2Replay
new file mode 100644
index 00000000..c647ce0b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-21.SC2Replay
new file mode 100644
index 00000000..1c0e4503
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-24.SC2Replay
new file mode 100644
index 00000000..40c6d498
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-26.SC2Replay
new file mode 100644
index 00000000..d4a172ba
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-32.SC2Replay
new file mode 100644
index 00000000..9985f72b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-34.SC2Replay
new file mode 100644
index 00000000..e4f0c30f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-35.SC2Replay
new file mode 100644
index 00000000..40adc65a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-42.SC2Replay
new file mode 100644
index 00000000..2c92c9c0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-44.SC2Replay
new file mode 100644
index 00000000..82901861
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-45.SC2Replay
new file mode 100644
index 00000000..1a008aff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-46.SC2Replay
new file mode 100644
index 00000000..aed11a6a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-57.SC2Replay
new file mode 100644
index 00000000..0b852f13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-59.SC2Replay
new file mode 100644
index 00000000..5aa8ec00
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-53-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-00.SC2Replay
new file mode 100644
index 00000000..a3cf9a4c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-02.SC2Replay
new file mode 100644
index 00000000..1c0a4361
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-04.SC2Replay
new file mode 100644
index 00000000..f7e1cc2c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-14.SC2Replay
new file mode 100644
index 00000000..716156ca
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-16.SC2Replay
new file mode 100644
index 00000000..fb3a31ea
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-18.SC2Replay
new file mode 100644
index 00000000..26c09898
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-19.SC2Replay
new file mode 100644
index 00000000..5ef6ac2f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-22.SC2Replay
new file mode 100644
index 00000000..4939f6c1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-23.SC2Replay
new file mode 100644
index 00000000..2515583d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-26.SC2Replay
new file mode 100644
index 00000000..c13686aa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-28.SC2Replay
new file mode 100644
index 00000000..e556c629
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-33.SC2Replay
new file mode 100644
index 00000000..66e28d78
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-41.SC2Replay
new file mode 100644
index 00000000..e05d00bb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-53.SC2Replay
new file mode 100644
index 00000000..94a1db7b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-54-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-04.SC2Replay
new file mode 100644
index 00000000..14c795ec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-07.SC2Replay
new file mode 100644
index 00000000..0d48d437
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-10.SC2Replay
new file mode 100644
index 00000000..c9499a9d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-12.SC2Replay
new file mode 100644
index 00000000..4707cedc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-17.SC2Replay
new file mode 100644
index 00000000..2dbd4589
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-19.SC2Replay
new file mode 100644
index 00000000..9f92b79d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-21.SC2Replay
new file mode 100644
index 00000000..7f54f65d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-37.SC2Replay
new file mode 100644
index 00000000..8833d3a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-39.SC2Replay
new file mode 100644
index 00000000..695b5dd5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-43.SC2Replay
new file mode 100644
index 00000000..40b98f4b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-44.SC2Replay
new file mode 100644
index 00000000..0d7e34b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-52.SC2Replay
new file mode 100644
index 00000000..07abecb3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-55-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-09.SC2Replay
new file mode 100644
index 00000000..7d93db49
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-10.SC2Replay
new file mode 100644
index 00000000..198b5e6e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-13.SC2Replay
new file mode 100644
index 00000000..42a58780
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-15.SC2Replay
new file mode 100644
index 00000000..71807f49
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-32.SC2Replay
new file mode 100644
index 00000000..880c0c55
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-52.SC2Replay
new file mode 100644
index 00000000..5870cccd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-53.SC2Replay
new file mode 100644
index 00000000..415ba8d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-54.SC2Replay
new file mode 100644
index 00000000..25d0d4fa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-55.SC2Replay
new file mode 100644
index 00000000..17e0b33e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-56.SC2Replay
new file mode 100644
index 00000000..176c4a77
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-56-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-13.SC2Replay
new file mode 100644
index 00000000..0e6c1abc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-16.SC2Replay
new file mode 100644
index 00000000..17fe2934
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-17.SC2Replay
new file mode 100644
index 00000000..49c95df6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-19.SC2Replay
new file mode 100644
index 00000000..78e55b85
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-25.SC2Replay
new file mode 100644
index 00000000..c5a6e7fb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-27.SC2Replay
new file mode 100644
index 00000000..bb6dcdec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-34.SC2Replay
new file mode 100644
index 00000000..e7983bf2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-39.SC2Replay
new file mode 100644
index 00000000..7c9e030d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-41.SC2Replay
new file mode 100644
index 00000000..f113a624
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-48.SC2Replay
new file mode 100644
index 00000000..6ae82269
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-53.SC2Replay
new file mode 100644
index 00000000..99d368b0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-54.SC2Replay
new file mode 100644
index 00000000..7dc200e5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-59.SC2Replay
new file mode 100644
index 00000000..1345a8e8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-57-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-01.SC2Replay
new file mode 100644
index 00000000..91f1528c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-03.SC2Replay
new file mode 100644
index 00000000..fcacd190
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-05.SC2Replay
new file mode 100644
index 00000000..22160251
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-08.SC2Replay
new file mode 100644
index 00000000..62028273
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-09.SC2Replay
new file mode 100644
index 00000000..1e2b6c4f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-10.SC2Replay
new file mode 100644
index 00000000..f1fd9498
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-13.SC2Replay
new file mode 100644
index 00000000..c0ce0a07
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-16.SC2Replay
new file mode 100644
index 00000000..05c4ad4e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-24.SC2Replay
new file mode 100644
index 00000000..1aa8ad34
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-33.SC2Replay
new file mode 100644
index 00000000..8efef1df
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-42.SC2Replay
new file mode 100644
index 00000000..5540d3ec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-44.SC2Replay
new file mode 100644
index 00000000..8079548d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-50.SC2Replay
new file mode 100644
index 00000000..b19119fa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-52.SC2Replay
new file mode 100644
index 00000000..fa20a999
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-58-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-01.SC2Replay
new file mode 100644
index 00000000..de777f3f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-02.SC2Replay
new file mode 100644
index 00000000..c1a32823
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-13.SC2Replay
new file mode 100644
index 00000000..71e19c4a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-20.SC2Replay
new file mode 100644
index 00000000..1749243b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-26.SC2Replay
new file mode 100644
index 00000000..9fddd3ff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-30.SC2Replay
new file mode 100644
index 00000000..50ba8ec7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-36.SC2Replay
new file mode 100644
index 00000000..338e9e13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-39.SC2Replay
new file mode 100644
index 00000000..fdf163b3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-44.SC2Replay
new file mode 100644
index 00000000..acfc23c1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-45.SC2Replay
new file mode 100644
index 00000000..5cdf6af4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-46.SC2Replay
new file mode 100644
index 00000000..9d5ff7d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-48.SC2Replay
new file mode 100644
index 00000000..de75cd3b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-55.SC2Replay
new file mode 100644
index 00000000..7aef9010
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-57.SC2Replay
new file mode 100644
index 00000000..5d3dd7c7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-14-59-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-03.SC2Replay
new file mode 100644
index 00000000..26f1107f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-10.SC2Replay
new file mode 100644
index 00000000..07322c25
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-13.SC2Replay
new file mode 100644
index 00000000..110983af
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-16.SC2Replay
new file mode 100644
index 00000000..9ff5e61d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-33.SC2Replay
new file mode 100644
index 00000000..ea1edbd8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-36.SC2Replay
new file mode 100644
index 00000000..d526564c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-39.SC2Replay
new file mode 100644
index 00000000..199b7117
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-44.SC2Replay
new file mode 100644
index 00000000..c7ebd2bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-46.SC2Replay
new file mode 100644
index 00000000..2e0de2b4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-48.SC2Replay
new file mode 100644
index 00000000..f5cb56e1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-00-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-45.SC2Replay
new file mode 100644
index 00000000..c7e0fdd8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-46.SC2Replay
new file mode 100644
index 00000000..45d0bfaa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-48.SC2Replay
new file mode 100644
index 00000000..77c6c4dc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-49.SC2Replay
new file mode 100644
index 00000000..2a4c7268
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-50.SC2Replay
new file mode 100644
index 00000000..8071cab7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-51.SC2Replay
new file mode 100644
index 00000000..39844cc8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-52.SC2Replay
new file mode 100644
index 00000000..10650c44
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-53.SC2Replay
new file mode 100644
index 00000000..2ea31aa3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-54.SC2Replay
new file mode 100644
index 00000000..580a826d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-04-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-05-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-05-04.SC2Replay
new file mode 100644
index 00000000..62c260f7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-05-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-32.SC2Replay
new file mode 100644
index 00000000..2d436218
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-37.SC2Replay
new file mode 100644
index 00000000..c61e5be6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-40.SC2Replay
new file mode 100644
index 00000000..398a21bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-07-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-11-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-11-50.SC2Replay
new file mode 100644
index 00000000..adc5f8b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-11-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-02.SC2Replay
new file mode 100644
index 00000000..4fc7e88b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-08.SC2Replay
new file mode 100644
index 00000000..66b32b1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-13.SC2Replay
new file mode 100644
index 00000000..76a9ba3d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-22.SC2Replay
new file mode 100644
index 00000000..61f52644
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-26.SC2Replay
new file mode 100644
index 00000000..20022d59
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-27.SC2Replay
new file mode 100644
index 00000000..c39f78d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-28.SC2Replay
new file mode 100644
index 00000000..2486847f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-30.SC2Replay
new file mode 100644
index 00000000..34853710
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-37.SC2Replay
new file mode 100644
index 00000000..5cf59032
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-38.SC2Replay
new file mode 100644
index 00000000..b5278595
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-44.SC2Replay
new file mode 100644
index 00000000..2e178218
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-46.SC2Replay
new file mode 100644
index 00000000..b2b11642
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-50.SC2Replay
new file mode 100644
index 00000000..1b9a5d60
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-52.SC2Replay
new file mode 100644
index 00000000..eff1e6e2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-54.SC2Replay
new file mode 100644
index 00000000..7e6371aa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-55.SC2Replay
new file mode 100644
index 00000000..8b8d99f3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-58.SC2Replay
new file mode 100644
index 00000000..58478862
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-12-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-05.SC2Replay
new file mode 100644
index 00000000..93003791
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-06.SC2Replay
new file mode 100644
index 00000000..af6b5fbf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-07.SC2Replay
new file mode 100644
index 00000000..1e79444a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-08.SC2Replay
new file mode 100644
index 00000000..c8c84c4e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-19.SC2Replay
new file mode 100644
index 00000000..e07b0d36
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-34.SC2Replay
new file mode 100644
index 00000000..7a6b4c0e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-42.SC2Replay
new file mode 100644
index 00000000..c6423cee
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-13-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-05.SC2Replay
new file mode 100644
index 00000000..73410ead
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-09.SC2Replay
new file mode 100644
index 00000000..b0c6e46d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-10.SC2Replay
new file mode 100644
index 00000000..bd164a19
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-12.SC2Replay
new file mode 100644
index 00000000..16233ab4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-13.SC2Replay
new file mode 100644
index 00000000..33eee69a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-14.SC2Replay
new file mode 100644
index 00000000..6a5215a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-15.SC2Replay
new file mode 100644
index 00000000..1190b5d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-21.SC2Replay
new file mode 100644
index 00000000..fd267b9e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-27.SC2Replay
new file mode 100644
index 00000000..6decf618
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-33.SC2Replay
new file mode 100644
index 00000000..6fc6b02f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-36.SC2Replay
new file mode 100644
index 00000000..10c78487
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-39.SC2Replay
new file mode 100644
index 00000000..6d3f9f67
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-48.SC2Replay
new file mode 100644
index 00000000..9842b89a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-49.SC2Replay
new file mode 100644
index 00000000..8a4c0d0d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-14-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-04.SC2Replay
new file mode 100644
index 00000000..26e991e6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-15.SC2Replay
new file mode 100644
index 00000000..45ed344a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-31.SC2Replay
new file mode 100644
index 00000000..2e13d2a3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-32.SC2Replay
new file mode 100644
index 00000000..dc55dfc6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-34.SC2Replay
new file mode 100644
index 00000000..41b56b96
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-37.SC2Replay
new file mode 100644
index 00000000..91e2f027
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-39.SC2Replay
new file mode 100644
index 00000000..5abc71fc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-40.SC2Replay
new file mode 100644
index 00000000..4ece2921
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-42.SC2Replay
new file mode 100644
index 00000000..cd74f728
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-43.SC2Replay
new file mode 100644
index 00000000..019f1fb0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-46.SC2Replay
new file mode 100644
index 00000000..891591fd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-49.SC2Replay
new file mode 100644
index 00000000..b0d5ced0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-52.SC2Replay
new file mode 100644
index 00000000..5be50b98
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-15-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-02.SC2Replay
new file mode 100644
index 00000000..3922239f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-12.SC2Replay
new file mode 100644
index 00000000..6ffc4f68
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-24.SC2Replay
new file mode 100644
index 00000000..db9d3536
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-25.SC2Replay
new file mode 100644
index 00000000..64ffd4b3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-27.SC2Replay
new file mode 100644
index 00000000..6432a18f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-30.SC2Replay
new file mode 100644
index 00000000..18c49f84
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-31.SC2Replay
new file mode 100644
index 00000000..8d503a1c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-39.SC2Replay
new file mode 100644
index 00000000..1edefc67
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-49.SC2Replay
new file mode 100644
index 00000000..7734e835
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-53.SC2Replay
new file mode 100644
index 00000000..fe0f110d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-55.SC2Replay
new file mode 100644
index 00000000..cd23ca31
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-16-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-07.SC2Replay
new file mode 100644
index 00000000..798c0a73
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-10.SC2Replay
new file mode 100644
index 00000000..78e8bc93
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-11.SC2Replay
new file mode 100644
index 00000000..52047352
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-12.SC2Replay
new file mode 100644
index 00000000..9d47c6a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-19.SC2Replay
new file mode 100644
index 00000000..d696ae90
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-20.SC2Replay
new file mode 100644
index 00000000..e391a35d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-35.SC2Replay
new file mode 100644
index 00000000..38ddff32
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-41.SC2Replay
new file mode 100644
index 00000000..8b594548
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-46.SC2Replay
new file mode 100644
index 00000000..6eeef8de
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-50.SC2Replay
new file mode 100644
index 00000000..2f2065e8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-52.SC2Replay
new file mode 100644
index 00000000..7ad354be
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-17-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-00.SC2Replay
new file mode 100644
index 00000000..7a2a2ad0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-06.SC2Replay
new file mode 100644
index 00000000..269293ae
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-07.SC2Replay
new file mode 100644
index 00000000..4076ab9d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-10.SC2Replay
new file mode 100644
index 00000000..e251935f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-20.SC2Replay
new file mode 100644
index 00000000..7433d202
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-21.SC2Replay
new file mode 100644
index 00000000..acf15cab
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-28.SC2Replay
new file mode 100644
index 00000000..9ec352a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-37.SC2Replay
new file mode 100644
index 00000000..dec4f23c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-55.SC2Replay
new file mode 100644
index 00000000..daf26357
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-57.SC2Replay
new file mode 100644
index 00000000..ff10f3e0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-18-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-01.SC2Replay
new file mode 100644
index 00000000..18d52f14
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-14.SC2Replay
new file mode 100644
index 00000000..9ea8c4a0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-22.SC2Replay
new file mode 100644
index 00000000..603561ac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-26.SC2Replay
new file mode 100644
index 00000000..ae701098
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-30.SC2Replay
new file mode 100644
index 00000000..b76da6f7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-34.SC2Replay
new file mode 100644
index 00000000..a1ed4ffb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-42.SC2Replay
new file mode 100644
index 00000000..92c16930
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-46.SC2Replay
new file mode 100644
index 00000000..1127cafe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-48.SC2Replay
new file mode 100644
index 00000000..4951b745
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-50.SC2Replay
new file mode 100644
index 00000000..7eb6c4b4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-57.SC2Replay
new file mode 100644
index 00000000..5eef5351
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-19-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-02.SC2Replay
new file mode 100644
index 00000000..a4481410
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-08.SC2Replay
new file mode 100644
index 00000000..a0f69943
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-09.SC2Replay
new file mode 100644
index 00000000..5ddb2fa1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-11.SC2Replay
new file mode 100644
index 00000000..215ef37a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-13.SC2Replay
new file mode 100644
index 00000000..7ac268e8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-19.SC2Replay
new file mode 100644
index 00000000..a07f7c93
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-29.SC2Replay
new file mode 100644
index 00000000..db461c80
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-35.SC2Replay
new file mode 100644
index 00000000..bf6440d2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-39.SC2Replay
new file mode 100644
index 00000000..95b1505a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-45.SC2Replay
new file mode 100644
index 00000000..105db10c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-46.SC2Replay
new file mode 100644
index 00000000..089114b0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-53.SC2Replay
new file mode 100644
index 00000000..29b43078
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-20-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-06.SC2Replay
new file mode 100644
index 00000000..242ae18d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-20.SC2Replay
new file mode 100644
index 00000000..c4e1ed69
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-21.SC2Replay
new file mode 100644
index 00000000..f7ca2d35
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-24.SC2Replay
new file mode 100644
index 00000000..032cd507
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-26.SC2Replay
new file mode 100644
index 00000000..0a16c1b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-37.SC2Replay
new file mode 100644
index 00000000..569ad536
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-43.SC2Replay
new file mode 100644
index 00000000..c48aebe3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-52.SC2Replay
new file mode 100644
index 00000000..96f888b2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-55.SC2Replay
new file mode 100644
index 00000000..7083720f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-58.SC2Replay
new file mode 100644
index 00000000..65836b82
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-59.SC2Replay
new file mode 100644
index 00000000..0cbb1d17
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-21-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-08.SC2Replay
new file mode 100644
index 00000000..d98fbc9f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-09.SC2Replay
new file mode 100644
index 00000000..d17cd46f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-10.SC2Replay
new file mode 100644
index 00000000..a342934e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-20.SC2Replay
new file mode 100644
index 00000000..fb678af3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-26.SC2Replay
new file mode 100644
index 00000000..7c015f53
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-33.SC2Replay
new file mode 100644
index 00000000..a2f5cad7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-38.SC2Replay
new file mode 100644
index 00000000..2ed0a12d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-42.SC2Replay
new file mode 100644
index 00000000..657b8ea6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-56.SC2Replay
new file mode 100644
index 00000000..549566e2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-22-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-07.SC2Replay
new file mode 100644
index 00000000..161e3bb6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-09.SC2Replay
new file mode 100644
index 00000000..bd21e49e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-15.SC2Replay
new file mode 100644
index 00000000..036690f6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-32.SC2Replay
new file mode 100644
index 00000000..2fc33145
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-37.SC2Replay
new file mode 100644
index 00000000..e01f6c1a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-42.SC2Replay
new file mode 100644
index 00000000..1070eb95
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-56.SC2Replay
new file mode 100644
index 00000000..c76c233b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-23-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-08.SC2Replay
new file mode 100644
index 00000000..de00dfc5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-10.SC2Replay
new file mode 100644
index 00000000..076f2157
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-16.SC2Replay
new file mode 100644
index 00000000..3acbf551
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-17.SC2Replay
new file mode 100644
index 00000000..9211d89d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-18.SC2Replay
new file mode 100644
index 00000000..b9c3a44c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-22.SC2Replay
new file mode 100644
index 00000000..9a645d5d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-24.SC2Replay
new file mode 100644
index 00000000..e8eb6405
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-28.SC2Replay
new file mode 100644
index 00000000..d965a318
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-33.SC2Replay
new file mode 100644
index 00000000..00f94b85
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-47.SC2Replay
new file mode 100644
index 00000000..332cf55e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-48.SC2Replay
new file mode 100644
index 00000000..cae77cc1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-50.SC2Replay
new file mode 100644
index 00000000..3823cc24
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-54.SC2Replay
new file mode 100644
index 00000000..5c8f3746
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-59.SC2Replay
new file mode 100644
index 00000000..3c5083a1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-24-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-00.SC2Replay
new file mode 100644
index 00000000..91981cff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-01.SC2Replay
new file mode 100644
index 00000000..8f6e35fc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-05.SC2Replay
new file mode 100644
index 00000000..b1b2a276
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-14.SC2Replay
new file mode 100644
index 00000000..2d59f102
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-16.SC2Replay
new file mode 100644
index 00000000..bc2b4278
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-18.SC2Replay
new file mode 100644
index 00000000..19a29586
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-20.SC2Replay
new file mode 100644
index 00000000..6f6a262b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-32.SC2Replay
new file mode 100644
index 00000000..9af7bc64
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-36.SC2Replay
new file mode 100644
index 00000000..6a91d6d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-41.SC2Replay
new file mode 100644
index 00000000..a6bb7f87
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-42.SC2Replay
new file mode 100644
index 00000000..01a1cbba
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-44.SC2Replay
new file mode 100644
index 00000000..39e59cac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-25-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-01.SC2Replay
new file mode 100644
index 00000000..18a5f1f2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-09.SC2Replay
new file mode 100644
index 00000000..79239340
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-13.SC2Replay
new file mode 100644
index 00000000..14f577ef
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-14.SC2Replay
new file mode 100644
index 00000000..4180aa68
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-15.SC2Replay
new file mode 100644
index 00000000..1193a6e2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-16.SC2Replay
new file mode 100644
index 00000000..a593778d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-18.SC2Replay
new file mode 100644
index 00000000..2f627f18
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-52.SC2Replay
new file mode 100644
index 00000000..b6111fc4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-26-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-01.SC2Replay
new file mode 100644
index 00000000..4c0a2f8b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-13.SC2Replay
new file mode 100644
index 00000000..6305408c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-18.SC2Replay
new file mode 100644
index 00000000..2b5fef16
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-22.SC2Replay
new file mode 100644
index 00000000..245f4516
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-26.SC2Replay
new file mode 100644
index 00000000..42086c93
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-31.SC2Replay
new file mode 100644
index 00000000..a2b47489
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-36.SC2Replay
new file mode 100644
index 00000000..5e3b692d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-37.SC2Replay
new file mode 100644
index 00000000..898357cd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-40.SC2Replay
new file mode 100644
index 00000000..137139a1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-41.SC2Replay
new file mode 100644
index 00000000..f76b1b1c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-47.SC2Replay
new file mode 100644
index 00000000..6febe990
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-57.SC2Replay
new file mode 100644
index 00000000..944b5f27
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-58.SC2Replay
new file mode 100644
index 00000000..be086fd4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-27-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-02.SC2Replay
new file mode 100644
index 00000000..c4c37dca
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-04.SC2Replay
new file mode 100644
index 00000000..ef9b7472
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-06.SC2Replay
new file mode 100644
index 00000000..1121ca80
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-09.SC2Replay
new file mode 100644
index 00000000..29e0ba7c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-11.SC2Replay
new file mode 100644
index 00000000..6eda52de
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-12.SC2Replay
new file mode 100644
index 00000000..50dda394
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-13.SC2Replay
new file mode 100644
index 00000000..927e0599
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-15.SC2Replay
new file mode 100644
index 00000000..0a3a437b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-17.SC2Replay
new file mode 100644
index 00000000..93a159cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-18.SC2Replay
new file mode 100644
index 00000000..0ec2ce18
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-22.SC2Replay
new file mode 100644
index 00000000..8c2d1118
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-24.SC2Replay
new file mode 100644
index 00000000..c50e0893
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-26.SC2Replay
new file mode 100644
index 00000000..32d9893d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-27.SC2Replay
new file mode 100644
index 00000000..2024bb86
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-28-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-38.SC2Replay
new file mode 100644
index 00000000..51fab048
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-44.SC2Replay
new file mode 100644
index 00000000..06312888
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-54.SC2Replay
new file mode 100644
index 00000000..95a399e0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-55.SC2Replay
new file mode 100644
index 00000000..38b54fef
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-35-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-00.SC2Replay
new file mode 100644
index 00000000..8a522303
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-03.SC2Replay
new file mode 100644
index 00000000..c328393f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-05.SC2Replay
new file mode 100644
index 00000000..4188bd60
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-08.SC2Replay
new file mode 100644
index 00000000..2d0701d7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-11.SC2Replay
new file mode 100644
index 00000000..974374b4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-13.SC2Replay
new file mode 100644
index 00000000..9ebd740b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-14.SC2Replay
new file mode 100644
index 00000000..9494602f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-16.SC2Replay
new file mode 100644
index 00000000..a4811820
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-19.SC2Replay
new file mode 100644
index 00000000..371d25d0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-21.SC2Replay
new file mode 100644
index 00000000..543b511f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-26.SC2Replay
new file mode 100644
index 00000000..3059f1ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-27.SC2Replay
new file mode 100644
index 00000000..134ed5bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-28.SC2Replay
new file mode 100644
index 00000000..640f99a0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-29.SC2Replay
new file mode 100644
index 00000000..ac1c596b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-31.SC2Replay
new file mode 100644
index 00000000..661e7e40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-34.SC2Replay
new file mode 100644
index 00000000..154b8d34
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-40.SC2Replay
new file mode 100644
index 00000000..fa2b6b2e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-42.SC2Replay
new file mode 100644
index 00000000..3663c4c2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-44.SC2Replay
new file mode 100644
index 00000000..9b6dc3f3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-58.SC2Replay
new file mode 100644
index 00000000..d88f51c3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-59.SC2Replay
new file mode 100644
index 00000000..5a463ab9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-36-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-18.SC2Replay
new file mode 100644
index 00000000..7ffdb6dc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-33.SC2Replay
new file mode 100644
index 00000000..b6559df6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-34.SC2Replay
new file mode 100644
index 00000000..bb4802d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-35.SC2Replay
new file mode 100644
index 00000000..f6fbf919
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-46.SC2Replay
new file mode 100644
index 00000000..d78c9149
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-47.SC2Replay
new file mode 100644
index 00000000..a02bbd86
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-58.SC2Replay
new file mode 100644
index 00000000..73eeef20
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-37-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-04.SC2Replay
new file mode 100644
index 00000000..e976c3b3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-10.SC2Replay
new file mode 100644
index 00000000..61a3a4ab
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-11.SC2Replay
new file mode 100644
index 00000000..02bbf5a7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-12.SC2Replay
new file mode 100644
index 00000000..1457c1f9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-14.SC2Replay
new file mode 100644
index 00000000..191b38ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-15.SC2Replay
new file mode 100644
index 00000000..c6685cac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-17.SC2Replay
new file mode 100644
index 00000000..4b2f14be
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-18.SC2Replay
new file mode 100644
index 00000000..809bea08
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-33.SC2Replay
new file mode 100644
index 00000000..7de170bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-35.SC2Replay
new file mode 100644
index 00000000..f224d16d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-38.SC2Replay
new file mode 100644
index 00000000..3fa2105c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-40.SC2Replay
new file mode 100644
index 00000000..f82f2e94
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-42.SC2Replay
new file mode 100644
index 00000000..a317aae0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-46.SC2Replay
new file mode 100644
index 00000000..7a6a042a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-48.SC2Replay
new file mode 100644
index 00000000..e8c4c330
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-51.SC2Replay
new file mode 100644
index 00000000..87192e3c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-56.SC2Replay
new file mode 100644
index 00000000..bdc11a5b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-58.SC2Replay
new file mode 100644
index 00000000..797c4267
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-38-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-00.SC2Replay
new file mode 100644
index 00000000..b07c07fe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-04.SC2Replay
new file mode 100644
index 00000000..e0339b33
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-08.SC2Replay
new file mode 100644
index 00000000..666bc202
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-17.SC2Replay
new file mode 100644
index 00000000..013f2f11
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-19.SC2Replay
new file mode 100644
index 00000000..f7ac4c46
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-28.SC2Replay
new file mode 100644
index 00000000..a9ba0cf5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-31.SC2Replay
new file mode 100644
index 00000000..b2483820
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-41.SC2Replay
new file mode 100644
index 00000000..82f77b4a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-43.SC2Replay
new file mode 100644
index 00000000..07692652
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-44.SC2Replay
new file mode 100644
index 00000000..695de114
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-54.SC2Replay
new file mode 100644
index 00000000..f9191874
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-56.SC2Replay
new file mode 100644
index 00000000..e66f2321
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-39-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-01.SC2Replay
new file mode 100644
index 00000000..2e052ee4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-03.SC2Replay
new file mode 100644
index 00000000..19e7139b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-08.SC2Replay
new file mode 100644
index 00000000..89cd099f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-09.SC2Replay
new file mode 100644
index 00000000..cba3d6ce
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-22.SC2Replay
new file mode 100644
index 00000000..7595b92c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-26.SC2Replay
new file mode 100644
index 00000000..fc551124
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-28.SC2Replay
new file mode 100644
index 00000000..45526339
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-33.SC2Replay
new file mode 100644
index 00000000..9511cfe1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-34.SC2Replay
new file mode 100644
index 00000000..30c29f9f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-38.SC2Replay
new file mode 100644
index 00000000..55da5294
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-40.SC2Replay
new file mode 100644
index 00000000..470e766a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-51.SC2Replay
new file mode 100644
index 00000000..0fc79f7f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-52.SC2Replay
new file mode 100644
index 00000000..3416e170
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-55.SC2Replay
new file mode 100644
index 00000000..7a325325
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-40-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-00.SC2Replay
new file mode 100644
index 00000000..af512521
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-02.SC2Replay
new file mode 100644
index 00000000..3fadb1a6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-11.SC2Replay
new file mode 100644
index 00000000..7a444645
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-12.SC2Replay
new file mode 100644
index 00000000..45d54471
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-16.SC2Replay
new file mode 100644
index 00000000..5bb25dfc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-28.SC2Replay
new file mode 100644
index 00000000..3fbdaf77
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-29.SC2Replay
new file mode 100644
index 00000000..2c4a45f8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-31.SC2Replay
new file mode 100644
index 00000000..d3024a4a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-41.SC2Replay
new file mode 100644
index 00000000..cf5e3047
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-49.SC2Replay
new file mode 100644
index 00000000..a30d1f40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-52.SC2Replay
new file mode 100644
index 00000000..d4ae6935
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-53.SC2Replay
new file mode 100644
index 00000000..04a3462b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-54.SC2Replay
new file mode 100644
index 00000000..fd230b9f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-55.SC2Replay
new file mode 100644
index 00000000..679685fd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-41-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-02.SC2Replay
new file mode 100644
index 00000000..23c9e0da
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-07.SC2Replay
new file mode 100644
index 00000000..5d6eb19b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-08.SC2Replay
new file mode 100644
index 00000000..096e809e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-21.SC2Replay
new file mode 100644
index 00000000..2d0bd4e6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-32.SC2Replay
new file mode 100644
index 00000000..9f879caf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-34.SC2Replay
new file mode 100644
index 00000000..44498873
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-35.SC2Replay
new file mode 100644
index 00000000..484f838f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-42.SC2Replay
new file mode 100644
index 00000000..7de341ec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-43.SC2Replay
new file mode 100644
index 00000000..88aee104
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-44.SC2Replay
new file mode 100644
index 00000000..c3888f6a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-58.SC2Replay
new file mode 100644
index 00000000..5d0ca372
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-42-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-00.SC2Replay
new file mode 100644
index 00000000..96b91bcc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-02.SC2Replay
new file mode 100644
index 00000000..d175671c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-07.SC2Replay
new file mode 100644
index 00000000..2392449c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-17.SC2Replay
new file mode 100644
index 00000000..e85478e2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-19.SC2Replay
new file mode 100644
index 00000000..10494c23
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-23.SC2Replay
new file mode 100644
index 00000000..5e1cd122
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-34.SC2Replay
new file mode 100644
index 00000000..0f3732b9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-37.SC2Replay
new file mode 100644
index 00000000..26961dd1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-41.SC2Replay
new file mode 100644
index 00000000..c74c2a61
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-42.SC2Replay
new file mode 100644
index 00000000..54f4e19b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-53.SC2Replay
new file mode 100644
index 00000000..a6c9ac30
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-58.SC2Replay
new file mode 100644
index 00000000..f7302cc5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-59.SC2Replay
new file mode 100644
index 00000000..b8e8b3ad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-43-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-05.SC2Replay
new file mode 100644
index 00000000..8fb25cb4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-08.SC2Replay
new file mode 100644
index 00000000..3f7c6ba3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-09.SC2Replay
new file mode 100644
index 00000000..6f91655a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-11.SC2Replay
new file mode 100644
index 00000000..a8b84d72
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-21.SC2Replay
new file mode 100644
index 00000000..99838567
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-23.SC2Replay
new file mode 100644
index 00000000..17b5a311
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-38.SC2Replay
new file mode 100644
index 00000000..143cae18
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-46.SC2Replay
new file mode 100644
index 00000000..c82d75b2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-47.SC2Replay
new file mode 100644
index 00000000..de68d63a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-52.SC2Replay
new file mode 100644
index 00000000..361caba7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-55.SC2Replay
new file mode 100644
index 00000000..3cf1aa7a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-56.SC2Replay
new file mode 100644
index 00000000..998843d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-44-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-03.SC2Replay
new file mode 100644
index 00000000..d5a8e65a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-06.SC2Replay
new file mode 100644
index 00000000..40cd2f2c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-07.SC2Replay
new file mode 100644
index 00000000..5284fdbc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-18.SC2Replay
new file mode 100644
index 00000000..d5a5faef
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-29.SC2Replay
new file mode 100644
index 00000000..1ed362c6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-31.SC2Replay
new file mode 100644
index 00000000..bd96af00
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-36.SC2Replay
new file mode 100644
index 00000000..ddd5ef00
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-41.SC2Replay
new file mode 100644
index 00000000..c540dc88
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-43.SC2Replay
new file mode 100644
index 00000000..46ec2d24
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-45.SC2Replay
new file mode 100644
index 00000000..c9e3be32
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-48.SC2Replay
new file mode 100644
index 00000000..a4fb2051
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-56.SC2Replay
new file mode 100644
index 00000000..6fa6aaed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-45-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-00.SC2Replay
new file mode 100644
index 00000000..de1082bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-01.SC2Replay
new file mode 100644
index 00000000..386ce797
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-02.SC2Replay
new file mode 100644
index 00000000..f0bb4696
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-07.SC2Replay
new file mode 100644
index 00000000..dba0d412
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-23.SC2Replay
new file mode 100644
index 00000000..4b9eac7b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-25.SC2Replay
new file mode 100644
index 00000000..f8b8c2a3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-27.SC2Replay
new file mode 100644
index 00000000..8bdb8a5a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-31.SC2Replay
new file mode 100644
index 00000000..2ee19eba
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-32.SC2Replay
new file mode 100644
index 00000000..24ff1fc7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-35.SC2Replay
new file mode 100644
index 00000000..b705b7ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-41.SC2Replay
new file mode 100644
index 00000000..b349590d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-49.SC2Replay
new file mode 100644
index 00000000..53aca34b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-50.SC2Replay
new file mode 100644
index 00000000..d70518b8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-56.SC2Replay
new file mode 100644
index 00000000..05fac53e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-59.SC2Replay
new file mode 100644
index 00000000..6718dba0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-46-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-00.SC2Replay
new file mode 100644
index 00000000..a5b25cb9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-01.SC2Replay
new file mode 100644
index 00000000..babc3ada
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-10.SC2Replay
new file mode 100644
index 00000000..7e30ec7b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-11.SC2Replay
new file mode 100644
index 00000000..aac3e011
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-12.SC2Replay
new file mode 100644
index 00000000..69a29a08
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-18.SC2Replay
new file mode 100644
index 00000000..95292405
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-25.SC2Replay
new file mode 100644
index 00000000..b912623e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-27.SC2Replay
new file mode 100644
index 00000000..c7418355
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-33.SC2Replay
new file mode 100644
index 00000000..7735bc64
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-40.SC2Replay
new file mode 100644
index 00000000..22e8dd7d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-41.SC2Replay
new file mode 100644
index 00000000..c3183ed3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-42.SC2Replay
new file mode 100644
index 00000000..125245b1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-47.SC2Replay
new file mode 100644
index 00000000..3019dea0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-56.SC2Replay
new file mode 100644
index 00000000..6233dbc5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-58.SC2Replay
new file mode 100644
index 00000000..da421372
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-47-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-08.SC2Replay
new file mode 100644
index 00000000..08d93f4c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-14.SC2Replay
new file mode 100644
index 00000000..93baf422
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-16.SC2Replay
new file mode 100644
index 00000000..7f52c1bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-19.SC2Replay
new file mode 100644
index 00000000..35ea8ac3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-20.SC2Replay
new file mode 100644
index 00000000..bdeb38d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-21.SC2Replay
new file mode 100644
index 00000000..628f07b0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-22.SC2Replay
new file mode 100644
index 00000000..272808b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-25.SC2Replay
new file mode 100644
index 00000000..f7dfd42b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-26.SC2Replay
new file mode 100644
index 00000000..c974fa3a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-31.SC2Replay
new file mode 100644
index 00000000..6598066f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-43.SC2Replay
new file mode 100644
index 00000000..3f931987
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-45.SC2Replay
new file mode 100644
index 00000000..6c9ffaf0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-49.SC2Replay
new file mode 100644
index 00000000..9bad3c2b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-51.SC2Replay
new file mode 100644
index 00000000..6d52c4e1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-52.SC2Replay
new file mode 100644
index 00000000..aeabb70b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-54.SC2Replay
new file mode 100644
index 00000000..cd760e73
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-55.SC2Replay
new file mode 100644
index 00000000..b8672fbc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-48-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-03.SC2Replay
new file mode 100644
index 00000000..258da298
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-04.SC2Replay
new file mode 100644
index 00000000..78219243
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-07.SC2Replay
new file mode 100644
index 00000000..5a7515b6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-09.SC2Replay
new file mode 100644
index 00000000..1144a21d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-10.SC2Replay
new file mode 100644
index 00000000..64bcdee2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-14.SC2Replay
new file mode 100644
index 00000000..3f28db71
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-17.SC2Replay
new file mode 100644
index 00000000..766b67c4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-24.SC2Replay
new file mode 100644
index 00000000..ad053bda
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-27.SC2Replay
new file mode 100644
index 00000000..a987ddc9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-30.SC2Replay
new file mode 100644
index 00000000..62721537
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-32.SC2Replay
new file mode 100644
index 00000000..5cc02d06
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-39.SC2Replay
new file mode 100644
index 00000000..f11342af
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-40.SC2Replay
new file mode 100644
index 00000000..32dfa554
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-44.SC2Replay
new file mode 100644
index 00000000..7aef25cd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-45.SC2Replay
new file mode 100644
index 00000000..8f765911
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-47.SC2Replay
new file mode 100644
index 00000000..31091c2a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-49.SC2Replay
new file mode 100644
index 00000000..9fea12a0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-49-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-34.SC2Replay
new file mode 100644
index 00000000..f24b2855
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-35.SC2Replay
new file mode 100644
index 00000000..56d21e87
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-57.SC2Replay
new file mode 100644
index 00000000..5ea6e9ef
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-57-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-06.SC2Replay
new file mode 100644
index 00000000..cc393d89
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-10.SC2Replay
new file mode 100644
index 00000000..7cc52a5d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-16.SC2Replay
new file mode 100644
index 00000000..d7f2c679
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-17.SC2Replay
new file mode 100644
index 00000000..58b14f2b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-21.SC2Replay
new file mode 100644
index 00000000..59e7038e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-22.SC2Replay
new file mode 100644
index 00000000..1e01765e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-23.SC2Replay
new file mode 100644
index 00000000..30da5513
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-24.SC2Replay
new file mode 100644
index 00000000..72d455f4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-25.SC2Replay
new file mode 100644
index 00000000..5ac5c856
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-26.SC2Replay
new file mode 100644
index 00000000..de2a7ecd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-34.SC2Replay
new file mode 100644
index 00000000..3eaab1b6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-35.SC2Replay
new file mode 100644
index 00000000..14bc454d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-36.SC2Replay
new file mode 100644
index 00000000..c16c001a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-38.SC2Replay
new file mode 100644
index 00000000..d85aeea3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-50.SC2Replay
new file mode 100644
index 00000000..09577af8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-54.SC2Replay
new file mode 100644
index 00000000..a1d673f8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-58-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-04.SC2Replay
new file mode 100644
index 00000000..8d2876db
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-05.SC2Replay
new file mode 100644
index 00000000..30f6e4c7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-11.SC2Replay
new file mode 100644
index 00000000..0eadd5d4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-14.SC2Replay
new file mode 100644
index 00000000..5ad92324
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-21.SC2Replay
new file mode 100644
index 00000000..42a62574
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-33.SC2Replay
new file mode 100644
index 00000000..c59dd07d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-38.SC2Replay
new file mode 100644
index 00000000..66fc0dfe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-39.SC2Replay
new file mode 100644
index 00000000..a19d4d15
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-42.SC2Replay
new file mode 100644
index 00000000..f7679bfc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-55.SC2Replay
new file mode 100644
index 00000000..7c789c93
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-57.SC2Replay
new file mode 100644
index 00000000..6a923f40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-58.SC2Replay
new file mode 100644
index 00000000..8c3ddaee
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-59.SC2Replay
new file mode 100644
index 00000000..e499ce8e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-15-59-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-18.SC2Replay
new file mode 100644
index 00000000..2f6460b9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-24.SC2Replay
new file mode 100644
index 00000000..adbf3358
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-25.SC2Replay
new file mode 100644
index 00000000..72b79d72
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-36.SC2Replay
new file mode 100644
index 00000000..1c30e832
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-37.SC2Replay
new file mode 100644
index 00000000..fca0bb7c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-39.SC2Replay
new file mode 100644
index 00000000..ae301027
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-45.SC2Replay
new file mode 100644
index 00000000..a3f1df5e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-50.SC2Replay
new file mode 100644
index 00000000..b66b6622
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-00-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-01.SC2Replay
new file mode 100644
index 00000000..4dc7c0a1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-08.SC2Replay
new file mode 100644
index 00000000..ecad0579
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-09.SC2Replay
new file mode 100644
index 00000000..080b3271
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-12.SC2Replay
new file mode 100644
index 00000000..2d30761f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-14.SC2Replay
new file mode 100644
index 00000000..923c7e1e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-17.SC2Replay
new file mode 100644
index 00000000..2e27c74f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-20.SC2Replay
new file mode 100644
index 00000000..e9713977
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-21.SC2Replay
new file mode 100644
index 00000000..dbb2ef19
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-24.SC2Replay
new file mode 100644
index 00000000..0cc6744b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-25.SC2Replay
new file mode 100644
index 00000000..abeb9656
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-35.SC2Replay
new file mode 100644
index 00000000..3a3d1276
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-43.SC2Replay
new file mode 100644
index 00000000..273594d3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-44.SC2Replay
new file mode 100644
index 00000000..5a6bec1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-45.SC2Replay
new file mode 100644
index 00000000..574d3cc3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-54.SC2Replay
new file mode 100644
index 00000000..e6b43e87
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-57.SC2Replay
new file mode 100644
index 00000000..3f415ccd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-59.SC2Replay
new file mode 100644
index 00000000..1edc6fdd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-01-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-06.SC2Replay
new file mode 100644
index 00000000..46522ff8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-14.SC2Replay
new file mode 100644
index 00000000..9257d75e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-16.SC2Replay
new file mode 100644
index 00000000..d74ac248
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-35.SC2Replay
new file mode 100644
index 00000000..681d3e8c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-48.SC2Replay
new file mode 100644
index 00000000..6976f05a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-02-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-02.SC2Replay
new file mode 100644
index 00000000..8c7d24cb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-04.SC2Replay
new file mode 100644
index 00000000..8d274356
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-06.SC2Replay
new file mode 100644
index 00000000..c07bf4ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-07.SC2Replay
new file mode 100644
index 00000000..46e2fc55
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-12.SC2Replay
new file mode 100644
index 00000000..bd51809a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-13.SC2Replay
new file mode 100644
index 00000000..ce7940bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-15.SC2Replay
new file mode 100644
index 00000000..c9e3b840
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-16.SC2Replay
new file mode 100644
index 00000000..b923ecad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-20.SC2Replay
new file mode 100644
index 00000000..d325d01e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-21.SC2Replay
new file mode 100644
index 00000000..32eda83c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-34.SC2Replay
new file mode 100644
index 00000000..071eb9c8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-42.SC2Replay
new file mode 100644
index 00000000..ffff7cb1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-48.SC2Replay
new file mode 100644
index 00000000..9263433a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-50.SC2Replay
new file mode 100644
index 00000000..5d093cd4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-53.SC2Replay
new file mode 100644
index 00000000..c0cff3dc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-54.SC2Replay
new file mode 100644
index 00000000..a609778b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-58.SC2Replay
new file mode 100644
index 00000000..7aaeef54
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-03-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-03.SC2Replay
new file mode 100644
index 00000000..baa1e959
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-04.SC2Replay
new file mode 100644
index 00000000..38dfde71
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-06.SC2Replay
new file mode 100644
index 00000000..8151d288
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-11.SC2Replay
new file mode 100644
index 00000000..0b0b4cac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-17.SC2Replay
new file mode 100644
index 00000000..fed98cf5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-22.SC2Replay
new file mode 100644
index 00000000..7c26c88d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-24.SC2Replay
new file mode 100644
index 00000000..307c8462
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-32.SC2Replay
new file mode 100644
index 00000000..595bd3fe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-37.SC2Replay
new file mode 100644
index 00000000..d293a905
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-39.SC2Replay
new file mode 100644
index 00000000..75b1c416
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-59.SC2Replay
new file mode 100644
index 00000000..cf2a6a58
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-04-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-01.SC2Replay
new file mode 100644
index 00000000..be93f756
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-03.SC2Replay
new file mode 100644
index 00000000..e00d3b1c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-06.SC2Replay
new file mode 100644
index 00000000..501ee487
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-09.SC2Replay
new file mode 100644
index 00000000..2df0acec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-18.SC2Replay
new file mode 100644
index 00000000..3a59e654
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-20.SC2Replay
new file mode 100644
index 00000000..973bdf68
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-25.SC2Replay
new file mode 100644
index 00000000..1c15c78d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-37.SC2Replay
new file mode 100644
index 00000000..d742829a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-38.SC2Replay
new file mode 100644
index 00000000..241b05d3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-43.SC2Replay
new file mode 100644
index 00000000..1bf9db91
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-46.SC2Replay
new file mode 100644
index 00000000..16f4a6ff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-49.SC2Replay
new file mode 100644
index 00000000..93960346
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-59.SC2Replay
new file mode 100644
index 00000000..70bd6630
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-05-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-04.SC2Replay
new file mode 100644
index 00000000..bd2d16b4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-12.SC2Replay
new file mode 100644
index 00000000..1e354709
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-13.SC2Replay
new file mode 100644
index 00000000..bce9a0f5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-25.SC2Replay
new file mode 100644
index 00000000..62770979
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-39.SC2Replay
new file mode 100644
index 00000000..e52bad43
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-41.SC2Replay
new file mode 100644
index 00000000..8aa195e1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-47.SC2Replay
new file mode 100644
index 00000000..01373c38
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-48.SC2Replay
new file mode 100644
index 00000000..5744795a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-58.SC2Replay
new file mode 100644
index 00000000..87e3d104
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-06-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-12.SC2Replay
new file mode 100644
index 00000000..5387d55b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-14.SC2Replay
new file mode 100644
index 00000000..630728f2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-16.SC2Replay
new file mode 100644
index 00000000..681ae5f0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-19.SC2Replay
new file mode 100644
index 00000000..f5404ef7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-27.SC2Replay
new file mode 100644
index 00000000..13ef1865
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-29.SC2Replay
new file mode 100644
index 00000000..4f0caa5b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-30.SC2Replay
new file mode 100644
index 00000000..9f1709f6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-44.SC2Replay
new file mode 100644
index 00000000..57e7b8cc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-46.SC2Replay
new file mode 100644
index 00000000..10f401bf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-47.SC2Replay
new file mode 100644
index 00000000..48115200
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-48.SC2Replay
new file mode 100644
index 00000000..e05fc97d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-55.SC2Replay
new file mode 100644
index 00000000..34602dfa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-07-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-09.SC2Replay
new file mode 100644
index 00000000..6ac4a449
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-13.SC2Replay
new file mode 100644
index 00000000..0eae79fa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-21.SC2Replay
new file mode 100644
index 00000000..cbde2b27
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-22.SC2Replay
new file mode 100644
index 00000000..51867d2d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-31.SC2Replay
new file mode 100644
index 00000000..32c626cb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-51.SC2Replay
new file mode 100644
index 00000000..bfc18f00
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-54.SC2Replay
new file mode 100644
index 00000000..a8e71e6e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-56.SC2Replay
new file mode 100644
index 00000000..b07d0f17
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-08-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-02.SC2Replay
new file mode 100644
index 00000000..e2a5853f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-05.SC2Replay
new file mode 100644
index 00000000..53e62f33
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-08.SC2Replay
new file mode 100644
index 00000000..43bcc80f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-10.SC2Replay
new file mode 100644
index 00000000..78cf2acb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-19.SC2Replay
new file mode 100644
index 00000000..aa079b76
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-20.SC2Replay
new file mode 100644
index 00000000..6eceb933
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-24.SC2Replay
new file mode 100644
index 00000000..9876df68
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-27.SC2Replay
new file mode 100644
index 00000000..e2d82419
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-31.SC2Replay
new file mode 100644
index 00000000..ec269e55
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-35.SC2Replay
new file mode 100644
index 00000000..76259523
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-51.SC2Replay
new file mode 100644
index 00000000..ee40a12f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-57.SC2Replay
new file mode 100644
index 00000000..4638b6ae
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-09-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-01.SC2Replay
new file mode 100644
index 00000000..966867a5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-02.SC2Replay
new file mode 100644
index 00000000..5d7600d5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-10.SC2Replay
new file mode 100644
index 00000000..0cba155a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-18.SC2Replay
new file mode 100644
index 00000000..0504e0bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-20.SC2Replay
new file mode 100644
index 00000000..e8efed2f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-30.SC2Replay
new file mode 100644
index 00000000..098ef8c1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-31.SC2Replay
new file mode 100644
index 00000000..b24b20e4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-34.SC2Replay
new file mode 100644
index 00000000..672fc9f4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-39.SC2Replay
new file mode 100644
index 00000000..003d654a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-40.SC2Replay
new file mode 100644
index 00000000..1214e9b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-42.SC2Replay
new file mode 100644
index 00000000..b3c3b845
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-43.SC2Replay
new file mode 100644
index 00000000..80b34852
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-56.SC2Replay
new file mode 100644
index 00000000..2d3c5ccb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-10-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-00.SC2Replay
new file mode 100644
index 00000000..f2ed87d8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-04.SC2Replay
new file mode 100644
index 00000000..1bbd16cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-09.SC2Replay
new file mode 100644
index 00000000..ec81065e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-15.SC2Replay
new file mode 100644
index 00000000..20a14b68
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-19.SC2Replay
new file mode 100644
index 00000000..c3b50cff
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-30.SC2Replay
new file mode 100644
index 00000000..92f18bf0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-36.SC2Replay
new file mode 100644
index 00000000..55cf3d5e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-39.SC2Replay
new file mode 100644
index 00000000..2af55f9d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-48.SC2Replay
new file mode 100644
index 00000000..70016d35
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-53.SC2Replay
new file mode 100644
index 00000000..0b6ca2d0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-54.SC2Replay
new file mode 100644
index 00000000..3deb85e7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-55.SC2Replay
new file mode 100644
index 00000000..4cb5db6f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-11-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-06.SC2Replay
new file mode 100644
index 00000000..885b0c4d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-12.SC2Replay
new file mode 100644
index 00000000..6172715e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-13.SC2Replay
new file mode 100644
index 00000000..69281f19
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-15.SC2Replay
new file mode 100644
index 00000000..2e4320a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-29.SC2Replay
new file mode 100644
index 00000000..0b7f0c80
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-31.SC2Replay
new file mode 100644
index 00000000..332c5faa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-34.SC2Replay
new file mode 100644
index 00000000..ceba8d7b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-36.SC2Replay
new file mode 100644
index 00000000..6a35b60c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-42.SC2Replay
new file mode 100644
index 00000000..d33cd8eb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-43.SC2Replay
new file mode 100644
index 00000000..597682f8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-46.SC2Replay
new file mode 100644
index 00000000..fd8a3de9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-48.SC2Replay
new file mode 100644
index 00000000..5afdd176
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-54.SC2Replay
new file mode 100644
index 00000000..e1360da0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-12-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-03.SC2Replay
new file mode 100644
index 00000000..f7ca844c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-08.SC2Replay
new file mode 100644
index 00000000..7f06700c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-19.SC2Replay
new file mode 100644
index 00000000..be1f8b70
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-21.SC2Replay
new file mode 100644
index 00000000..51e58c67
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-24.SC2Replay
new file mode 100644
index 00000000..6c9d175b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-26.SC2Replay
new file mode 100644
index 00000000..023c78f7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-27.SC2Replay
new file mode 100644
index 00000000..3fb31590
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-32.SC2Replay
new file mode 100644
index 00000000..90fb85ce
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-33.SC2Replay
new file mode 100644
index 00000000..127fb122
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-35.SC2Replay
new file mode 100644
index 00000000..4ef80492
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-39.SC2Replay
new file mode 100644
index 00000000..2da9b01e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-47.SC2Replay
new file mode 100644
index 00000000..1e96c477
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-49.SC2Replay
new file mode 100644
index 00000000..cb0ef5f2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-50.SC2Replay
new file mode 100644
index 00000000..180bbd5f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-52.SC2Replay
new file mode 100644
index 00000000..635b3a86
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-13-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-00.SC2Replay
new file mode 100644
index 00000000..e4752d06
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-01.SC2Replay
new file mode 100644
index 00000000..ffeb2780
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-14-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-35.SC2Replay
new file mode 100644
index 00000000..b6b9548c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-36.SC2Replay
new file mode 100644
index 00000000..59c2ca1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-37.SC2Replay
new file mode 100644
index 00000000..4100a902
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-38.SC2Replay
new file mode 100644
index 00000000..10b039a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-42.SC2Replay
new file mode 100644
index 00000000..0f364df9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-48.SC2Replay
new file mode 100644
index 00000000..5268a0bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-16-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-04.SC2Replay
new file mode 100644
index 00000000..432f4be3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-18.SC2Replay
new file mode 100644
index 00000000..59f38645
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-21.SC2Replay
new file mode 100644
index 00000000..817f13eb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-29.SC2Replay
new file mode 100644
index 00000000..ec39597c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-35.SC2Replay
new file mode 100644
index 00000000..357d7604
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-43.SC2Replay
new file mode 100644
index 00000000..0ab92ac5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-45.SC2Replay
new file mode 100644
index 00000000..83262eb6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-46.SC2Replay
new file mode 100644
index 00000000..c78d6885
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-49.SC2Replay
new file mode 100644
index 00000000..85183620
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-51.SC2Replay
new file mode 100644
index 00000000..6b767517
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-54.SC2Replay
new file mode 100644
index 00000000..6b258d0a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-57.SC2Replay
new file mode 100644
index 00000000..9d313d1b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-59.SC2Replay
new file mode 100644
index 00000000..bbf2a8c6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-17-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-00.SC2Replay
new file mode 100644
index 00000000..6f522798
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-03.SC2Replay
new file mode 100644
index 00000000..5a769462
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-06.SC2Replay
new file mode 100644
index 00000000..39a773fb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-12.SC2Replay
new file mode 100644
index 00000000..26f0eeae
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-17.SC2Replay
new file mode 100644
index 00000000..899df6a7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-23.SC2Replay
new file mode 100644
index 00000000..358111cc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-24.SC2Replay
new file mode 100644
index 00000000..5931dd5f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-39.SC2Replay
new file mode 100644
index 00000000..aef4b9c5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-40.SC2Replay
new file mode 100644
index 00000000..afdb6961
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-42.SC2Replay
new file mode 100644
index 00000000..192cf212
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-50.SC2Replay
new file mode 100644
index 00000000..f0c65126
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-53.SC2Replay
new file mode 100644
index 00000000..5935dd7b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-54.SC2Replay
new file mode 100644
index 00000000..55dae7f8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-56.SC2Replay
new file mode 100644
index 00000000..3542f70d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-57.SC2Replay
new file mode 100644
index 00000000..8d3a6744
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-18-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-05.SC2Replay
new file mode 100644
index 00000000..25e39494
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-07.SC2Replay
new file mode 100644
index 00000000..ee6341bc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-08.SC2Replay
new file mode 100644
index 00000000..34d5dae8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-09.SC2Replay
new file mode 100644
index 00000000..6ffd39bd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-10.SC2Replay
new file mode 100644
index 00000000..f67f002a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-11.SC2Replay
new file mode 100644
index 00000000..abe29ffe
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-22.SC2Replay
new file mode 100644
index 00000000..363bb2d6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-24.SC2Replay
new file mode 100644
index 00000000..d6000ee4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-25.SC2Replay
new file mode 100644
index 00000000..b2f4e06a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-29.SC2Replay
new file mode 100644
index 00000000..0fa879c9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-43.SC2Replay
new file mode 100644
index 00000000..ce97c5d5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-45.SC2Replay
new file mode 100644
index 00000000..d3743afa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-55.SC2Replay
new file mode 100644
index 00000000..b2ade911
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-56.SC2Replay
new file mode 100644
index 00000000..9d0b016f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-19-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-10.SC2Replay
new file mode 100644
index 00000000..28d5ac76
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-19.SC2Replay
new file mode 100644
index 00000000..c35ebe62
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-20.SC2Replay
new file mode 100644
index 00000000..4c4a8b5e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-25.SC2Replay
new file mode 100644
index 00000000..29378b91
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-26.SC2Replay
new file mode 100644
index 00000000..1b59137b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-27.SC2Replay
new file mode 100644
index 00000000..4afda906
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-28.SC2Replay
new file mode 100644
index 00000000..4bdd5d01
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-29.SC2Replay
new file mode 100644
index 00000000..64678119
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-34.SC2Replay
new file mode 100644
index 00000000..33aa265e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-40.SC2Replay
new file mode 100644
index 00000000..93036e1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-43.SC2Replay
new file mode 100644
index 00000000..14b75a14
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-45.SC2Replay
new file mode 100644
index 00000000..a332e06c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-46.SC2Replay
new file mode 100644
index 00000000..d15ccff0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-47.SC2Replay
new file mode 100644
index 00000000..5f4562b9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-52.SC2Replay
new file mode 100644
index 00000000..16343e16
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-54.SC2Replay
new file mode 100644
index 00000000..704cb9b2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-57.SC2Replay
new file mode 100644
index 00000000..65d6c0d8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-20-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-11.SC2Replay
new file mode 100644
index 00000000..591bfb66
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-14.SC2Replay
new file mode 100644
index 00000000..8278189d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-17.SC2Replay
new file mode 100644
index 00000000..7d5aba1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-18.SC2Replay
new file mode 100644
index 00000000..a7808129
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-21.SC2Replay
new file mode 100644
index 00000000..bb163413
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-22.SC2Replay
new file mode 100644
index 00000000..8c7ffbd3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-24.SC2Replay
new file mode 100644
index 00000000..a25c70a9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-27.SC2Replay
new file mode 100644
index 00000000..a136c4a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-31.SC2Replay
new file mode 100644
index 00000000..ec9604c8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-36.SC2Replay
new file mode 100644
index 00000000..725362cc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-38.SC2Replay
new file mode 100644
index 00000000..9f43a115
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-39.SC2Replay
new file mode 100644
index 00000000..8dfd85fd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-42.SC2Replay
new file mode 100644
index 00000000..8b41b951
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-44.SC2Replay
new file mode 100644
index 00000000..2ada080e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-47.SC2Replay
new file mode 100644
index 00000000..fde3bee2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-50.SC2Replay
new file mode 100644
index 00000000..ee7e314f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-52.SC2Replay
new file mode 100644
index 00000000..ecc4209f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-21-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-00.SC2Replay
new file mode 100644
index 00000000..74ff2d9b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-02.SC2Replay
new file mode 100644
index 00000000..7ab3c68b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-06.SC2Replay
new file mode 100644
index 00000000..71ce0db5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-07.SC2Replay
new file mode 100644
index 00000000..d8815423
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-09.SC2Replay
new file mode 100644
index 00000000..623995eb
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-13.SC2Replay
new file mode 100644
index 00000000..30816856
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-14.SC2Replay
new file mode 100644
index 00000000..13589640
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-15.SC2Replay
new file mode 100644
index 00000000..16b95ac8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-16.SC2Replay
new file mode 100644
index 00000000..136930b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-20.SC2Replay
new file mode 100644
index 00000000..7950d89d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-24.SC2Replay
new file mode 100644
index 00000000..c3cd9d86
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-25.SC2Replay
new file mode 100644
index 00000000..b9f0bfe7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-26.SC2Replay
new file mode 100644
index 00000000..ba98c80e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-31.SC2Replay
new file mode 100644
index 00000000..681a0193
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-33.SC2Replay
new file mode 100644
index 00000000..3af493b2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-35.SC2Replay
new file mode 100644
index 00000000..eb65e784
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-37.SC2Replay
new file mode 100644
index 00000000..533ad289
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-39.SC2Replay
new file mode 100644
index 00000000..7f6ea0ec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-50.SC2Replay
new file mode 100644
index 00000000..a372191c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-59.SC2Replay
new file mode 100644
index 00000000..4abd97e2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-22-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-13.SC2Replay
new file mode 100644
index 00000000..f77faae6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-14.SC2Replay
new file mode 100644
index 00000000..64d4f6b6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-16.SC2Replay
new file mode 100644
index 00000000..29a5eae4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-20.SC2Replay
new file mode 100644
index 00000000..cab07391
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-23.SC2Replay
new file mode 100644
index 00000000..56335089
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-24.SC2Replay
new file mode 100644
index 00000000..3723a31f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-26.SC2Replay
new file mode 100644
index 00000000..202d4ab7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-28.SC2Replay
new file mode 100644
index 00000000..9048c761
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-29.SC2Replay
new file mode 100644
index 00000000..d352efda
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-31.SC2Replay
new file mode 100644
index 00000000..5058ef33
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-32.SC2Replay
new file mode 100644
index 00000000..20f1dafd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-33.SC2Replay
new file mode 100644
index 00000000..2b9beedd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-36.SC2Replay
new file mode 100644
index 00000000..d2c91d5d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-37.SC2Replay
new file mode 100644
index 00000000..a5fb681b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-41.SC2Replay
new file mode 100644
index 00000000..4d148657
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-47.SC2Replay
new file mode 100644
index 00000000..7baf0885
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-54.SC2Replay
new file mode 100644
index 00000000..5eeec1a2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-27-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-06.SC2Replay
new file mode 100644
index 00000000..69269f1d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-18.SC2Replay
new file mode 100644
index 00000000..a20a619e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-21.SC2Replay
new file mode 100644
index 00000000..dac65e61
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-23.SC2Replay
new file mode 100644
index 00000000..954baacf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-30.SC2Replay
new file mode 100644
index 00000000..b5cb41db
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-31.SC2Replay
new file mode 100644
index 00000000..3841279e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-39.SC2Replay
new file mode 100644
index 00000000..69f5e2c0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-40.SC2Replay
new file mode 100644
index 00000000..523f045c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-43.SC2Replay
new file mode 100644
index 00000000..f97534c2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-56.SC2Replay
new file mode 100644
index 00000000..b8afb403
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-28-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-01.SC2Replay
new file mode 100644
index 00000000..872de747
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-03.SC2Replay
new file mode 100644
index 00000000..ef6d2125
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-05.SC2Replay
new file mode 100644
index 00000000..1f4d95de
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-08.SC2Replay
new file mode 100644
index 00000000..ee6687f6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-12.SC2Replay
new file mode 100644
index 00000000..32f6465d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-28.SC2Replay
new file mode 100644
index 00000000..fb6ccd0a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-30.SC2Replay
new file mode 100644
index 00000000..f9e3aaaf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-47.SC2Replay
new file mode 100644
index 00000000..b9ff9ba6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-51.SC2Replay
new file mode 100644
index 00000000..b0857f11
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-56.SC2Replay
new file mode 100644
index 00000000..5b328488
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-57.SC2Replay
new file mode 100644
index 00000000..e42cdf2d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-59.SC2Replay
new file mode 100644
index 00000000..affbcc05
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-29-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-00.SC2Replay
new file mode 100644
index 00000000..7326a345
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-02.SC2Replay
new file mode 100644
index 00000000..b347e483
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-09.SC2Replay
new file mode 100644
index 00000000..6a7e2ced
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-10.SC2Replay
new file mode 100644
index 00000000..d1f81f9c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-12.SC2Replay
new file mode 100644
index 00000000..2ab46b5a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-18.SC2Replay
new file mode 100644
index 00000000..4c829fe7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-19.SC2Replay
new file mode 100644
index 00000000..9019da06
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-22.SC2Replay
new file mode 100644
index 00000000..f51e9c73
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-23.SC2Replay
new file mode 100644
index 00000000..164f1fe7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-26.SC2Replay
new file mode 100644
index 00000000..edc68f40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-27.SC2Replay
new file mode 100644
index 00000000..e9ebe70f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-32.SC2Replay
new file mode 100644
index 00000000..50218452
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-39.SC2Replay
new file mode 100644
index 00000000..6df0b715
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-30-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-02.SC2Replay
new file mode 100644
index 00000000..cc53c906
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-09.SC2Replay
new file mode 100644
index 00000000..9032dcc7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-14.SC2Replay
new file mode 100644
index 00000000..3b9e2ecf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-15.SC2Replay
new file mode 100644
index 00000000..5f12977d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-18.SC2Replay
new file mode 100644
index 00000000..8911c688
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-20.SC2Replay
new file mode 100644
index 00000000..63e86399
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-21.SC2Replay
new file mode 100644
index 00000000..24990a40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-25.SC2Replay
new file mode 100644
index 00000000..d44608a9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-26.SC2Replay
new file mode 100644
index 00000000..ae3f857e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-27.SC2Replay
new file mode 100644
index 00000000..6e2f11f9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-29.SC2Replay
new file mode 100644
index 00000000..4f731b53
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-32.SC2Replay
new file mode 100644
index 00000000..0b13d600
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-48.SC2Replay
new file mode 100644
index 00000000..36206b92
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-59.SC2Replay
new file mode 100644
index 00000000..d747c0a2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-31-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-05.SC2Replay
new file mode 100644
index 00000000..d88d972f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-09.SC2Replay
new file mode 100644
index 00000000..0d909999
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-12.SC2Replay
new file mode 100644
index 00000000..351ce052
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-19.SC2Replay
new file mode 100644
index 00000000..8b419db0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-22.SC2Replay
new file mode 100644
index 00000000..1d662000
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-23.SC2Replay
new file mode 100644
index 00000000..919b0d92
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-25.SC2Replay
new file mode 100644
index 00000000..8b8c20a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-36.SC2Replay
new file mode 100644
index 00000000..72a25379
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-39.SC2Replay
new file mode 100644
index 00000000..684c119a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-52.SC2Replay
new file mode 100644
index 00000000..7421789f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-55.SC2Replay
new file mode 100644
index 00000000..280a2a07
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-57.SC2Replay
new file mode 100644
index 00000000..8100f667
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-32-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-02.SC2Replay
new file mode 100644
index 00000000..c171ec5e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-04.SC2Replay
new file mode 100644
index 00000000..daff02b1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-07.SC2Replay
new file mode 100644
index 00000000..39a2dad0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-08.SC2Replay
new file mode 100644
index 00000000..ae18c347
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-10.SC2Replay
new file mode 100644
index 00000000..3a878c7a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-13.SC2Replay
new file mode 100644
index 00000000..b8affe8b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-14.SC2Replay
new file mode 100644
index 00000000..745d265a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-18.SC2Replay
new file mode 100644
index 00000000..c88aadbd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-20.SC2Replay
new file mode 100644
index 00000000..d7f7a6c0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-26.SC2Replay
new file mode 100644
index 00000000..8e633d81
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-28.SC2Replay
new file mode 100644
index 00000000..78556746
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-30.SC2Replay
new file mode 100644
index 00000000..52ea35ea
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-32.SC2Replay
new file mode 100644
index 00000000..9c1af71d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-33.SC2Replay
new file mode 100644
index 00000000..b31ea847
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-36.SC2Replay
new file mode 100644
index 00000000..c0aa484c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-38.SC2Replay
new file mode 100644
index 00000000..b7616a32
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-39.SC2Replay
new file mode 100644
index 00000000..b2353dd2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-42.SC2Replay
new file mode 100644
index 00000000..fb3e1572
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-45.SC2Replay
new file mode 100644
index 00000000..1b08a365
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-52.SC2Replay
new file mode 100644
index 00000000..9c175c47
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-54.SC2Replay
new file mode 100644
index 00000000..d3ba3c25
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-33-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-02.SC2Replay
new file mode 100644
index 00000000..d9148722
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-07.SC2Replay
new file mode 100644
index 00000000..5346a0cf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-09.SC2Replay
new file mode 100644
index 00000000..0fbd15d1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-10.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-10.SC2Replay
new file mode 100644
index 00000000..6100d4c3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-10.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-12.SC2Replay
new file mode 100644
index 00000000..c43bcbc3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-14.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-14.SC2Replay
new file mode 100644
index 00000000..64221bab
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-14.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-25.SC2Replay
new file mode 100644
index 00000000..a4dc95cd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-26.SC2Replay
new file mode 100644
index 00000000..c8abb536
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-34-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-15.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-15.SC2Replay
new file mode 100644
index 00000000..52259ec8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-15.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-18.SC2Replay
new file mode 100644
index 00000000..3745434a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-19.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-19.SC2Replay
new file mode 100644
index 00000000..17cbe75b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-19.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-20.SC2Replay
new file mode 100644
index 00000000..320ecc82
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-21.SC2Replay
new file mode 100644
index 00000000..fd928fe1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-23.SC2Replay
new file mode 100644
index 00000000..a00c5d1f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-24.SC2Replay
new file mode 100644
index 00000000..b2f431ae
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-26.SC2Replay
new file mode 100644
index 00000000..76dd60b2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-27.SC2Replay
new file mode 100644
index 00000000..ec288d42
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-28.SC2Replay
new file mode 100644
index 00000000..f37f3317
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-30.SC2Replay
new file mode 100644
index 00000000..603f9f31
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-31.SC2Replay
new file mode 100644
index 00000000..7978b12b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-36.SC2Replay
new file mode 100644
index 00000000..37d4b6be
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-45.SC2Replay
new file mode 100644
index 00000000..daa9c514
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-47.SC2Replay
new file mode 100644
index 00000000..e739d938
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-57.SC2Replay
new file mode 100644
index 00000000..941f8f63
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-59.SC2Replay
new file mode 100644
index 00000000..2e5ba0ce
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-38-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-04.SC2Replay
new file mode 100644
index 00000000..1dbe1fec
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-05.SC2Replay
new file mode 100644
index 00000000..1d2b6356
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-09.SC2Replay
new file mode 100644
index 00000000..d8e3180c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-24.SC2Replay
new file mode 100644
index 00000000..9973af28
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-26.SC2Replay
new file mode 100644
index 00000000..f9bca1fc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-29.SC2Replay
new file mode 100644
index 00000000..e11e26b9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-30.SC2Replay
new file mode 100644
index 00000000..178080d3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-32.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-32.SC2Replay
new file mode 100644
index 00000000..b15f01c7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-32.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-34.SC2Replay
new file mode 100644
index 00000000..91306cb9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-37.SC2Replay
new file mode 100644
index 00000000..55810236
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-38.SC2Replay
new file mode 100644
index 00000000..a6a6f30e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-40.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-40.SC2Replay
new file mode 100644
index 00000000..efe3bbe3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-40.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-41.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-41.SC2Replay
new file mode 100644
index 00000000..f4704758
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-41.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-43.SC2Replay
new file mode 100644
index 00000000..9565eaad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-45.SC2Replay
new file mode 100644
index 00000000..dfcd07ad
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-46.SC2Replay
new file mode 100644
index 00000000..745cab64
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-47.SC2Replay
new file mode 100644
index 00000000..1e303a59
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-55.SC2Replay
new file mode 100644
index 00000000..e429a770
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-56.SC2Replay
new file mode 100644
index 00000000..1b7c6f26
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-58.SC2Replay
new file mode 100644
index 00000000..6a91aa02
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-59.SC2Replay
new file mode 100644
index 00000000..f6f7d6ab
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-39-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-05.SC2Replay
new file mode 100644
index 00000000..e35688a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-07.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-07.SC2Replay
new file mode 100644
index 00000000..2bb1b845
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-07.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-08.SC2Replay
new file mode 100644
index 00000000..4f95a99b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-11.SC2Replay
new file mode 100644
index 00000000..52a1a082
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-25.SC2Replay
new file mode 100644
index 00000000..3fc181a8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-26.SC2Replay
new file mode 100644
index 00000000..9b75ddfa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-27.SC2Replay
new file mode 100644
index 00000000..6e771f59
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-31.SC2Replay
new file mode 100644
index 00000000..2fa471e3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-35.SC2Replay
new file mode 100644
index 00000000..7f334277
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-39.SC2Replay
new file mode 100644
index 00000000..4eca5d53
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-43.SC2Replay
new file mode 100644
index 00000000..0f085b35
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-44.SC2Replay
new file mode 100644
index 00000000..55fdaf81
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-46.SC2Replay
new file mode 100644
index 00000000..6bad1ed9
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-47.SC2Replay
new file mode 100644
index 00000000..14cd0463
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-48.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-48.SC2Replay
new file mode 100644
index 00000000..e2a02b4b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-48.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-50.SC2Replay
new file mode 100644
index 00000000..81f54de6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-53.SC2Replay
new file mode 100644
index 00000000..ded72da2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-54.SC2Replay
new file mode 100644
index 00000000..bf3a0768
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-55.SC2Replay
new file mode 100644
index 00000000..1352d49e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-56.SC2Replay
new file mode 100644
index 00000000..f54e84ac
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-58.SC2Replay
new file mode 100644
index 00000000..64dee706
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-40-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-01.SC2Replay
new file mode 100644
index 00000000..6b926a13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-02.SC2Replay
new file mode 100644
index 00000000..5575403f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-04.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-04.SC2Replay
new file mode 100644
index 00000000..70f9f423
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-04.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-08.SC2Replay
new file mode 100644
index 00000000..344ba8d2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-13.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-13.SC2Replay
new file mode 100644
index 00000000..90fa6826
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-13.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-17.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-17.SC2Replay
new file mode 100644
index 00000000..c99ea6f1
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-17.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-24.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-24.SC2Replay
new file mode 100644
index 00000000..a88576c8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-24.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-25.SC2Replay
new file mode 100644
index 00000000..fedde68c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-36.SC2Replay
new file mode 100644
index 00000000..295d5b46
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-37.SC2Replay
new file mode 100644
index 00000000..7a1433c7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-38.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-38.SC2Replay
new file mode 100644
index 00000000..eb19c336
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-38.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-39.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-39.SC2Replay
new file mode 100644
index 00000000..af284700
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-39.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-43.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-43.SC2Replay
new file mode 100644
index 00000000..fb88f9e8
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-43.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-49.SC2Replay
new file mode 100644
index 00000000..fb2556fc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-54.SC2Replay
new file mode 100644
index 00000000..711f6a8b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-57.SC2Replay
new file mode 100644
index 00000000..7eb2ada6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-59.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-59.SC2Replay
new file mode 100644
index 00000000..395249a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-41-59.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-00.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-00.SC2Replay
new file mode 100644
index 00000000..239db8d4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-00.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-01.SC2Replay
new file mode 100644
index 00000000..ea8e37df
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-02.SC2Replay
new file mode 100644
index 00000000..32e39c4b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-03.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-03.SC2Replay
new file mode 100644
index 00000000..a1000d3d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-03.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-08.SC2Replay
new file mode 100644
index 00000000..7ee83e13
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-11.SC2Replay
new file mode 100644
index 00000000..e88c84ba
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-16.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-16.SC2Replay
new file mode 100644
index 00000000..d44aa8f3
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-16.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-20.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-20.SC2Replay
new file mode 100644
index 00000000..ea271eaf
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-20.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-22.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-22.SC2Replay
new file mode 100644
index 00000000..dc477532
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-22.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-23.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-23.SC2Replay
new file mode 100644
index 00000000..7ca20be5
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-23.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-26.SC2Replay
new file mode 100644
index 00000000..05578c19
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-27.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-27.SC2Replay
new file mode 100644
index 00000000..3bf69bc0
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-27.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-28.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-28.SC2Replay
new file mode 100644
index 00000000..cecc1e30
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-28.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-34.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-34.SC2Replay
new file mode 100644
index 00000000..552a9b76
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-34.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-37.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-37.SC2Replay
new file mode 100644
index 00000000..617c27fa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-37.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-42.SC2Replay
new file mode 100644
index 00000000..16fd3949
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-44.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-44.SC2Replay
new file mode 100644
index 00000000..0d94b2f6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-44.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-45.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-45.SC2Replay
new file mode 100644
index 00000000..403f50dd
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-45.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-46.SC2Replay
new file mode 100644
index 00000000..a0430c40
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-47.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-47.SC2Replay
new file mode 100644
index 00000000..e9401daa
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-47.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-49.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-49.SC2Replay
new file mode 100644
index 00000000..ff6c5a25
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-49.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-51.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-51.SC2Replay
new file mode 100644
index 00000000..e12d045b
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-51.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-52.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-52.SC2Replay
new file mode 100644
index 00000000..7ee5a48f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-52.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-53.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-53.SC2Replay
new file mode 100644
index 00000000..67f43b2e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-53.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-54.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-54.SC2Replay
new file mode 100644
index 00000000..41b3974a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-54.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-55.SC2Replay
new file mode 100644
index 00000000..031e5485
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-56.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-56.SC2Replay
new file mode 100644
index 00000000..49da99d2
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-56.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-57.SC2Replay
new file mode 100644
index 00000000..ecdff803
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-42-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-01.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-01.SC2Replay
new file mode 100644
index 00000000..0e07a0de
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-01.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-02.SC2Replay
new file mode 100644
index 00000000..e5aefaf6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-05.SC2Replay
new file mode 100644
index 00000000..1489fa57
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-05.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-06.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-06.SC2Replay
new file mode 100644
index 00000000..7a1debb4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-11-16-43-06.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-12.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-12.SC2Replay
new file mode 100644
index 00000000..d094e97f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-12.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-31.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-31.SC2Replay
new file mode 100644
index 00000000..47bee066
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-31.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-57.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-57.SC2Replay
new file mode 100644
index 00000000..8308379e
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-38-57.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-08.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-08.SC2Replay
new file mode 100644
index 00000000..263ffc16
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-08.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-26.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-26.SC2Replay
new file mode 100644
index 00000000..644ee41d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-26.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-42.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-42.SC2Replay
new file mode 100644
index 00000000..ce3dca9d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-15-21-39-42.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-44-55.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-44-55.SC2Replay
new file mode 100644
index 00000000..5533a43a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-44-55.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-21.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-21.SC2Replay
new file mode 100644
index 00000000..d8fb5f75
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-21.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-33.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-33.SC2Replay
new file mode 100644
index 00000000..4388d68d
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-45-33.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-11.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-11.SC2Replay
new file mode 100644
index 00000000..fa9266cc
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-11.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-25.SC2Replay
new file mode 100644
index 00000000..d8a67d3c
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-46-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-50-46.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-50-46.SC2Replay
new file mode 100644
index 00000000..bdd9e779
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-50-46.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-51-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-51-09.SC2Replay
new file mode 100644
index 00000000..d4e81a78
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-51-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-18.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-18.SC2Replay
new file mode 100644
index 00000000..17749e5f
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-18.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-30.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-30.SC2Replay
new file mode 100644
index 00000000..5a532005
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-52-30.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-36.SC2Replay
new file mode 100644
index 00000000..317e3178
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-58.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-58.SC2Replay
new file mode 100644
index 00000000..826b1ece
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-54-58.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-29.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-29.SC2Replay
new file mode 100644
index 00000000..f46922ed
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-29.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-50.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-50.SC2Replay
new file mode 100644
index 00000000..0c6e31e6
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-13-57-50.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-09.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-09.SC2Replay
new file mode 100644
index 00000000..3f9ce306
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-09.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-25.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-25.SC2Replay
new file mode 100644
index 00000000..264fc0b7
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-01-25.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-11-36.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-11-36.SC2Replay
new file mode 100644
index 00000000..683b7d32
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-11-36.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-12-02.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-12-02.SC2Replay
new file mode 100644
index 00000000..9f08b1a4
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-12-02.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-15-35.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-15-35.SC2Replay
new file mode 100644
index 00000000..f7c6e8da
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-15-35.SC2Replay differ
diff --git a/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-16-05.SC2Replay b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-16-05.SC2Replay
new file mode 100644
index 00000000..4741900a
Binary files /dev/null and b/CompSci/2023-Final-Semester/CM3070-Final-Project/final-project-code-repository/replays/kane-ai/pysc2_2023-07-16-14-16-05.SC2Replay differ
